{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59854389",
   "metadata": {},
   "source": [
    "## RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a1cf2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b196a674",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"b\",3), (\"b\",4), (\"a\",5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2730eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 4), ('a', 5)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduceByKey(lambda x, y: x if x >= y else y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771c2b40",
   "metadata": {},
   "source": [
    "## GraphFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "816bbd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addPyFile(\"/home/bigbenchung/spark-3.3.2-bin-hadoop3/jars/graphframes-0.8.2-spark3.0-s_2.12.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d850447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2af8df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  a|  Alice|\n",
      "|  b|    Bob|\n",
      "|  c|Charlie|\n",
      "|  d|  David|\n",
      "|  e| Esther|\n",
      "|  f|  Fanny|\n",
      "|  g|  Gabby|\n",
      "+---+-------+\n",
      "\n",
      "+---+---+------------+\n",
      "|src|dst|relationship|\n",
      "+---+---+------------+\n",
      "|  a|  b|      friend|\n",
      "|  b|  a|      friend|\n",
      "|  b|  c|      friend|\n",
      "|  c|  b|      friend|\n",
      "|  a|  c|      friend|\n",
      "|  c|  a|      friend|\n",
      "|  a|  d|      friend|\n",
      "|  b|  c|      friend|\n",
      "|  c|  b|      friend|\n",
      "|  d|  e|      friend|\n",
      "|  e|  d|      friend|\n",
      "|  b|  a|      follow|\n",
      "|  c|  b|      follow|\n",
      "|  f|  c|      follow|\n",
      "|  e|  f|      follow|\n",
      "|  e|  d|      friend|\n",
      "|  d|  a|      friend|\n",
      "|  a|  e|      friend|\n",
      "|  g|  e|      follow|\n",
      "+---+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vertics DataFrame\n",
    "v = spark.createDataFrame([\n",
    "  (\"a\", \"Alice\"),\n",
    "  (\"b\", \"Bob\"),\n",
    "  (\"c\", \"Charlie\"),\n",
    "  (\"d\", \"David\"),\n",
    "  (\"e\", \"Esther\"),\n",
    "  (\"f\", \"Fanny\"),\n",
    "  (\"g\", \"Gabby\")\n",
    "], [\"id\", \"name\"])\n",
    "\n",
    "# Edges DataFrame\n",
    "e = spark.createDataFrame([\n",
    "  (\"a\", \"b\", \"friend\"),\n",
    "  (\"b\", \"a\", \"friend\"),\n",
    "  (\"b\", \"c\", \"friend\"),\n",
    "  (\"c\", \"b\", \"friend\"),\n",
    "  (\"a\", \"c\", \"friend\"),\n",
    "  (\"c\", \"a\", \"friend\"),\n",
    "  (\"a\", \"d\", \"friend\"),\n",
    "  (\"b\", \"c\", \"friend\"),\n",
    "  (\"c\", \"b\", \"friend\"),\n",
    "  (\"d\", \"e\", \"friend\"),\n",
    "  (\"e\", \"d\", \"friend\"),\n",
    "  (\"b\", \"a\", \"follow\"),\n",
    "  (\"c\", \"b\", \"follow\"),\n",
    "  (\"f\", \"c\", \"follow\"),\n",
    "  (\"e\", \"f\", \"follow\"),\n",
    "  (\"e\", \"d\", \"friend\"),\n",
    "  (\"d\", \"a\", \"friend\"),\n",
    "  (\"a\", \"e\", \"friend\"),\n",
    "  (\"g\", \"e\", \"follow\")\n",
    "], [\"src\", \"dst\", \"relationship\"])\n",
    "\n",
    "# Create a GraphFrame\n",
    "g = GraphFrame(v, e)\n",
    "\n",
    "g.vertices.show()\n",
    "g.edges.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d2f6780",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = g.find(\"(a)-[e1]->(b);(b)-[e2]->(c);(c)-[e3]->(a)\") \\\n",
    "        .filter('e1.relationship = \"friend\" and e2.relationship = \"friend\" and e3.relationship = \"friend\"') \\\n",
    "        .filter('a.id < b.id').filter('b.id < c.id') \\\n",
    "        .select(\"a.name\", \"b.name\", \"c.name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a55fd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-------+\n",
      "| name|name|   name|\n",
      "+-----+----+-------+\n",
      "|Alice| Bob|Charlie|\n",
      "|Alice| Bob|Charlie|\n",
      "+-----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83b1b4d",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "038c7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad6963c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StreamingContext with batch interval of 5 seconds\n",
    "ssc = StreamingContext(sc, 5)\n",
    "ssc.checkpoint(\"checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44076286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DStream that will connect to localhost at port 9999\n",
    "# Start Netcat server: nc -lk 9999 \n",
    "# lines = ssc.socketTextStream('localhost', 9999)\n",
    "\n",
    "rdd = sc.textFile(\"data/adj_noun_pairs.txt\", 8).map(lambda l: tuple(l.split())).filter(lambda p: len(p)==2)\n",
    "rddQueue = rdd.randomSplit([1]*10, 123)\n",
    "lines = ssc.queueStream(rddQueue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d598d0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:14:01 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "Start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:14:09 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[(('external', 'link'), 836), (('19th', 'century'), 327), (('same', 'time'), 280), (('20th', 'century'), 266), (('first', 'time'), 264), (('other', 'hand'), 236), (('large', 'number'), 227), (('civil', 'war'), 211), (('political', 'party'), 201), (('recent', 'year'), 189)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:14:14 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 18:14:14 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[(('external', 'link'), 1622), (('19th', 'century'), 608), (('same', 'time'), 555), (('20th', 'century'), 544), (('first', 'time'), 532), (('other', 'hand'), 426), (('large', 'number'), 419), (('civil', 'war'), 412), (('recent', 'year'), 404), (('political', 'party'), 391)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:14:19 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 18:14:19 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[(('external', 'link'), 2427), (('19th', 'century'), 897), (('20th', 'century'), 834), (('same', 'time'), 830), (('first', 'time'), 799), (('civil', 'war'), 654), (('large', 'number'), 630), (('other', 'hand'), 629), (('political', 'party'), 571), (('recent', 'year'), 564)]\n",
      "24/05/20 18:14:19 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:14:24 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[(('external', 'link'), 3248), (('19th', 'century'), 1187), (('20th', 'century'), 1128), (('same', 'time'), 1115), (('first', 'time'), 1059), (('civil', 'war'), 909), (('large', 'number'), 852), (('other', 'hand'), 836), (('political', 'party'), 762), (('recent', 'year'), 756)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('external', 'link'), 4075), (('19th', 'century'), 1457), (('20th', 'century'), 1388), (('same', 'time'), 1359), (('first', 'time'), 1310), (('civil', 'war'), 1137), (('large', 'number'), 1065), (('other', 'hand'), 1036), (('political', 'party'), 953), (('recent', 'year'), 944)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:14:36 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 18:14:36 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('external', 'link'), 4890), (('19th', 'century'), 1741), (('20th', 'century'), 1676), (('same', 'time'), 1665), (('first', 'time'), 1558), (('civil', 'war'), 1352), (('large', 'number'), 1282), (('other', 'hand'), 1228), (('political', 'party'), 1149), (('recent', 'year'), 1129)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:14:47 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 18:14:47 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('external', 'link'), 5727), (('19th', 'century'), 2027), (('20th', 'century'), 1950), (('same', 'time'), 1937), (('first', 'time'), 1826), (('civil', 'war'), 1561), (('large', 'number'), 1498), (('other', 'hand'), 1443), (('political', 'party'), 1337), (('other', 'country'), 1301)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:>                                                       (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:14:51 WARN BatchedWriteAheadLog: BatchedWriteAheadLog Writer queue interrupted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:=======================>                                 (5 + 7) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:14:55 ERROR JobScheduler: Error generating jobs for time 1716200085000 ms\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 410, in func\n",
      "    return oldfunc(rdd)  # type: ignore[arg-type, call-arg]\n",
      "  File \"/tmp/ipykernel_21948/1068134567.py\", line 9, in <lambda>\n",
      "    counts_sorted = running_counts.transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 1037, in sortBy\n",
      "    self.keyBy(keyfunc)  # type: ignore[type-var]\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 995, in sortByKey\n",
      "    rddSize = self.count()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 1521, in count\n",
      "    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 1508, in sum\n",
      "    return self.mapPartitions(lambda x: [sum(x)]).fold(  # type: ignore[return-value]\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 1336, in fold\n",
      "    vals = self.mapPartitions(func).collect()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 1197, in collect\n",
      "    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": java.lang.InterruptedException\n",
      "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1302)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:334)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:943)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.GeneratedMethodAccessor131.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonTransformedDStream.compute(PythonDStream.scala:246)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)\n",
      "\tat org.apache.spark.streaming.DStreamGraph.$anonfun$generateJobs$2(DStreamGraph.scala:123)\n",
      "\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n",
      "\tat scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:75)\n",
      "\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n",
      "\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n",
      "\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n",
      "\tat org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:122)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator.$anonfun$generateJobs$1(JobGenerator.scala:254)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:252)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:186)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:91)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:90)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonTransformedDStream.compute(PythonDStream.scala:246)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)\n",
      "\tat org.apache.spark.streaming.DStreamGraph.$anonfun$generateJobs$2(DStreamGraph.scala:123)\n",
      "\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n",
      "\tat scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:75)\n",
      "\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n",
      "\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n",
      "\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n",
      "\tat org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:122)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator.$anonfun$generateJobs$1(JobGenerator.scala:254)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:252)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:186)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:91)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:90)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('external', 'link'), 6503), (('19th', 'century'), 2328), (('20th', 'century'), 2238), (('same', 'time'), 2221), (('first', 'time'), 2096), (('civil', 'war'), 1775), (('large', 'number'), 1688), (('other', 'hand'), 1654), (('political', 'party'), 1537), (('other', 'country'), 1463)]\n",
      "Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:==============================================>         (10 + 2) / 12]\r"
     ]
    }
   ],
   "source": [
    "def updateFunc(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    return sum(newValues, runningCount)\n",
    "    # add the new values with the previous running count to get the new count\n",
    "    \n",
    "running_counts = lines.map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y).updateStateByKey(updateFunc)\n",
    "\n",
    "counts_sorted = running_counts.transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n",
    "\n",
    "def printResult(rdd):\n",
    "    print(rdd.take(10))\n",
    "    \n",
    "counts_sorted.foreachRDD(printResult)\n",
    "\n",
    "ssc.start()  # Start the computation\n",
    "print(\"Start\")\n",
    "ssc.awaitTermination(50)  # Wait for the computation to terminate\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcd8e78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:32:47 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "Start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('external', 'link'), 836), (('19th', 'century'), 327), (('same', 'time'), 280), (('other', 'hand'), 236), (('large', 'number'), 227), (('civil', 'war'), 211), (('political', 'party'), 201), (('recent', 'year'), 189), (('other', 'country'), 179), (('many', 'people'), 174)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:32:59 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 18:32:59 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 18:32:59 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[(('external', 'link'), 1622), (('19th', 'century'), 608), (('same', 'time'), 555), (('other', 'hand'), 426), (('large', 'number'), 419), (('civil', 'war'), 412), (('recent', 'year'), 404), (('political', 'party'), 391), (('other', 'country'), 360), (('many', 'people'), 333)]\n",
      "24/05/20 18:32:59 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:33:03 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[(('external', 'link'), 2427), (('19th', 'century'), 897), (('same', 'time'), 830), (('civil', 'war'), 654), (('large', 'number'), 630), (('other', 'hand'), 629), (('political', 'party'), 571), (('recent', 'year'), 564), (('other', 'country'), 549), (('many', 'people'), 500)]\n",
      "24/05/20 18:33:03 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:33:09 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[(('external', 'link'), 3248), (('19th', 'century'), 1187), (('same', 'time'), 1115), (('civil', 'war'), 909), (('large', 'number'), 852), (('other', 'hand'), 836), (('political', 'party'), 762), (('recent', 'year'), 756), (('other', 'country'), 741), (('many', 'people'), 653)]\n",
      "24/05/20 18:33:09 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:33:14 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[(('external', 'link'), 4075), (('19th', 'century'), 1457), (('same', 'time'), 1359), (('civil', 'war'), 1137), (('large', 'number'), 1065), (('other', 'hand'), 1036), (('political', 'party'), 953), (('recent', 'year'), 944), (('other', 'country'), 921), (('many', 'people'), 811)]\n",
      "24/05/20 18:33:14 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('external', 'link'), 4890), (('19th', 'century'), 1741), (('same', 'time'), 1665), (('civil', 'war'), 1352), (('large', 'number'), 1282), (('other', 'hand'), 1228), (('political', 'party'), 1149), (('recent', 'year'), 1129), (('other', 'country'), 1095), (('many', 'people'), 1015)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:33:25 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[(('external', 'link'), 5727), (('19th', 'century'), 2027), (('same', 'time'), 1937), (('civil', 'war'), 1561), (('large', 'number'), 1498), (('other', 'hand'), 1443), (('political', 'party'), 1337), (('other', 'country'), 1301), (('recent', 'year'), 1283), (('many', 'people'), 1207)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:33:32 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 18:33:32 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[(('external', 'link'), 6503), (('19th', 'century'), 2328), (('same', 'time'), 2221), (('civil', 'war'), 1775), (('large', 'number'), 1688), (('other', 'hand'), 1654), (('political', 'party'), 1537), (('other', 'country'), 1463), (('recent', 'year'), 1452), (('many', 'people'), 1399)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 88:====>                                                   (1 + 11) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:33:37 WARN BatchedWriteAheadLog: BatchedWriteAheadLog Writer queue interrupted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 88:===================================================>    (11 + 1) / 12]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:33:38 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[(('external', 'link'), 7340), (('19th', 'century'), 2595), (('same', 'time'), 2482), (('civil', 'war'), 2005), (('large', 'number'), 1895), (('other', 'hand'), 1838), (('political', 'party'), 1712), (('other', 'country'), 1662), (('recent', 'year'), 1640), (('many', 'people'), 1564)]\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "def updateFunc(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    return sum(newValues, runningCount)\n",
    "    # add the new values with the previous running count to get the new count\n",
    "    \n",
    "running_counts = lines.map(lambda word: (word, 1)).updateStateByKey(updateFunc)\n",
    "\n",
    "counts_sorted = running_counts.map(lambda x: (x[0][1], x)) \\\n",
    "                .reduceByKey(lambda x,y: x if x[1] > y[1] else y) \\\n",
    "                .map(lambda x: x[1]) \\\n",
    "                .transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n",
    "\n",
    "def printResult(rdd):\n",
    "    print(rdd.take(10))\n",
    "    \n",
    "counts_sorted.foreachRDD(printResult)\n",
    "\n",
    "ssc.start()  # Start the computation\n",
    "print(\"Start\")\n",
    "ssc.awaitTermination(50)  # Wait for the computation to terminate\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4966cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
