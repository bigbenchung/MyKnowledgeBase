{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2f92724",
   "metadata": {},
   "source": [
    "### K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a846c48c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final centers:  [array([0.82604552, 0.5360665 ]), array([0.3917835 , 0.17797951]), array([0.31331048, 0.70637383])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def parseVector(line):\n",
    "    return np.array([float(x) for x in line.split()])\n",
    "\n",
    "def closestPoint(p, centers):\n",
    "    bestIndex = 0\n",
    "    closest = float(\"+inf\")\n",
    "    for i in range(len(centers)):\n",
    "        tempDist = np.sum((p - centers[i]) ** 2)\n",
    "        if tempDist < closest:\n",
    "            closest = tempDist\n",
    "            bestIndex = i\n",
    "    return bestIndex\n",
    "\n",
    "lines = sc.textFile('data/kmeans_bigdata.txt', 5)  \n",
    "\n",
    "# The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/kmeans_bigdata.txt\n",
    "# lines = sc.textFile('../data/kmeans_bigdata.txt', 5)  \n",
    "# lines is an RDD of strings\n",
    "K = 3\n",
    "convergeDist = 0.01  \n",
    "# terminate algorithm when the total distance from old center to new centers is less than this value\n",
    "\n",
    "data = lines.map(parseVector).cache() # data is an RDD of arrays\n",
    "\n",
    "kCenters = data.takeSample(False, K, 1)  # intial centers as a list of arrays\n",
    "tempDist = 1.0  # total distance from old centers to new centers\n",
    "\n",
    "while tempDist > convergeDist:\n",
    "    closest = data.map(lambda p: (closestPoint(p, kCenters), (p, 1)))\n",
    "    # for each point in data, find its closest center\n",
    "    # closest is an RDD of tuples (index of closest center, (point, 1))\n",
    "        \n",
    "    pointStats = closest.reduceByKey(lambda p1, p2: (p1[0] + p2[0], p1[1] + p2[1]))\n",
    "    # pointStats is an RDD of tuples (index of center,\n",
    "    # (array of sums of coordinates, total number of points assigned))\n",
    "    \n",
    "    newCenters = pointStats.map(lambda st: (st[0], st[1][0] / st[1][1])).collect()\n",
    "    # compute the new centers\n",
    "    \n",
    "    tempDist = sum(np.sum((kCenters[i] - p) ** 2) for (i, p) in newCenters)\n",
    "    # compute the total disctance from old centers to new centers\n",
    "    \n",
    "    for (i, p) in newCenters:\n",
    "        kCenters[i] = p\n",
    "        \n",
    "print(\"Final centers: \", kCenters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46701ff6",
   "metadata": {},
   "source": [
    "### PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ea0851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "links: [('1', (<pyspark.resultiterable.ResultIterable object at 0x7f552a032520>, 1.0)), ('4', (<pyspark.resultiterable.ResultIterable object at 0x7f5529f3b160>, 1.0)), ('2', (<pyspark.resultiterable.ResultIterable object at 0x7f5529f3b190>, 1.0)), ('3', (<pyspark.resultiterable.ResultIterable object at 0x7f5529f3b1c0>, 1.0))]\n",
      "rank: [('1', 1.4249999999999998), ('4', 1.0), ('2', 0.575), ('3', 1.0)]\n",
      "links: [('1', (<pyspark.resultiterable.ResultIterable object at 0x7f552a032670>, 1.4249999999999998)), ('4', (<pyspark.resultiterable.ResultIterable object at 0x7f5529ec1790>, 1.0)), ('2', (<pyspark.resultiterable.ResultIterable object at 0x7f5529ec1940>, 0.575)), ('3', (<pyspark.resultiterable.ResultIterable object at 0x7f5529ec15b0>, 1.0))]\n",
      "rank: [('1', 1.244375), ('4', 1.0), ('2', 0.7556249999999999), ('3', 0.9999999999999999)]\n",
      "links: [('1', (<pyspark.resultiterable.ResultIterable object at 0x7f5529f2d8e0>, 1.244375)), ('4', (<pyspark.resultiterable.ResultIterable object at 0x7f5529ec7520>, 1.0)), ('2', (<pyspark.resultiterable.ResultIterable object at 0x7f5529ec7640>, 0.7556249999999999)), ('3', (<pyspark.resultiterable.ResultIterable object at 0x7f5529ec7670>, 0.9999999999999999))]\n",
      "rank: [('1', 1.3211406249999997), ('4', 0.9999999999999999), ('2', 0.678859375), ('3', 1.0)]\n",
      "links: [('1', (<pyspark.resultiterable.ResultIterable object at 0x7f5529f3bbe0>, 1.3211406249999997)), ('4', (<pyspark.resultiterable.ResultIterable object at 0x7f5529f21520>, 0.9999999999999999)), ('2', (<pyspark.resultiterable.ResultIterable object at 0x7f5529f21b50>, 0.678859375)), ('3', (<pyspark.resultiterable.ResultIterable object at 0x7f552a00e610>, 1.0))]\n",
      "rank: [('1', 1.288515234375), ('4', 1.0), ('2', 0.7114847656249998), ('3', 0.9999999999999999)]\n",
      "links: [('1', (<pyspark.resultiterable.ResultIterable object at 0x7f5529f156d0>, 1.288515234375)), ('4', (<pyspark.resultiterable.ResultIterable object at 0x7f5529ec73a0>, 1.0)), ('2', (<pyspark.resultiterable.ResultIterable object at 0x7f5529ecb340>, 0.7114847656249998)), ('3', (<pyspark.resultiterable.ResultIterable object at 0x7f5529ecb040>, 0.9999999999999999))]\n",
      "rank: [('1', 1.3023810253906247), ('4', 0.9999999999999999), ('2', 0.6976189746093749), ('3', 0.9999999999999999)]\n",
      "links: [('1', (<pyspark.resultiterable.ResultIterable object at 0x7f5529ecbbe0>, 1.3023810253906247)), ('4', (<pyspark.resultiterable.ResultIterable object at 0x7f5529f2dac0>, 0.9999999999999999)), ('2', (<pyspark.resultiterable.ResultIterable object at 0x7f5529f2d1f0>, 0.6976189746093749)), ('3', (<pyspark.resultiterable.ResultIterable object at 0x7f5529f2d3d0>, 0.9999999999999999))]\n",
      "rank: [('1', 1.2964880642089842), ('4', 0.9999999999999999), ('2', 0.7035119357910156), ('3', 0.9999999999999998)]\n",
      "links: [('1', (<pyspark.resultiterable.ResultIterable object at 0x7f5529ecc3a0>, 1.2964880642089842)), ('4', (<pyspark.resultiterable.ResultIterable object at 0x7f5529ec7130>, 0.9999999999999999)), ('2', (<pyspark.resultiterable.ResultIterable object at 0x7f5529ec7af0>, 0.7035119357910156)), ('3', (<pyspark.resultiterable.ResultIterable object at 0x7f5529ec79d0>, 0.9999999999999998))]\n",
      "rank: [('1', 1.2989925727111813), ('4', 0.9999999999999998), ('2', 0.7010074272888183), ('3', 0.9999999999999999)]\n",
      "links: [('1', (<pyspark.resultiterable.ResultIterable object at 0x7f5529ecf4c0>, 1.2989925727111813)), ('4', (<pyspark.resultiterable.ResultIterable object at 0x7f5529f2d610>, 0.9999999999999998)), ('2', (<pyspark.resultiterable.ResultIterable object at 0x7f5529f2d1f0>, 0.7010074272888183)), ('3', (<pyspark.resultiterable.ResultIterable object at 0x7f5529f2d160>, 0.9999999999999999))]\n",
      "rank: [('1', 1.2979281565977474), ('4', 0.9999999999999999), ('2', 0.702071843402252), ('3', 0.9999999999999998)]\n",
      "links: [('1', (<pyspark.resultiterable.ResultIterable object at 0x7f5529ed2340>, 1.2979281565977474)), ('4', (<pyspark.resultiterable.ResultIterable object at 0x7f5529ecb340>, 0.9999999999999999)), ('2', (<pyspark.resultiterable.ResultIterable object at 0x7f5529ecba60>, 0.702071843402252)), ('3', (<pyspark.resultiterable.ResultIterable object at 0x7f5529ecbdf0>, 0.9999999999999998))]\n",
      "rank: [('1', 1.2983805334459568), ('4', 0.9999999999999998), ('2', 0.7016194665540426), ('3', 0.9999999999999998)]\n",
      "links: [('1', (<pyspark.resultiterable.ResultIterable object at 0x7f5529ed73d0>, 1.2983805334459568)), ('4', (<pyspark.resultiterable.ResultIterable object at 0x7f5529eccfa0>, 0.9999999999999998)), ('2', (<pyspark.resultiterable.ResultIterable object at 0x7f5529ecc430>, 0.7016194665540426)), ('3', (<pyspark.resultiterable.ResultIterable object at 0x7f5529ecca60>, 0.9999999999999998))]\n",
      "rank: [('1', 1.2981882732854677), ('4', 0.9999999999999998), ('2', 0.7018117267145316), ('3', 0.9999999999999998)]\n",
      "[('1', 1.2981882732854677), ('4', 0.9999999999999998), ('3', 0.9999999999999998), ('2', 0.7018117267145316)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from operator import add\n",
    "\n",
    "def computeContribs(urls, rank):\n",
    "    # Calculates URL contributions to the rank of other URLs.\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n",
    "\n",
    "def parseNeighbors(urls):\n",
    "    # Parses a urls pair string into urls pair.\"\"\"\n",
    "    parts = urls.split(' ')\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "# Loads in input file. It should be in format of:\n",
    "#     URL         neighbor URL\n",
    "#     URL         neighbor URL\n",
    "#     URL         neighbor URL\n",
    "#     ...\n",
    "\n",
    "lines = sc.textFile(\"data/pagerank_data.txt\", 2)\n",
    "# lines = sc.textFile(\"../data/dblp.in\", 5)\n",
    "\n",
    "numOfIterations = 10\n",
    "\n",
    "# Loads all URLs from input file and initialize their neighbors. ''\n",
    "links = lines.map(lambda urls: parseNeighbors(urls)) \\\n",
    "             .groupByKey()\n",
    "\n",
    "# Loads all URLs with other URL(s) link to from input file \n",
    "# and initialize ranks of them to one.\n",
    "ranks = links.mapValues(lambda neighbors: 1.0)\n",
    "\n",
    "# Calculates and updates URL ranks continuously using PageRank algorithm.\n",
    "for iteration in range(numOfIterations):\n",
    "    # Calculates URL contributions to the rank of other URLs.\n",
    "    contribs = links.join(ranks) \\\n",
    "                    .flatMap(lambda url_urls_rank:\n",
    "                             computeContribs(url_urls_rank[1][0],\n",
    "                                             url_urls_rank[1][1]))\n",
    "    # After the join, each element in the RDD is of the form\n",
    "    # (url, (list of neighbor urls, rank))\n",
    "    print(\"links:\", links.join(ranks).collect())\n",
    "    # Re-calculates URL ranks based on neighbor contributions.\n",
    "    ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)\n",
    "    # ranks = contribs.reduceByKey(add).map(lambda t: (t[0], t[1] * 0.85 + 0.15))\n",
    "    print(\"rank:\", ranks.collect())\n",
    "print(ranks.top(5, lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f04a71e",
   "metadata": {},
   "source": [
    "### PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d2797ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile('data/adj_noun_pairs.txt', 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc9fcd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 166:=================================================>       (7 + 1) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3162692"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a410a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['early radical',\n",
       " 'french revolution',\n",
       " 'pejorative way',\n",
       " 'violent means',\n",
       " 'positive label']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b45fc9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[161] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting lines into word pairs. \n",
    "# Data is dirty: some lines have more than 2 words, so filter them out.\n",
    "pairs = lines.map(lambda l: tuple(l.split())).filter(lambda p: len(p)==2)\n",
    "pairs.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45716374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 168:==========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3162674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "N = pairs.count()\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bb73102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('16th', 'century'), 950),\n",
       " (('civil', 'war'), 2236),\n",
       " (('social', 'class'), 155),\n",
       " (('sixteenth', 'century'), 137),\n",
       " (('late', '1970'), 444)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the frequency of each pair.\n",
    "# Ignore pairs that not frequent enough\n",
    "pair_freqs = pairs.map(lambda p: (p,1)).reduceByKey(lambda f1, f2: f1 + f2)\\\n",
    "                  .filter(lambda pf: pf[1] >= 100)\n",
    "pair_freqs.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8aed0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('revolution', 2305),\n",
       " ('wealth', 363),\n",
       " ('idea', 2852),\n",
       " ('anarchism', 67),\n",
       " ('something', 744)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing the frequencies of the adjectives and the nouns\n",
    "a_freqs = pairs.map(lambda p: (p[0],1)).reduceByKey(lambda x,y: x+y)\n",
    "n_freqs = pairs.map(lambda p: (p[1],1)).reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "n_freqs.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d8e2582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104304\n",
      "2012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 178:==========================================>              (6 + 2) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(n_freqs.count())\n",
    "print(a_freqs.count())\n",
    "print(pair_freqs.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37805c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1191"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Broadcasting the adjective and noun frequencies. \n",
    "#a_dict = a_freqs.collectAsMap()\n",
    "#a_dict = sc.parallelize(a_dict).map(lambda x: x)\n",
    "#a_dict.value['violent']\n",
    "n_dict = n_freqs.collectAsMap()\n",
    "a_dict = a_freqs.collectAsMap()\n",
    "a_dict['violent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "832a2dca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(14.41018838546462, ('magna', 'carta')),\n",
       " (13.071365888694997, ('polish-lithuanian', 'Commonwealth')),\n",
       " (12.990597616733414, ('nitrous', 'oxide')),\n",
       " (12.64972604311254, ('latter-day', 'Saints')),\n",
       " (12.50658937509916, ('stainless', 'steel')),\n",
       " (12.482331020687814, ('pave', 'runway')),\n",
       " (12.19140721768055, ('corporal', 'punishment')),\n",
       " (12.183248694293388, ('capital', 'punishment')),\n",
       " (12.147015483562537, ('rush', 'yard')),\n",
       " (12.109945794428935, ('globular', 'cluster'))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import *\n",
    "\n",
    "# Computing the PMI for a pair.\n",
    "def pmi_score(pair_freq):\n",
    "    w1, w2 = pair_freq[0]\n",
    "    f = pair_freq[1]\n",
    "    pmi = log(float(f)*N/(a_dict[w1]*n_dict[w2]), 2)\n",
    "    return pmi, (w1, w2)\n",
    "\n",
    "# Computing the PMI for all pairs.\n",
    "scored_pairs = pair_freqs.map(pmi_score)\n",
    "\n",
    "# Printing the most strongly associated pairs. \n",
    "scored_pairs.top(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c353553",
   "metadata": {},
   "source": [
    "# Divide-and-Conqueror"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed0942f",
   "metadata": {},
   "source": [
    "### Prefix Sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f9cbd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4], [3, 5], [6, 7], [0, 1]]\n",
      "[5, 8, 13, 1]\n",
      "[5, 13, 26, 27]\n",
      "[1, 5, 8, 13, 19, 26, 26, 27]\n"
     ]
    }
   ],
   "source": [
    "x = [1, 4, 3, 5, 6, 7, 0, 1]\n",
    "\n",
    "rdd = sc.parallelize(x, 4).cache()\n",
    "\n",
    "print(rdd.glom().collect())\n",
    "\n",
    "def f(iterator):\n",
    "    yield sum(iterator)\n",
    "\n",
    "sums = rdd.mapPartitions(f).collect()\n",
    "\n",
    "print(sums)\n",
    "\n",
    "for i in range(1, len(sums)):\n",
    "    sums[i] += sums[i-1]\n",
    "\n",
    "print(sums)\n",
    "\n",
    "def g(index, iterator):\n",
    "    global sums\n",
    "    if index == 0:\n",
    "        s = 0\n",
    "    else:\n",
    "        s = sums[index-1]\n",
    "    for i in iterator:\n",
    "        s += i\n",
    "        yield s\n",
    "\n",
    "prefix_sums = rdd.mapPartitionsWithIndex(g)\n",
    "print(prefix_sums.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389f2166",
   "metadata": {},
   "source": [
    "### Monotonicity checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59da64be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 4, 5], [7, 3, 10, 14], [16, 20, 21, 24], [24, 26, 27, 30]]\n",
      "[(True, 1, 5), (False, 7, 14), (True, 16, 24), (True, 24, 30)]\n",
      "Not monotone\n"
     ]
    }
   ],
   "source": [
    "x = [1, 3, 4, 5, 7, 3, 10, 14, 16, 20, 21, 24, 24, 26, 27, 30]\n",
    "\n",
    "rdd = sc.parallelize(x, 4).cache()\n",
    "print(rdd.glom().collect())\n",
    "\n",
    "def f(it):\n",
    "    first = next(it)\n",
    "    last = first\n",
    "    increasing = True\n",
    "    for i in it:\n",
    "        if i < last:\n",
    "            increasing = False\n",
    "        last = i\n",
    "    yield increasing, first, last\n",
    "\n",
    "results = rdd.mapPartitions(f).collect()\n",
    "\n",
    "print(results)\n",
    "\n",
    "increasing = True\n",
    "if results[0][0] == False:\n",
    "    increasing = False\n",
    "else:\n",
    "    for i in range(1, len(results)):\n",
    "        if results[i][0] == False or results[i][1] < results[i-1][2]:\n",
    "            increasing = False\n",
    "if increasing:\n",
    "    print(\"Monotone\")\n",
    "else:\n",
    "    print(\"Not monotone\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5913995",
   "metadata": {},
   "source": [
    "### 2D Maxima\n",
    "The sequential algorithm: https://en.wikipedia.org/wiki/Maxima_of_a_point_set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c8793f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(99990, 53983), (99977, 59457), (99958, 59581), (99956, 79605), (99885, 83645), (99827, 89705), (99740, 98217), (99662, 98973), (96815, 99398), (96167, 99795), (92693, 99960), (92580, 99974), (59920, 99992), (45834, 100000)]\n"
     ]
    }
   ],
   "source": [
    "numPartitions = 10\n",
    "\n",
    "points = sc.textFile('data/points.txt',numPartitions)\n",
    "pairs = points.map(lambda l: tuple(l.split()))\n",
    "pairs = pairs.map(lambda pair: (int(pair[0]),int(pair[1]))).sortByKey(False)\n",
    "\n",
    "def findmaxY(l):\n",
    "    m = 0\n",
    "    for x in l:\n",
    "        if x[1] > m:\n",
    "            m = x[1]\n",
    "    yield m\n",
    "\n",
    "pairs.cache()\n",
    "maxY = pairs.mapPartitions(findmaxY).collect()\n",
    "for i in range(1, len(maxY)):\n",
    "    if maxY[i-1] > maxY[i]:\n",
    "        maxY[i] = maxY[i-1]\n",
    "\n",
    "def findmaxima(i, l):\n",
    "    if i == 0:\n",
    "        m = 0\n",
    "    else:\n",
    "        m = maxY[i-1]\n",
    "    for x in l:\n",
    "        if x[1] > m:\n",
    "            m = x[1]\n",
    "            yield x\n",
    "\n",
    "maxima = pairs.mapPartitionsWithIndex(findmaxima)\n",
    "            \n",
    "print(maxima.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9984fe41",
   "metadata": {},
   "source": [
    "### Maximum Subarray Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da03ed55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# Classical divide and conquer algorithm\n",
    "\n",
    "A = [-3, 2, 1, -4, 5, 2, -1, 3, -1]\n",
    "\n",
    "def MaxSubarray(A, p, r):\n",
    "    if p == r:\n",
    "        return A[p]\n",
    "    q = (p+r)//2\n",
    "    M1 = MaxSubarray(A, p, q)\n",
    "    M2 = MaxSubarray(A, q+1, r)\n",
    "    Lm = -float('inf')\n",
    "    Rm = Lm\n",
    "    V = 0\n",
    "    for i in range(q, p-1, -1):\n",
    "        V += A[i]\n",
    "        if V > Lm:\n",
    "            Lm = V\n",
    "    V = 0\n",
    "    for i in range(q+1, r+1):\n",
    "        V += A[i]\n",
    "        if V > Rm:\n",
    "            Rm = V\n",
    "    return max(M1, M2, Lm+Rm)\n",
    "\n",
    "print(MaxSubarray(A, 0, len(A)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b3e605d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# Linear-time algorithm\n",
    "# Written in a way so that we can call it for each partition\n",
    "\n",
    "def linear_time(it):\n",
    "    Vmax = -float('inf')\n",
    "    V = 0\n",
    "    for Ai in it:\n",
    "        V += Ai\n",
    "        if V < Ai:\n",
    "            V = Ai\n",
    "        if V > Vmax:\n",
    "            Vmax = V\n",
    "    yield Vmax\n",
    "    \n",
    "print(next(linear_time(A)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db934a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, -3, 7, 1]\n",
      "[(2, -1), (0, 1), (7, 7), (2, 2)]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# The Spark algorithm:\n",
    "\n",
    "def compute_sum(it):\n",
    "    yield sum(it)\n",
    "\n",
    "def compute_LmRm(index, it):\n",
    "    Rm = -float('inf')\n",
    "    L = sums[index]\n",
    "    Lm = L\n",
    "    R = 0\n",
    "    for Ai in it:\n",
    "        L -= Ai\n",
    "        R += Ai\n",
    "        if L > Lm:\n",
    "            Lm = L\n",
    "        if R > Rm:\n",
    "            Rm = R\n",
    "    yield (Lm, Rm)\n",
    "\n",
    "num_partitions = 4\n",
    "rdd = sc.parallelize(A, num_partitions).cache()\n",
    "sums = rdd.mapPartitions(compute_sum).collect()\n",
    "print(sums)\n",
    "LmRms = rdd.mapPartitionsWithIndex(compute_LmRm).collect()\n",
    "print(LmRms)\n",
    "best = max(rdd.mapPartitions(linear_time).collect())\n",
    "\n",
    "for i in range(num_partitions-1):\n",
    "    for j in range(i+1, num_partitions):\n",
    "        x = LmRms[i][0] + sum(sums[i+1:j]) + LmRms[j][1]\n",
    "        if x > best:\n",
    "            best = x\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf255b88",
   "metadata": {},
   "source": [
    "### Sample Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0ebd623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2046, 2561, 3206, 3720, 5499, 7527, 8887]\n",
      "[[2607, 7280, 3984, 7984, 2179, 5065, 6471, 5499, 5501, 3067, 9834, 9147], [605, 2641, 7432, 516, 4491, 3240, 1716, 5578, 2617, 9481, 4989, 6655], [941, 7795, 4256, 2138, 674, 2175, 2919, 6353, 3480, 5562, 8033, 9292], [8887, 9460, 3100, 854, 8748, 4515, 8579, 9374, 4619, 8693, 5079, 3836], [1875, 5427, 3206, 6999, 2561, 9359, 7241, 8352, 2710, 8036, 167, 2046], [4484, 1062, 9319, 5382, 2406, 559, 4488, 6605, 8460, 542, 9387, 4394], [8874, 8660, 974, 3335, 2256, 2519, 3655, 1208, 2208, 3275, 5284, 3370], [5247, 4730, 1527, 2621, 2300, 3720, 5336, 6032, 8763, 9500, 7527, 4558, 7101, 5720, 6061, 9598]]\n",
      "[[(0, []), (1, [2179]), (2, [2607, 3067]), (3, []), (4, [3984, 5065, 5499]), (5, [7280, 6471, 5501]), (6, [7984]), (7, [9834, 9147])], [(0, [605, 516, 1716]), (1, []), (2, [2641, 2617]), (3, [3240]), (4, [4491, 4989]), (5, [7432, 5578, 6655]), (6, []), (7, [9481])], [(0, [941, 674]), (1, [2138, 2175]), (2, [2919]), (3, [3480]), (4, [4256]), (5, [6353, 5562]), (6, [7795, 8033]), (7, [9292])], [(0, [854]), (1, []), (2, [3100]), (3, []), (4, [4515, 4619, 5079, 3836]), (5, []), (6, [8887, 8748, 8579, 8693]), (7, [9460, 9374])], [(0, [1875, 167, 2046]), (1, [2561]), (2, [3206, 2710]), (3, []), (4, [5427]), (5, [6999, 7241]), (6, [8352, 8036]), (7, [9359])], [(0, [1062, 559, 542]), (1, [2406]), (2, []), (3, []), (4, [4484, 5382, 4488, 4394]), (5, [6605]), (6, [8460]), (7, [9319, 9387])], [(0, [974, 1208]), (1, [2256, 2519, 2208]), (2, []), (3, [3335, 3655, 3275, 3370]), (4, [5284]), (5, []), (6, [8874, 8660]), (7, [])], [(0, [1527]), (1, [2300]), (2, [2621]), (3, [3720]), (4, [5247, 4730, 5336, 4558]), (5, [6032, 7527, 7101, 5720, 6061]), (6, [8763]), (7, [9500, 9598])]]\n",
      "[[167, 516, 542, 559, 605, 674, 854, 941, 974, 1062, 1208, 1527, 1716, 1875, 2046], [2138, 2175, 2179, 2208, 2256, 2300, 2406, 2519, 2561], [2607, 2617, 2621, 2641, 2710, 2919, 3067, 3100, 3206], [3240, 3275, 3335, 3370, 3480, 3655, 3720], [3836, 3984, 4256, 4394, 4484, 4488, 4491, 4515, 4558, 4619, 4730, 4989, 5065, 5079, 5247, 5284, 5336, 5382, 5427, 5499], [5501, 5562, 5578, 5720, 6032, 6061, 6353, 6471, 6605, 6655, 6999, 7101, 7241, 7280, 7432, 7527], [7795, 7984, 8033, 8036, 8352, 8460, 8579, 8660, 8693, 8748, 8763, 8874, 8887], [9147, 9292, 9319, 9359, 9374, 9387, 9460, 9481, 9500, 9598, 9834]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "n = 100 # size of RDD to be sorted\n",
    "p = 8   # number of partitions/workers\n",
    "s = 3  # sample size factor \n",
    "\n",
    "r = sc.parallelize([random.randrange(1, n*100, 1) for i in range(n)], p)\n",
    "\n",
    "sample = r.takeSample(False, s*p)\n",
    "sample.sort()\n",
    "splitters = [0] + sample[s::s] \n",
    "print(splitters)\n",
    "\n",
    "def findBucket(par):\n",
    "    buckets = [[] for i in range(p)]\n",
    "    for x in par:\n",
    "        # Do a binary search\n",
    "        l, r = 0, p-1\n",
    "        while l < r:\n",
    "            mid = (l+r+1)//2\n",
    "            if splitters[mid] < x:\n",
    "                l = mid\n",
    "            else:\n",
    "                r = mid-1\n",
    "        buckets[l].append(x)\n",
    "    for i in range(p):\n",
    "        yield (i, buckets[i])\n",
    "\n",
    "def sortBucket(b):\n",
    "    l = []\n",
    "    for x in b[1]:\n",
    "        l += x\n",
    "    l.sort()\n",
    "    return l\n",
    "        \n",
    "r1 = r.mapPartitions(findBucket)\n",
    "r2 = r1.groupByKey()\n",
    "r3 = r2.flatMap(sortBucket)\n",
    "\n",
    "print(r.glom().collect())\n",
    "print(r1.glom().collect())\n",
    "print(r3.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b5b017",
   "metadata": {},
   "source": [
    "### String lexicographic Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8344cc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<\n"
     ]
    }
   ],
   "source": [
    "x = 'abcccbcbcacaccacaabb'\n",
    "y = 'abcccbcccacaccacaabb'\n",
    "\n",
    "numPartitions = 4\n",
    "rdd = sc.parallelize(zip(x,y), numPartitions)\n",
    "\n",
    "def compareStrings(string_pairs):\n",
    "    for x, y in string_pairs:\n",
    "        if x < y:\n",
    "            return '<'\n",
    "        elif x > y:\n",
    "            return '>'\n",
    "    return '='\n",
    "\n",
    "comparisons = rdd.mapPartitions(compareStrings).filter(lambda sign: sign != '=')\n",
    "if not comparisons.isEmpty():\n",
    "    result = comparisons.first()\n",
    "else:\n",
    "    result = \"=\"\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb73cc61",
   "metadata": {},
   "source": [
    "### Find the first (adj, noun) pair in which the noun is 'unification'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ccd9bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 210:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/19 18:27:06 WARN BlockManager: Task 382 already completed, not releasing lock for rdd_222_1\n",
      "24/05/19 18:27:06 WARN BlockManager: Task 384 already completed, not releasing lock for rdd_222_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 210:============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/19 18:27:07 WARN BlockManager: Task 383 already completed, not releasing lock for rdd_222_2\n",
      "24/05/19 18:27:07 WARN BlockManager: Task 385 already completed, not releasing lock for rdd_222_4\n",
      "several\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "numPartitions = 10\n",
    "path_to_file = \"data/adj_noun_pairs.txt\"\n",
    "\n",
    "lines = sc.textFile(path_to_file, numPartitions)\n",
    "pairs = lines.map(lambda l: tuple(l.split())).filter(lambda p: len(p)==2)\n",
    "pairs.cache()\n",
    "\n",
    "def returnFirst(pairs):\n",
    "    txt = 'unification'\n",
    "    for pair in pairs:\n",
    "        adj, noun = pair\n",
    "        if noun == txt:\n",
    "            return [adj]\n",
    "    return \"\"\n",
    "\n",
    "first_adj = pairs.mapPartitions(returnFirst).filter(lambda x: x != \"\").first()\n",
    "print(first_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff7a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
