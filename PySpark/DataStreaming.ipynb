{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b158d4c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2024-05-19 22:18:40\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2024-05-19 22:18:40\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2024-05-19 22:18:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2024-05-19 22:18:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/19 22:18:47 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/05/19 22:18:47 WARN BlockManager: Block input-0-1716128327400 replicated to only 0 peer(s) instead of 1 peers\n",
      "24/05/19 22:18:49 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/05/19 22:18:49 WARN BlockManager: Block input-0-1716128328800 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2024-05-19 22:18:50\n",
      "-------------------------------------------\n",
      "ak p\n",
      "ap k\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-05-19 22:18:50\n",
      "-------------------------------------------\n",
      "('p', 1)\n",
      "('ap', 1)\n",
      "('ak', 1)\n",
      "('k', 1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2024-05-19 22:18:55\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2024-05-19 22:18:55\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/19 22:18:58 WARN SocketReceiver: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.net.SocketInputStream.socketRead(SocketInputStream.java:116)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:171)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n",
      "\tat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.io.InputStreamReader.read(InputStreamReader.java:184)\n",
      "\tat java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:324)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:389)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "24/05/19 22:18:58 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n",
      "24/05/19 22:18:58 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.net.SocketInputStream.socketRead(SocketInputStream.java:116)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:171)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n",
      "\tat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.io.InputStreamReader.read(InputStreamReader.java:184)\n",
      "\tat java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:324)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:389)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "24/05/19 22:18:58 WARN ReceiverSupervisorImpl: Receiver has been stopped\n",
      "Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"receiver-supervisor-future-0\" java.lang.Error: java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1155)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.lang.Thread.sleep(Native Method)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:196)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\t... 2 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a StreamingContext with batch interval of 5 seconds\n",
    "ssc = StreamingContext(sc, 5)\n",
    "\n",
    "# Create a DStream that will connect to localhost at port 9999\n",
    "# Start Netcat server: nc -lk 9999 \n",
    "lines = ssc.socketTextStream('localhost', 9999)\n",
    "\n",
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "lines.pprint()\n",
    "wordCounts.pprint()\n",
    "\n",
    "ssc.start()  # Start the computation\n",
    "print(\"Start\")\n",
    "ssc.awaitTermination(20)  # Wait for the computation to terminate\n",
    "ssc.stop(stopSparkContext=False)  # Stop the StreamingContext without stopping the SparkContext\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f71d236",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2024-04-15 19:57:30\n",
      "-------------------------------------------\n",
      "('other', 15486)\n",
      "('first', 10815)\n",
      "('many', 9773)\n",
      "('new', 6272)\n",
      "('system', 5063)\n",
      "('american', 4744)\n",
      "('several', 4545)\n",
      "('century', 4492)\n",
      "('same', 4394)\n",
      "('=', 4313)\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2024-04-15 19:57:35\n",
      "-------------------------------------------\n",
      "('other', 15319)\n",
      "('first', 10709)\n",
      "('many', 9575)\n",
      "('new', 6242)\n",
      "('system', 5111)\n",
      "('american', 4777)\n",
      "('century', 4538)\n",
      "('several', 4515)\n",
      "('same', 4453)\n",
      "('=', 4361)\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2024-04-15 19:57:40\n",
      "-------------------------------------------\n",
      "('other', 15346)\n",
      "('first', 10517)\n",
      "('many', 9706)\n",
      "('new', 6218)\n",
      "('system', 5266)\n",
      "('american', 4940)\n",
      "('several', 4615)\n",
      "('=', 4451)\n",
      "('century', 4438)\n",
      "('same', 4270)\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2024-04-15 19:57:45\n",
      "-------------------------------------------\n",
      "('other', 15196)\n",
      "('first', 10617)\n",
      "('many', 9848)\n",
      "('new', 6289)\n",
      "('system', 5093)\n",
      "('american', 4824)\n",
      "('several', 4519)\n",
      "('century', 4493)\n",
      "('=', 4332)\n",
      "('same', 4260)\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2024-04-15 19:57:50\n",
      "-------------------------------------------\n",
      "('other', 15091)\n",
      "('first', 10463)\n",
      "('many', 9703)\n",
      "('new', 6175)\n",
      "('system', 5102)\n",
      "('american', 4829)\n",
      "('several', 4523)\n",
      "('century', 4520)\n",
      "('=', 4369)\n",
      "('same', 4325)\n",
      "...\n",
      "\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a queue of RDDs\n",
    "rdd = sc.textFile('../data/adj_noun_pairs.txt', 8)\n",
    "\n",
    "# split the rdd into 5 equal-size parts\n",
    "rddQueue = rdd.randomSplit([1,1,1,1,1], 123)\n",
    "        \n",
    "# Create a StreamingContext with batch interval of 5 seconds\n",
    "ssc = StreamingContext(sc, 5)\n",
    "\n",
    "# Feed the rdd queue to a DStream\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "# Do word-counting as before\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Use transform() to access any rdd transformations not directly available in SparkStreaming\n",
    "topWords = wordCounts.transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n",
    "topWords.pprint()\n",
    "\n",
    "ssc.start()  # Start the computation\n",
    "ssc.awaitTermination(25)  # Wait for the computation to terminate\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8883426c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2024-04-15 20:02:50\n",
      "-------------------------------------------\n",
      "(7890.0, 'great')\n",
      "(6180.0, 'popular')\n",
      "(5544.0, 'best')\n",
      "(4662.0, 'good')\n",
      "(4242.0, 'important')\n",
      "(2340.0, 'strong')\n",
      "(2322.0, 'greater')\n",
      "(2058.0, 'successful')\n",
      "(1850.0, 'novel')\n",
      "(1790.0, 'natural')\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2024-04-15 20:02:55\n",
      "-------------------------------------------\n",
      "(7578.0, 'great')\n",
      "(6126.0, 'popular')\n",
      "(5604.0, 'best')\n",
      "(4749.0, 'good')\n",
      "(4172.0, 'important')\n",
      "(2253.0, 'greater')\n",
      "(2252.0, 'strong')\n",
      "(2001.0, 'successful')\n",
      "(1948.0, 'novel')\n",
      "(1804.0, 'natural')\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2024-04-15 20:03:00\n",
      "-------------------------------------------\n",
      "(7893.0, 'great')\n",
      "(6009.0, 'popular')\n",
      "(5574.0, 'best')\n",
      "(4677.0, 'good')\n",
      "(4298.0, 'important')\n",
      "(2326.0, 'strong')\n",
      "(2172.0, 'greater')\n",
      "(1944.0, 'successful')\n",
      "(1868.0, 'novel')\n",
      "(1761.0, 'natural')\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2024-04-15 20:03:05\n",
      "-------------------------------------------\n",
      "(7776.0, 'great')\n",
      "(5925.0, 'popular')\n",
      "(5538.0, 'best')\n",
      "(4671.0, 'good')\n",
      "(4156.0, 'important')\n",
      "(2362.0, 'strong')\n",
      "(2205.0, 'greater')\n",
      "(1932.0, 'successful')\n",
      "(1892.0, 'novel')\n",
      "(1803.0, 'natural')\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2024-04-15 20:03:10\n",
      "-------------------------------------------\n",
      "(7566.0, 'great')\n",
      "(5916.0, 'popular')\n",
      "(5334.0, 'best')\n",
      "(4548.0, 'good')\n",
      "(4268.0, 'important')\n",
      "(2415.0, 'greater')\n",
      "(2314.0, 'strong')\n",
      "(2109.0, 'successful')\n",
      "(2018.0, 'novel')\n",
      "(1821.0, 'natural')\n",
      "...\n",
      "\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Find the most positive words in windows of 5 seconds from streaming data\n",
    "# URL for data: https://cse.hkust.edu.hk/msbd5003/data/AFINN-111.txt\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "def parse_line(l):\n",
    "    x = l.split(\"\\t\")\n",
    "    return (x[0], float(x[1]))\n",
    "\n",
    "word_sentiments = sc.textFile(\"../data/AFINN-111.txt\") \\\n",
    "                    .map(parse_line).cache()\n",
    "    \n",
    "ssc = StreamingContext(sc, 5)\n",
    "rdd = sc.textFile('../data/adj_noun_pairs.txt', 8)\n",
    "rddQueue = rdd.randomSplit([1,1,1,1,1], 123)\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "word_counts = lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "                   .map(lambda word: (word, 1)) \\\n",
    "                   .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Determine the words with the highest sentiment values by joining the streaming RDD\n",
    "# with the static RDD inside the transform() method and then multiplying\n",
    "# the frequency of the words by its sentiment value\n",
    "happiest_words = word_counts.transform(lambda rdd: word_sentiments.join(rdd)) \\\n",
    "                            .map(lambda t:\n",
    "                                 (t[1][0] * t[1][1], t[0])) \\\n",
    "                            .transform(lambda rdd: rdd.sortByKey(False))\n",
    "\n",
    "happiest_words.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination(25)\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "933d2c5e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  51430\n",
      "[('other', 7782), ('first', 5404), ('many', 4878), ('new', 3219), ('system', 2539)]\n",
      "refinery: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  76917\n",
      "[('other', 15486), ('first', 10815), ('many', 9773), ('new', 6272), ('system', 5063)]\n",
      "refinery: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  97338\n",
      "[('other', 23129), ('first', 16145), ('many', 14534), ('new', 9363), ('system', 7636)]\n",
      "refinery: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  114786\n",
      "[('other', 30805), ('first', 21524), ('many', 19348), ('new', 12514), ('system', 10174)]\n",
      "refinery: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  130393\n",
      "[('other', 38452), ('first', 26792), ('many', 24239), ('new', 15618), ('system', 12777)]\n",
      "refinery: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  144898\n",
      "[('other', 46151), ('first', 32041), ('many', 29054), ('new', 18732), ('system', 15440)]\n",
      "refinery: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  158013\n",
      "[('other', 53728), ('first', 37395), ('many', 33933), ('new', 21787), ('system', 17946)]\n",
      "refinery: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  170610\n",
      "[('other', 61347), ('first', 42658), ('many', 38902), ('new', 25021), ('system', 20533)]\n",
      "refinery: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  182330\n",
      "[('other', 68982), ('first', 47856), ('many', 43780), ('new', 28095), ('system', 23031)]\n",
      "refinery: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  193450\n",
      "[('other', 76438), ('first', 53121), ('many', 48605), ('new', 31196), ('system', 25635)]\n",
      "refinery: 40\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Stateful word count\n",
    "\n",
    "ssc = StreamingContext(sc, 5)\n",
    "# Provide a checkpointing directory.  Required for stateful transformations\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "rdd = sc.textFile('../data/adj_noun_pairs.txt', 8)\n",
    "rddQueue = rdd.randomSplit([1]*10, 123)\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "def updateFunc(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    return sum(newValues, runningCount)\n",
    "    # add the new values with the previous running count to get the new count\n",
    "\n",
    "running_counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "                      .map(lambda word: (word, 1))\\\n",
    "                      .reduceByKey(lambda a, b: a + b) \\\n",
    "                      .updateStateByKey(updateFunc)\n",
    "\n",
    "counts_sorted = running_counts.transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n",
    "\n",
    "def printResults(rdd):\n",
    "    print(\"Total distinct words: \", rdd.count())\n",
    "    print(rdd.take(5))\n",
    "    print('refinery:', rdd.lookup('refinery')[0])\n",
    "\n",
    "counts_sorted.foreachRDD(printResults)\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination(50)\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a55232c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  51430\n",
      "[('other', 7782, 7782), ('first', 5404, 5404), ('many', 4878, 4878), ('new', 3219, 3219), ('system', 2539, 2539)]\n",
      "refinery: 1 , 1\n",
      "Next threshold =  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  13971\n",
      "[('other', 15481, 15486), ('first', 10810, 10815), ('many', 9768, 9773), ('new', 6267, 6272), ('system', 5058, 5063)]\n",
      "refinery: 1 , 6\n",
      "Next threshold =  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  12164\n",
      "[('other', 23119, 23129), ('first', 16135, 16145), ('many', 14524, 14534), ('new', 9353, 9363), ('system', 7626, 7636)]\n",
      "refinery: 1 , 11\n",
      "Next threshold =  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  12317\n",
      "[('other', 30791, 30805), ('first', 21510, 21524), ('many', 19334, 19348), ('new', 12500, 12514), ('system', 10160, 10174)]\n",
      "refinery: 2 , 16\n",
      "Next threshold =  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  11650\n",
      "[('other', 38433, 38452), ('first', 26773, 26792), ('many', 24220, 24239), ('new', 15599, 15618), ('system', 12758, 12777)]\n",
      "refinery: 0 , 19\n",
      "Next threshold =  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  11396\n",
      "[('other', 46127, 46151), ('first', 32017, 32041), ('many', 29030, 29054), ('new', 18708, 18732), ('system', 15416, 15440)]\n",
      "refinery: 0 , 24\n",
      "Next threshold =  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  11963\n",
      "[('other', 53700, 53728), ('first', 37367, 37395), ('many', 33905, 33933), ('new', 21759, 21787), ('system', 17918, 17946)]\n",
      "refinery: 0 , 28\n",
      "Next threshold =  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  11415\n",
      "[('other', 61314, 61347), ('first', 42625, 42658), ('many', 38869, 38902), ('new', 24988, 25021), ('system', 20500, 20533)]\n",
      "refinery: 0 , 33\n",
      "Next threshold =  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  11314\n",
      "[('other', 68944, 68982), ('first', 47818, 47856), ('many', 43742, 43780), ('new', 28057, 28095), ('system', 22993, 23031)]\n",
      "refinery: 3 , 41\n",
      "Next threshold =  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words:  11204\n",
      "[('other', 76395, 76438), ('first', 53078, 53121), ('many', 48562, 48605), ('new', 31153, 31196), ('system', 25592, 25635)]\n",
      "refinery: 1 , 44\n",
      "Next threshold =  5\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# MG algorithm for approximate word count\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "k = 10000\n",
    "threshold = 0\n",
    "total_decrement = 0\n",
    "\n",
    "ssc = StreamingContext(sc, 5)\n",
    "# Provide a checkpointing directory.  Required for stateful transformations\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "rdd = sc.textFile('../data/adj_noun_pairs.txt', 8)\n",
    "rddQueue = rdd.randomSplit([1]*10, 123)\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "def updateFunc(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    newValue = sum(newValues, runningCount) - threshold\n",
    "    return newValue if newValue > 0 else None\n",
    "    # add the new values with the previous running count to get the new count\n",
    "\n",
    "running_counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "                      .map(lambda word: (word, 1))\\\n",
    "                      .reduceByKey(lambda a, b: a + b) \\\n",
    "                      .updateStateByKey(updateFunc)\n",
    "            \n",
    "counts_sorted = running_counts.transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n",
    "\n",
    "def printResults(rdd):\n",
    "    global threshold, total_decrement \n",
    "    rdd.cache()\n",
    "    print(\"Total distinct words: \", rdd.count())\n",
    "    print(rdd.map(lambda x: (x[0], x[1], x[1]+total_decrement)).take(5))\n",
    "    lower_bound = rdd.lookup('refinery')\n",
    "    if len(lower_bound) > 0:\n",
    "        lower_bound = lower_bound[0]\n",
    "    else:\n",
    "        lower_bound = 0\n",
    "    print('refinery:', lower_bound, ',', lower_bound + total_decrement)\n",
    "    if rdd.count() > k:\n",
    "        threshold = rdd.zipWithIndex().map(lambda x: (x[1], x[0])).lookup(k)[0][1]\n",
    "    else:\n",
    "        threhold = 0\n",
    "    print(\"Next threshold = \", threshold)\n",
    "    total_decrement += threshold\n",
    "    rdd.unpersist()\n",
    "\n",
    "counts_sorted.foreachRDD(printResults)\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination(50)\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fa38589",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2024-04-15 21:27:54\n",
      "-------------------------------------------\n",
      "0\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-15 21:27:57\n",
      "-------------------------------------------\n",
      "1\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-15 21:28:00\n",
      "-------------------------------------------\n",
      "2\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-15 21:28:03\n",
      "-------------------------------------------\n",
      "3\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-15 21:28:06\n",
      "-------------------------------------------\n",
      "4\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-15 21:28:09\n",
      "-------------------------------------------\n",
      "4\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-15 21:28:12\n",
      "-------------------------------------------\n",
      "4\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-04-15 21:28:15\n",
      "-------------------------------------------\n",
      "\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a queue of RDDs\n",
    "rddQueue = []\n",
    "for i in range(5):\n",
    "    rdd = sc.parallelize([i, i, i, i, i])\n",
    "    rddQueue += [rdd]\n",
    "        \n",
    "# Create a StreamingContext with batch interval of 3 seconds\n",
    "ssc = StreamingContext(sc, 3)\n",
    "\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "# Feed the rdd queue to a DStream\n",
    "nums = ssc.queueStream(rddQueue)\n",
    "\n",
    "# Compute the sum over a sliding window of 9 seconds for every 3 seconds\n",
    "slidingSum = nums.reduceByWindow(lambda x, y: max(x,y), None, 9, 3)\n",
    "#slidingSum = nums.reduceByWindow(lambda x, y: x + y, lambda x, y: x - y, 9, 3)\n",
    "\n",
    "slidingSum.pprint()\n",
    "\n",
    "ssc.start()  # Start the computation\n",
    "ssc.awaitTermination(24)  # Wait for the computation to terminate\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0740f4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/29 22:08:48 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n",
      "24/04/29 22:08:50 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-b7e17fa6-4901-4f16-b270-bd3f8f2c2252. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/29 22:08:50 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:==========================================>            (155 + 6) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/29 22:09:15 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@3925c7a4 is aborting.\n",
      "24/04/29 22:09:15 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@3925c7a4 aborted.\n",
      "24/04/29 22:09:15 WARN Shell: Interrupted while joining on: Thread[Thread-1809,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1257)\n",
      "\tat java.lang.Thread.join(Thread.java:1331)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:677)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveMkdir(FileSystem.java:1356)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.mkdir(DelegateToFileSystem.java:185)\n",
      "\tat org.apache.hadoop.fs.FilterFs.mkdir(FilterFs.java:219)\n",
      "\tat org.apache.hadoop.fs.FileContext$4.next(FileContext.java:809)\n",
      "\tat org.apache.hadoop.fs.FileContext$4.next(FileContext.java:805)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:812)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.mkdirs(CheckpointFileManager.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.init(HDFSBackedStateStoreProvider.scala:248)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreProvider$.createAndInit(StateStore.scala:326)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$getStateStoreProvider$5(StateStore.scala:535)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getStateStoreProvider(StateStore.scala:534)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.get(StateStore.scala:495)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:125)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/04/29 22:09:15 WARN Shell: Interrupted while joining on: Thread[Thread-1811,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1257)\n",
      "\tat java.lang.Thread.join(Thread.java:1331)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1590)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:395)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:642)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:352)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:395)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/04/29 22:09:15 WARN Shell: Interrupted while joining on: Thread[Thread-1810,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1257)\n",
      "\tat java.lang.Thread.join(Thread.java:1331)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:747)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:395)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:642)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:352)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:395)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/04/29 22:09:15 WARN Shell: Interrupted while joining on: Thread[Thread-1813,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1257)\n",
      "\tat java.lang.Thread.join(Thread.java:1331)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1590)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:395)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:642)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:352)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:395)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/04/29 22:09:15 WARN Shell: Interrupted while joining on: Thread[Thread-1812,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1257)\n",
      "\tat java.lang.Thread.join(Thread.java:1331)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:360)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:327)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:333)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:395)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:642)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:352)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:395)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/04/29 22:09:15 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 157 (task 163, attempt 0, stage 1.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:774)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:456)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/04/29 22:09:15 ERROR DataWritingSparkTask: Aborting commit for partition 157 (task 163, attempt 0, stage 1.0)\n",
      "24/04/29 22:09:15 ERROR DataWritingSparkTask: Aborted commit for partition 157 (task 163, attempt 0, stage 1.0)\n",
      "24/04/29 22:09:15 WARN TaskSetManager: Lost task 157.0 in stage 1.0 (TID 163) (192.168.1.60 executor driver): TaskKilled (Stage cancelled)\n",
      "24/04/29 22:09:15 WARN TaskSetManager: Lost task 162.0 in stage 1.0 (TID 168) (192.168.1.60 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:===========================================>           (157 + 6) / 200]\r",
      "\r",
      "[Stage 1:===========================================>           (157 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/29 22:09:15 WARN TaskSetManager: Lost task 159.0 in stage 1.0 (TID 165) (192.168.1.60 executor driver): TaskKilled (Stage cancelled)\n",
      "24/04/29 22:09:16 WARN TaskSetManager: Lost task 160.0 in stage 1.0 (TID 166) (192.168.1.60 executor driver): TaskKilled (Stage cancelled)\n",
      "24/04/29 22:09:16 WARN TaskSetManager: Lost task 158.0 in stage 1.0 (TID 164) (192.168.1.60 executor driver): TaskKilled (Stage cancelled)\n",
      "Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:===========================================>           (157 + 2) / 200]\r"
     ]
    }
   ],
   "source": [
    "# Word count using structured streaming: Complete mode vs update mode\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "lines = spark\\\n",
    "        .readStream\\\n",
    "        .format('socket')\\\n",
    "        .option('host', 'localhost')\\\n",
    "        .option('port', '9999')\\\n",
    "        .load()\n",
    "        \n",
    "# Split the lines into words, retaining timestamps\n",
    "# split() splits each line into an array, and explode() turns the array into multiple rows\n",
    "words = lines.select(explode(split(lines.value, ' ')).alias('word'))\n",
    "\n",
    "word_counts = words.groupBy('word').count()\n",
    "\n",
    "# Start running the query\n",
    "query = word_counts\\\n",
    "        .writeStream\\\n",
    "        .outputMode('complete')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .start()\n",
    "\n",
    "# try update mode; append mode not supported\n",
    "\n",
    "query.awaitTermination(25)\n",
    "query.stop()\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ef6278f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/29 22:11:07 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n",
      "24/04/29 22:11:07 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ddb10c12-977c-4b27-9232-c48780e8f753. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/29 22:11:07 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+\n",
      "|word|\n",
      "+----+\n",
      "+----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------+\n",
      "|word    |\n",
      "+--------+\n",
      "|aeasfaef|\n",
      "+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+---------------+\n",
      "|word           |\n",
      "+---------------+\n",
      "|aefaefasefasefa|\n",
      "|afefaefaef     |\n",
      "+---------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----------------------+\n",
      "|word                   |\n",
      "+-----------------------+\n",
      "|aefapsoeifjapoiefjpaio |\n",
      "|aefijapeiofjapseifjapf |\n",
      "|jiafoespfioasjpfiajpoei|\n",
      "+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/Conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/bigbenchung/Conda/lib/python3.9/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/home/bigbenchung/Conda/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28632/1249299890.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finished\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.3.2-bin-hadoop3/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timeout must be a positive integer or float. Got %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Conda/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/Conda/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Conda/lib/python3.9/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Conda/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Append mode with selection condition\n",
    "# Note: complete mode not supported if no aggregation\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "lines = spark\\\n",
    "        .readStream\\\n",
    "        .format('socket')\\\n",
    "        .option('host', 'localhost')\\\n",
    "        .option('port', '9999')\\\n",
    "        .load()\n",
    "        \n",
    "# Split the lines into words, retaining timestamps\n",
    "# split() splits each line into an array, and explode() turns the array into multiple rows\n",
    "words = lines.select(explode(split(lines.value, ' ')).alias('word'))\n",
    "\n",
    "long_words = words.filter(length(words['word'])>=3)\n",
    "\n",
    "# Start running the query \n",
    "query = long_words\\\n",
    "        .writeStream\\\n",
    "        .outputMode('append')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .start()\n",
    "\n",
    "query.awaitTermination(25)\n",
    "query.stop()\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08561c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/29 21:53:30 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n",
      "24/04/29 21:53:30 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-5c032ec6-235b-4868-b523-70c4fa3153dd. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/29 21:53:30 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:==================================================>    (185 + 6) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/29 21:53:56 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@732abb5d is aborting.\n",
      "24/04/29 21:53:56 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@732abb5d aborted.\n",
      "24/04/29 21:53:56 WARN Shell: Interrupted while joining on: Thread[Thread-4307,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1257)\n",
      "\tat java.lang.Thread.join(Thread.java:1331)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:747)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.$anonfun$close$3(statefulOperators.scala:481)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:642)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:352)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.close(statefulOperators.scala:481)\n",
      "\tat org.apache.spark.util.NextIterator.closeIfNeeded(NextIterator.scala:66)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:75)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:435)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/04/29 21:53:56 WARN TaskSetManager: Lost task 188.0 in stage 4.0 (TID 402) (192.168.1.60 executor driver): TaskKilled (Stage cancelled)\n",
      "24/04/29 21:53:56 WARN TaskSetManager: Lost task 186.0 in stage 4.0 (TID 400) (192.168.1.60 executor driver): TaskKilled (Stage cancelled)\n",
      "24/04/29 21:53:56 WARN Shell: Interrupted while joining on: Thread[Thread-4308,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1257)\n",
      "\tat java.lang.Thread.join(Thread.java:1331)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:677)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveMkdir(FileSystem.java:1356)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.mkdir(DelegateToFileSystem.java:185)\n",
      "\tat org.apache.hadoop.fs.FilterFs.mkdir(FilterFs.java:219)\n",
      "\tat org.apache.hadoop.fs.FileContext$4.next(FileContext.java:809)\n",
      "\tat org.apache.hadoop.fs.FileContext$4.next(FileContext.java:805)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:812)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.mkdirs(CheckpointFileManager.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.init(HDFSBackedStateStoreProvider.scala:248)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreProvider$.createAndInit(StateStore.scala:326)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$getStateStoreProvider$5(StateStore.scala:535)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.getStateStoreProvider(StateStore.scala:534)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.get(StateStore.scala:495)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:125)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/04/29 21:53:56 WARN TaskSetManager: Lost task 189.0 in stage 4.0 (TID 403) (192.168.1.60 executor driver): TaskKilled (Stage cancelled)\n",
      "24/04/29 21:53:56 WARN TaskSetManager: Lost task 187.0 in stage 4.0 (TID 401) (192.168.1.60 executor driver): TaskKilled (Stage cancelled)\n",
      "24/04/29 21:53:56 WARN TaskSetManager: Lost task 190.0 in stage 4.0 (TID 404) (192.168.1.60 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/29 21:53:56 ERROR Utils: Aborting task\n",
      "java.lang.IllegalStateException: Error committing version 1 into HDFSStateStore[id=(op=0,part=185),dir=file:/tmp/temporary-5c032ec6-235b-4868-b523-70c4fa3153dd/state/0/185]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.$anonfun$close$3(statefulOperators.scala:481)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:642)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:352)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.close(statefulOperators.scala:481)\n",
      "\tat org.apache.spark.util.NextIterator.closeIfNeeded(NextIterator.scala:66)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:75)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:435)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-5c032ec6-235b-4868-b523-70c4fa3153dd/state/0/185/..1.delta.85bcdf85-156e-44bb-8396-696d8f208658.TID399.tmp.crc does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 26 more\n",
      "24/04/29 21:53:56 ERROR DataWritingSparkTask: Aborting commit for partition 185 (task 399, attempt 0, stage 4.0)\n",
      "24/04/29 21:53:56 ERROR DataWritingSparkTask: Aborted commit for partition 185 (task 399, attempt 0, stage 4.0)\n",
      "24/04/29 21:53:56 WARN TaskSetManager: Lost task 185.0 in stage 4.0 (TID 399) (192.168.1.60 executor driver): TaskKilled (Stage cancelled)\n",
      "24/04/29 21:53:56 WARN FileUtil: Failed to delete file or dir [/tmp/temporary-5c032ec6-235b-4868-b523-70c4fa3153dd/state/0]: it still exists.\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "lines = spark\\\n",
    "        .readStream\\\n",
    "        .format('socket')\\\n",
    "        .option('host', 'localhost')\\\n",
    "        .option('port', '9999')\\\n",
    "        .option('includeTimestamp', 'true')\\\n",
    "        .load()\n",
    "        \n",
    "# Split the lines into words, retaining timestamps\n",
    "# split() splits each line into an array, and explode() turns the array into multiple rows\n",
    "words = lines.select(explode(split(lines.value, ' ')).alias('word'),\n",
    "                     lines.timestamp)\n",
    "\n",
    "windowedCounts = words.groupBy(\n",
    "    window(words.timestamp, \"10 seconds\", \"5 seconds\"),\n",
    "    words.word)\\\n",
    "    .count()\n",
    "\n",
    "# Start running the query \n",
    "query = windowedCounts\\\n",
    "        .writeStream\\\n",
    "        .outputMode('update')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .start()\n",
    "\n",
    "query.awaitTermination(25)\n",
    "query.stop()\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c05bcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+-----+\n",
      "|timestamp|value|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2024-04-15 21:57:48.456|0    |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2024-04-15 21:57:49.456|1    |\n",
      "|2024-04-15 21:57:50.456|2    |\n",
      "|2024-04-15 21:57:51.456|3    |\n",
      "|2024-04-15 21:57:52.456|4    |\n",
      "|2024-04-15 21:57:53.456|5    |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2024-04-15 21:57:54.456|6    |\n",
      "|2024-04-15 21:57:55.456|7    |\n",
      "|2024-04-15 21:57:56.456|8    |\n",
      "|2024-04-15 21:57:57.456|9    |\n",
      "|2024-04-15 21:57:58.456|10   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2024-04-15 21:57:59.456|11   |\n",
      "|2024-04-15 21:58:00.456|12   |\n",
      "|2024-04-15 21:58:01.456|13   |\n",
      "|2024-04-15 21:58:02.456|14   |\n",
      "|2024-04-15 21:58:03.456|15   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2024-04-15 21:58:04.456|16   |\n",
      "|2024-04-15 21:58:05.456|17   |\n",
      "|2024-04-15 21:58:06.456|18   |\n",
      "|2024-04-15 21:58:07.456|19   |\n",
      "|2024-04-15 21:58:08.456|20   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Rate source (for testing) - Generates data at the specified number of rows per second, \n",
    "# each output row contains a timestamp and value. Where timestamp is a Timestamp type \n",
    "# containing the time of message dispatch, and value is of Long type containing the message count, \n",
    "# starting from 0 as the first row. This source is intended for testing and benchmarking.\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "streamingDf = spark\\\n",
    "        .readStream\\\n",
    "        .format('rate')\\\n",
    "        .load()\n",
    "        \n",
    "# Start running the query \n",
    "query = streamingDf\\\n",
    "        .writeStream\\\n",
    "        .outputMode('append')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .start()\n",
    "\n",
    "query.awaitTermination(25)\n",
    "query.stop()\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c839fb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+-----+\n",
      "|name|count|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------+-----+\n",
      "|name  |count|\n",
      "+------+-----+\n",
      "|orange|1    |\n",
      "|apple |1    |\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------+-----+\n",
      "|name  |count|\n",
      "+------+-----+\n",
      "|orange|1    |\n",
      "|apple |1    |\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+------+-----+\n",
      "|name  |count|\n",
      "+------+-----+\n",
      "|orange|1    |\n",
      "|apple |2    |\n",
      "|banana|1    |\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+------+-----+\n",
      "|name  |count|\n",
      "+------+-----+\n",
      "|orange|1    |\n",
      "|apple |2    |\n",
      "|banana|1    |\n",
      "+------+-----+\n",
      "\n",
      "Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/15 22:02:25 ERROR TorrentBroadcast: Store broadcast broadcast_594 fail, remove all pieces of the broadcast\n"
     ]
    }
   ],
   "source": [
    "# StreamStatic Joins\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "staticDf = spark.createDataFrame([(1, 'apple'), (2, 'orange'), (10, 'banana'), (12, 'apple')], ['id', 'name'])\n",
    "\n",
    "streamingDf = spark\\\n",
    "        .readStream\\\n",
    "        .format('rate')\\\n",
    "        .load()\n",
    "\n",
    "# Start running the query \n",
    "query = streamingDf.join(staticDf, streamingDf['value'] == staticDf['id']).groupBy('name').count()\\\n",
    "        .writeStream\\\n",
    "        .outputMode('complete')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .start()\n",
    "# also support leftOuter, but not rightOuter\n",
    "\n",
    "query.awaitTermination(25)\n",
    "query.stop()\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d082075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----+---------+---------+\n",
      "|value|timestamp|timestamp|\n",
      "+-----+---------+---------+\n",
      "+-----+---------+---------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----+-----------------------+-----------------------+\n",
      "|value|timestamp              |timestamp              |\n",
      "+-----+-----------------------+-----------------------+\n",
      "|2    |2024-04-15 22:03:33.939|2024-04-15 22:03:32.967|\n",
      "|0    |2024-04-15 22:03:31.939|2024-04-15 22:03:31.967|\n",
      "|1    |2024-04-15 22:03:32.939|2024-04-15 22:03:32.467|\n",
      "+-----+-----------------------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----+-----------------------+-----------------------+\n",
      "|value|timestamp              |timestamp              |\n",
      "+-----+-----------------------+-----------------------+\n",
      "|4    |2024-04-15 22:03:35.939|2024-04-15 22:03:33.967|\n",
      "|5    |2024-04-15 22:03:36.939|2024-04-15 22:03:34.467|\n",
      "|6    |2024-04-15 22:03:37.939|2024-04-15 22:03:34.967|\n",
      "|3    |2024-04-15 22:03:34.939|2024-04-15 22:03:33.467|\n",
      "|7    |2024-04-15 22:03:38.939|2024-04-15 22:03:35.467|\n",
      "+-----+-----------------------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----+-----------------------+-----------------------+\n",
      "|value|timestamp              |timestamp              |\n",
      "+-----+-----------------------+-----------------------+\n",
      "|8    |2024-04-15 22:03:39.939|2024-04-15 22:03:35.967|\n",
      "|12   |2024-04-15 22:03:43.939|2024-04-15 22:03:37.967|\n",
      "|9    |2024-04-15 22:03:40.939|2024-04-15 22:03:36.467|\n",
      "|10   |2024-04-15 22:03:41.939|2024-04-15 22:03:36.967|\n",
      "|11   |2024-04-15 22:03:42.939|2024-04-15 22:03:37.467|\n",
      "+-----+-----------------------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-----+-----------------------+-----------------------+\n",
      "|value|timestamp              |timestamp              |\n",
      "+-----+-----------------------+-----------------------+\n",
      "|15   |2024-04-15 22:03:46.939|2024-04-15 22:03:39.467|\n",
      "|14   |2024-04-15 22:03:45.939|2024-04-15 22:03:38.967|\n",
      "|16   |2024-04-15 22:03:47.939|2024-04-15 22:03:39.967|\n",
      "|17   |2024-04-15 22:03:48.939|2024-04-15 22:03:40.467|\n",
      "|13   |2024-04-15 22:03:44.939|2024-04-15 22:03:38.467|\n",
      "+-----+-----------------------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-----+-----------------------+-----------------------+\n",
      "|value|timestamp              |timestamp              |\n",
      "+-----+-----------------------+-----------------------+\n",
      "|19   |2024-04-15 22:03:50.939|2024-04-15 22:03:41.467|\n",
      "|22   |2024-04-15 22:03:53.939|2024-04-15 22:03:42.967|\n",
      "|18   |2024-04-15 22:03:49.939|2024-04-15 22:03:40.967|\n",
      "|20   |2024-04-15 22:03:51.939|2024-04-15 22:03:41.967|\n",
      "|21   |2024-04-15 22:03:52.939|2024-04-15 22:03:42.467|\n",
      "+-----+-----------------------+-----------------------+\n",
      "\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Stream-stream Joins\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 4) \n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "streamingDf = spark\\\n",
    "        .readStream\\\n",
    "        .format('rate')\\\n",
    "        .load()\n",
    "\n",
    "streamingDf2 = spark\\\n",
    "        .readStream\\\n",
    "        .format('rate')\\\n",
    "        .option('rowsPerSecond', 2) \\\n",
    "        .load()\n",
    "\n",
    "# Start running the query \n",
    "query = streamingDf.join(streamingDf2, 'value')\\\n",
    "        .writeStream\\\n",
    "        .outputMode('append')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .start()\n",
    "\n",
    "query.awaitTermination(25)\n",
    "query.stop()\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "02078f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----+---------+---------+\n",
      "|value|timestamp|timestamp|\n",
      "+-----+---------+---------+\n",
      "+-----+---------+---------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----+-----------------------+-----------------------+\n",
      "|value|timestamp              |timestamp              |\n",
      "+-----+-----------------------+-----------------------+\n",
      "|2    |2024-04-15 17:16:03.386|2024-04-15 17:16:01.414|\n",
      "|0    |2024-04-15 17:16:01.386|2024-04-15 17:16:01.413|\n",
      "|1    |2024-04-15 17:16:02.386|2024-04-15 17:16:01.414|\n",
      "+-----+-----------------------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----+-----------------------+-----------------------+\n",
      "|value|timestamp              |timestamp              |\n",
      "+-----+-----------------------+-----------------------+\n",
      "|4    |2024-04-15 17:16:05.386|2024-04-15 17:16:01.415|\n",
      "|5    |2024-04-15 17:16:06.386|2024-04-15 17:16:01.416|\n",
      "|6    |2024-04-15 17:16:07.386|2024-04-15 17:16:01.416|\n",
      "|3    |2024-04-15 17:16:04.386|2024-04-15 17:16:01.415|\n",
      "|7    |2024-04-15 17:16:08.386|2024-04-15 17:16:01.417|\n",
      "+-----+-----------------------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----+-----------------------+-----------------------+\n",
      "|value|timestamp              |timestamp              |\n",
      "+-----+-----------------------+-----------------------+\n",
      "|8    |2024-04-15 17:16:09.386|2024-04-15 17:16:01.417|\n",
      "|12   |2024-04-15 17:16:13.386|2024-04-15 17:16:01.419|\n",
      "|9    |2024-04-15 17:16:10.386|2024-04-15 17:16:01.418|\n",
      "|10   |2024-04-15 17:16:11.386|2024-04-15 17:16:01.418|\n",
      "|11   |2024-04-15 17:16:12.386|2024-04-15 17:16:01.419|\n",
      "+-----+-----------------------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-----+-----------------------+-----------------------+\n",
      "|value|timestamp              |timestamp              |\n",
      "+-----+-----------------------+-----------------------+\n",
      "|15   |2024-04-15 17:16:16.386|2024-04-15 17:16:01.421|\n",
      "|14   |2024-04-15 17:16:15.386|2024-04-15 17:16:01.42 |\n",
      "|16   |2024-04-15 17:16:17.386|2024-04-15 17:16:01.421|\n",
      "|17   |2024-04-15 17:16:18.386|2024-04-15 17:16:01.422|\n",
      "|13   |2024-04-15 17:16:14.386|2024-04-15 17:16:01.42 |\n",
      "+-----+-----------------------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-----+-----------------------+-----------------------+\n",
      "|value|timestamp              |timestamp              |\n",
      "+-----+-----------------------+-----------------------+\n",
      "|19   |2024-04-15 17:16:20.386|2024-04-15 17:16:01.423|\n",
      "|22   |2024-04-15 17:16:23.386|2024-04-15 17:16:01.424|\n",
      "|18   |2024-04-15 17:16:19.386|2024-04-15 17:16:01.422|\n",
      "|20   |2024-04-15 17:16:21.386|2024-04-15 17:16:01.423|\n",
      "|21   |2024-04-15 17:16:22.386|2024-04-15 17:16:01.424|\n",
      "+-----+-----------------------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+-----+-----------------------+-----------------------+\n",
      "|value|timestamp              |timestamp              |\n",
      "+-----+-----------------------+-----------------------+\n",
      "|23   |2024-04-15 17:16:24.386|2024-04-15 17:16:01.425|\n",
      "|25   |2024-04-15 17:16:26.386|2024-04-15 17:16:01.426|\n",
      "|26   |2024-04-15 17:16:27.386|2024-04-15 17:16:01.426|\n",
      "|24   |2024-04-15 17:16:25.386|2024-04-15 17:16:01.425|\n",
      "|27   |2024-04-15 17:16:28.386|2024-04-15 17:16:01.427|\n",
      "+-----+-----------------------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+-----+-----------------------+-----------------------+\n",
      "|value|timestamp              |timestamp              |\n",
      "+-----+-----------------------+-----------------------+\n",
      "|28   |2024-04-15 17:16:29.386|2024-04-15 17:16:01.427|\n",
      "|29   |2024-04-15 17:16:30.386|2024-04-15 17:16:01.428|\n",
      "|30   |2024-04-15 17:16:31.386|2024-04-15 17:16:01.428|\n",
      "|31   |2024-04-15 17:16:32.386|2024-04-15 17:16:01.429|\n",
      "|32   |2024-04-15 17:16:33.386|2024-04-15 17:16:01.429|\n",
      "+-----+-----------------------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+-----+-----------------------+-----------------------+\n",
      "|value|timestamp              |timestamp              |\n",
      "+-----+-----------------------+-----------------------+\n",
      "|33   |2024-04-15 17:16:34.386|2024-04-15 17:16:01.43 |\n",
      "|34   |2024-04-15 17:16:35.386|2024-04-15 17:16:01.43 |\n",
      "|36   |2024-04-15 17:16:37.386|2024-04-15 17:16:01.431|\n",
      "|37   |2024-04-15 17:16:38.386|2024-04-15 17:16:01.432|\n",
      "|35   |2024-04-15 17:16:36.386|2024-04-15 17:16:01.431|\n",
      "+-----+-----------------------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+-----+-----------------------+-----------------------+\n",
      "|value|timestamp              |timestamp              |\n",
      "+-----+-----------------------+-----------------------+\n",
      "|42   |2024-04-15 17:16:43.386|2024-04-15 17:16:01.434|\n",
      "|40   |2024-04-15 17:16:41.386|2024-04-15 17:16:01.433|\n",
      "|38   |2024-04-15 17:16:39.386|2024-04-15 17:16:01.432|\n",
      "|39   |2024-04-15 17:16:40.386|2024-04-15 17:16:01.433|\n",
      "|41   |2024-04-15 17:16:42.386|2024-04-15 17:16:01.434|\n",
      "+-----+-----------------------+-----------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+-----+-----------------------+-----------------------+\n",
      "|value|timestamp              |timestamp              |\n",
      "+-----+-----------------------+-----------------------+\n",
      "|43   |2024-04-15 17:16:44.386|2024-04-15 17:16:01.435|\n",
      "|44   |2024-04-15 17:16:45.386|2024-04-15 17:16:01.435|\n",
      "|46   |2024-04-15 17:16:47.386|2024-04-15 17:16:01.436|\n",
      "|45   |2024-04-15 17:16:46.386|2024-04-15 17:16:01.436|\n",
      "|47   |2024-04-15 17:16:48.386|2024-04-15 17:16:01.437|\n",
      "+-----+-----------------------+-----------------------+\n",
      "\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Streamstream Joins with watemarking\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 4) \n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "streamingDf = spark\\\n",
    "        .readStream\\\n",
    "        .format('rate')\\\n",
    "        .load()\n",
    "\n",
    "streamingDf2 = spark\\\n",
    "        .readStream\\\n",
    "        .format('rate')\\\n",
    "        .option('rowsPerSecond', 2) \\\n",
    "        .load()\n",
    "\n",
    "# Start running the query \n",
    "query = streamingDf.withWatermark('timestamp', '3 seconds').join(streamingDf2.withWatermark('timestamp', '3 seconds'), 'value')\\\n",
    "        .writeStream\\\n",
    "        .outputMode('append')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .start()\n",
    "\n",
    "query.awaitTermination(50)\n",
    "query.stop()\n",
    "print(\"Finished\")\n",
    "\n",
    "#The current watermark is computed by looking at the MAX(eventTime) seen across all of the partitions in \n",
    "#the query minus a user specified delayThreshold. Due to the cost of coordinating this value across partitions,\n",
    "#the actual watermark used is only guaranteed to be at least delayThreshold behind the actual event time. \n",
    "#In some cases we may still process records that arrive more than delayThreshold late."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc901c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a StreamingContext with batch interval of 5 seconds\n",
    "ssc = StreamingContext(sc, 5)\n",
    "\n",
    "# Create a DStream that will connect to localhost at port 9999\n",
    "# Start Netcat server: nc -lk 9999 \n",
    "lines = ssc.socketTextStream('localhost', 9999)\n",
    "\n",
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "lines.pprint()\n",
    "wordCounts.pprint()\n",
    "\n",
    "ssc.start()  # Start the computation\n",
    "print(\"Start\")\n",
    "ssc.awaitTermination(20)  # Wait for the computation to terminate\n",
    "ssc.stop(stopSparkContext=False)  # Stop the StreamingContext without stopping the SparkContext\n",
    "\n",
    "print(\"Finished\")\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a queue of RDDs\n",
    "rdd = sc.textFile('../data/adj_noun_pairs.txt', 8)\n",
    "\n",
    "# split the rdd into 5 equal-size parts\n",
    "rddQueue = rdd.randomSplit([1,1,1,1,1], 123)\n",
    "        \n",
    "# Create a StreamingContext with batch interval of 5 seconds\n",
    "ssc = StreamingContext(sc, 5)\n",
    "\n",
    "# Feed the rdd queue to a DStream\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "# Do word-counting as before\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Use transform() to access any rdd transformations not directly available in SparkStreaming\n",
    "topWords = wordCounts.transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n",
    "topWords.pprint()\n",
    "\n",
    "ssc.start()  # Start the computation\n",
    "ssc.awaitTermination(25)  # Wait for the computation to terminate\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")\n",
    "\n",
    "# Find the most positive words in windows of 5 seconds from streaming data\n",
    "# URL for data: https://cse.hkust.edu.hk/msbd5003/data/AFINN-111.txt\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "def parse_line(l):\n",
    "    x = l.split(\"\\t\")\n",
    "    return (x[0], float(x[1]))\n",
    "\n",
    "word_sentiments = sc.textFile(\"../data/AFINN-111.txt\") \\\n",
    "                    .map(parse_line).cache()\n",
    "    \n",
    "ssc = StreamingContext(sc, 5)\n",
    "rdd = sc.textFile('../data/adj_noun_pairs.txt', 8)\n",
    "rddQueue = rdd.randomSplit([1,1,1,1,1], 123)\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "word_counts = lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "                   .map(lambda word: (word, 1)) \\\n",
    "                   .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Determine the words with the highest sentiment values by joining the streaming RDD\n",
    "# with the static RDD inside the transform() method and then multiplying\n",
    "# the frequency of the words by its sentiment value\n",
    "happiest_words = word_counts.transform(lambda rdd: word_sentiments.join(rdd)) \\\n",
    "                            .map(lambda t:\n",
    "                                 (t[1][0] * t[1][1], t[0])) \\\n",
    "                            .transform(lambda rdd: rdd.sortByKey(False))\n",
    "\n",
    "happiest_words.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination(25)\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Stateful word count\n",
    "\n",
    "ssc = StreamingContext(sc, 5)\n",
    "# Provide a checkpointing directory.  Required for stateful transformations\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "rdd = sc.textFile('../data/adj_noun_pairs.txt', 8)\n",
    "rddQueue = rdd.randomSplit([1]*10, 123)\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "def updateFunc(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    return sum(newValues, runningCount)\n",
    "    # add the new values with the previous running count to get the new count\n",
    "\n",
    "running_counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "                      .map(lambda word: (word, 1))\\\n",
    "                      .reduceByKey(lambda a, b: a + b) \\\n",
    "                      .updateStateByKey(updateFunc)\n",
    "\n",
    "counts_sorted = running_counts.transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n",
    "\n",
    "def printResults(rdd):\n",
    "    print(\"Total distinct words: \", rdd.count())\n",
    "    print(rdd.take(5))\n",
    "    print('refinery:', rdd.lookup('refinery')[0])\n",
    "\n",
    "counts_sorted.foreachRDD(printResults)\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination(50)\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")\n",
    "\n",
    "# MG algorithm for approximate word count\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "k = 10000\n",
    "threshold = 0\n",
    "total_decrement = 0\n",
    "\n",
    "ssc = StreamingContext(sc, 5)\n",
    "# Provide a checkpointing directory.  Required for stateful transformations\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "rdd = sc.textFile('../data/adj_noun_pairs.txt', 8)\n",
    "rddQueue = rdd.randomSplit([1]*10, 123)\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "def updateFunc(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    newValue = sum(newValues, runningCount) - threshold\n",
    "    return newValue if newValue > 0 else None\n",
    "    # add the new values with the previous running count to get the new count\n",
    "\n",
    "running_counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "                      .map(lambda word: (word, 1))\\\n",
    "                      .reduceByKey(lambda a, b: a + b) \\\n",
    "                      .updateStateByKey(updateFunc)\n",
    "            \n",
    "counts_sorted = running_counts.transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n",
    "\n",
    "def printResults(rdd):\n",
    "    global threshold, total_decrement \n",
    "    rdd.cache()\n",
    "    print(\"Total distinct words: \", rdd.count())\n",
    "    print(rdd.map(lambda x: (x[0], x[1], x[1]+total_decrement)).take(5))\n",
    "    lower_bound = rdd.lookup('refinery')\n",
    "    if len(lower_bound) > 0:\n",
    "        lower_bound = lower_bound[0]\n",
    "    else:\n",
    "        lower_bound = 0\n",
    "    print('refinery:', lower_bound, ',', lower_bound + total_decrement)\n",
    "    if rdd.count() > k:\n",
    "        threshold = rdd.zipWithIndex().map(lambda x: (x[1], x[0])).lookup(k)[0][1]\n",
    "    else:\n",
    "        threhold = 0\n",
    "    print(\"Next threshold = \", threshold)\n",
    "    total_decrement += threshold\n",
    "    rdd.unpersist()\n",
    "\n",
    "counts_sorted.foreachRDD(printResults)\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination(50)\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a queue of RDDs\n",
    "rddQueue = []\n",
    "for i in range(5):\n",
    "    rdd = sc.parallelize([i, i, i, i, i])\n",
    "    rddQueue += [rdd]\n",
    "        \n",
    "# Create a StreamingContext with batch interval of 3 seconds\n",
    "ssc = StreamingContext(sc, 3)\n",
    "\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "# Feed the rdd queue to a DStream\n",
    "nums = ssc.queueStream(rddQueue)\n",
    "\n",
    "# Compute the sum over a sliding window of 9 seconds for every 3 seconds\n",
    "slidingSum = nums.reduceByWindow(lambda x, y: max(x,y), None, 9, 3)\n",
    "#slidingSum = nums.reduceByWindow(lambda x, y: x + y, lambda x, y: x - y, 9, 3)\n",
    "\n",
    "slidingSum.pprint()\n",
    "\n",
    "ssc.start()  # Start the computation\n",
    "ssc.awaitTermination(24)  # Wait for the computation to terminate\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")\n",
    "\n",
    "# Word count using structured streaming: Complete mode vs update mode\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "lines = spark\\\n",
    "        .readStream\\\n",
    "        .format('socket')\\\n",
    "        .option('host', 'localhost')\\\n",
    "        .option('port', '9999')\\\n",
    "        .load()\n",
    "        \n",
    "# Split the lines into words, retaining timestamps\n",
    "# split() splits each line into an array, and explode() turns the array into multiple rows\n",
    "words = lines.select(explode(split(lines.value, ' ')).alias('word'))\n",
    "\n",
    "word_counts = words.groupBy('word').count()\n",
    "\n",
    "# Start running the query\n",
    "query = word_counts\\\n",
    "        .writeStream\\\n",
    "        .outputMode('complete')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .start()\n",
    "\n",
    "# try update mode; append mode not supported\n",
    "\n",
    "query.awaitTermination(25)\n",
    "query.stop()\n",
    "print(\"Finished\")\n",
    "\n",
    "# Append mode with selection condition\n",
    "# Note: complete mode not supported if no aggregation\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "lines = spark\\\n",
    "        .readStream\\\n",
    "        .format('socket')\\\n",
    "        .option('host', 'localhost')\\\n",
    "        .option('port', '9999')\\\n",
    "        .load()\n",
    "        \n",
    "# Split the lines into words, retaining timestamps\n",
    "# split() splits each line into an array, and explode() turns the array into multiple rows\n",
    "words = lines.select(explode(split(lines.value, ' ')).alias('word'))\n",
    "\n",
    "long_words = words.filter(length(words['word'])>=3)\n",
    "\n",
    "# Start running the query \n",
    "query = long_words\\\n",
    "        .writeStream\\\n",
    "        .outputMode('append')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .start()\n",
    "\n",
    "query.awaitTermination(25)\n",
    "query.stop()\n",
    "print(\"Finished\")\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "lines = spark\\\n",
    "        .readStream\\\n",
    "        .format('socket')\\\n",
    "        .option('host', 'localhost')\\\n",
    "        .option('port', '9999')\\\n",
    "        .option('includeTimestamp', 'true')\\\n",
    "        .load()\n",
    "        \n",
    "# Split the lines into words, retaining timestamps\n",
    "# split() splits each line into an array, and explode() turns the array into multiple rows\n",
    "words = lines.select(explode(split(lines.value, ' ')).alias('word'),\n",
    "                     lines.timestamp)\n",
    "\n",
    "windowedCounts = words.groupBy(\n",
    "    window(words.timestamp, \"10 seconds\", \"5 seconds\"),\n",
    "    words.word)\\\n",
    "    .count()\n",
    "\n",
    "# Start running the query \n",
    "query = windowedCounts\\\n",
    "        .writeStream\\\n",
    "        .outputMode('update')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .start()\n",
    "\n",
    "query.awaitTermination(25)\n",
    "query.stop()\n",
    "print(\"Finished\")\n",
    "\n",
    "# Rate source (for testing) - Generates data at the specified number of rows per second, \n",
    "# each output row contains a timestamp and value. Where timestamp is a Timestamp type \n",
    "# containing the time of message dispatch, and value is of Long type containing the message count, \n",
    "# starting from 0 as the first row. This source is intended for testing and benchmarking.\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "streamingDf = spark\\\n",
    "        .readStream\\\n",
    "        .format('rate')\\\n",
    "        .load()\n",
    "        \n",
    "# Start running the query \n",
    "query = streamingDf\\\n",
    "        .writeStream\\\n",
    "        .outputMode('append')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .start()\n",
    "\n",
    "query.awaitTermination(25)\n",
    "query.stop()\n",
    "print(\"Finished\")\n",
    "\n",
    "# StreamStatic Joins\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "staticDf = spark.createDataFrame([(1, 'apple'), (2, 'orange'), (10, 'banana'), (12, 'apple')], ['id', 'name'])\n",
    "\n",
    "streamingDf = spark\\\n",
    "        .readStream\\\n",
    "        .format('rate')\\\n",
    "        .load()\n",
    "\n",
    "# Start running the query \n",
    "query = streamingDf.join(staticDf, streamingDf['value'] == staticDf['id']).groupBy('name').count()\\\n",
    "        .writeStream\\\n",
    "        .outputMode('complete')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .start()\n",
    "# also support leftOuter, but not rightOuter\n",
    "\n",
    "query.awaitTermination(25)\n",
    "query.stop()\n",
    "print(\"Finished\")\n",
    "\n",
    "# Stream-stream Joins\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 4) \n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "streamingDf = spark\\\n",
    "        .readStream\\\n",
    "        .format('rate')\\\n",
    "        .load()\n",
    "\n",
    "streamingDf2 = spark\\\n",
    "        .readStream\\\n",
    "        .format('rate')\\\n",
    "        .option('rowsPerSecond', 2) \\\n",
    "        .load()\n",
    "\n",
    "# Start running the query \n",
    "query = streamingDf.join(streamingDf2, 'value')\\\n",
    "        .writeStream\\\n",
    "        .outputMode('append')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .start()\n",
    "\n",
    "query.awaitTermination(25)\n",
    "query.stop()\n",
    "print(\"Finished\")\n",
    "\n",
    "# Streamstream Joins with watemarking\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 4) \n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "streamingDf = spark\\\n",
    "        .readStream\\\n",
    "        .format('rate')\\\n",
    "        .load()\n",
    "\n",
    "streamingDf2 = spark\\\n",
    "        .readStream\\\n",
    "        .format('rate')\\\n",
    "        .option('rowsPerSecond', 2) \\\n",
    "        .load()\n",
    "\n",
    "# Start running the query \n",
    "query = streamingDf.withWatermark('timestamp', '3 seconds').join(streamingDf2.withWatermark('timestamp', '3 seconds'), 'value')\\\n",
    "        .writeStream\\\n",
    "        .outputMode('append')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .start()\n",
    "\n",
    "query.awaitTermination(50)\n",
    "query.stop()\n",
    "print(\"Finished\")\n",
    "\n",
    "#The current watermark is computed by looking at the MAX(eventTime) seen across all of the partitions in \n",
    "#the query minus a user specified delayThreshold. Due to the cost of coordinating this value across partitions,\n",
    "#the actual watermark used is only guaranteed to be at least delayThreshold behind the actual event time. \n",
    "#In some cases we may still process records that arrive more than delayThreshold late."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a40cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
