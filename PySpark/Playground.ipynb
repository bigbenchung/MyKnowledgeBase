{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59854389",
   "metadata": {},
   "source": [
    "## RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a1cf2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b196a674",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"b\",3), (\"b\",4), (\"a\",5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2730eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 4), ('a', 5)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduceByKey(lambda x, y: x if x >= y else y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a1cc9e",
   "metadata": {},
   "source": [
    "## DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47be6135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f94e402",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCustomer = spark.read.csv('data/Customer.csv', header=True, inferSchema=True)\n",
    "dfDetail = spark.read.csv('data/SalesOrderDetail.csv', header=True, inferSchema=True)\n",
    "dfHeader = spark.read.csv('data/SalesOrderHeader.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b658f6",
   "metadata": {},
   "source": [
    "In the example relational database used in the lectures, each customer is served by a salesperson, which is recorded in the \"SalesPerson\" column in the \"Customer\" table (see the database schema at the header of each file).\n",
    "\n",
    "Use DataFrame API to generate a report of each salesperson's performance, which is the total net price of his/her customer's orders.  Note that an order consists of multiple order details, and the net price of an order detail is UnitPrice * OrderQty * (1 - UnitPriceDiscount).\n",
    "\n",
    "All salespeople should be reported, even if they have 0 order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21b22a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- NameStyle: integer (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- FirstName: string (nullable = true)\n",
      " |-- MiddleName: string (nullable = true)\n",
      " |-- LastName: string (nullable = true)\n",
      " |-- Suffix: string (nullable = true)\n",
      " |-- CompanyName: string (nullable = true)\n",
      " |-- SalesPerson: string (nullable = true)\n",
      " |-- EmailAddress: string (nullable = true)\n",
      " |-- Phone: string (nullable = true)\n",
      " |-- PasswordHash: string (nullable = true)\n",
      " |-- PasswordSalt: string (nullable = true)\n",
      " |-- rowguid: string (nullable = true)\n",
      " |-- ModifiedDate: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfCustomer.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88780e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SalesOrderID: integer (nullable = true)\n",
      " |-- RevisionNumber: integer (nullable = true)\n",
      " |-- OrderDate: timestamp (nullable = true)\n",
      " |-- DueDate: timestamp (nullable = true)\n",
      " |-- ShipDate: timestamp (nullable = true)\n",
      " |-- Status: integer (nullable = true)\n",
      " |-- OnlineOrderFlag: integer (nullable = true)\n",
      " |-- SalesOrderNumber: string (nullable = true)\n",
      " |-- PurchaseOrderNumber: string (nullable = true)\n",
      " |-- AccountNumber: string (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- ShipToAddressID: integer (nullable = true)\n",
      " |-- BillToAddressID: integer (nullable = true)\n",
      " |-- ShipMethod: string (nullable = true)\n",
      " |-- CreditCardApprovalCode: string (nullable = true)\n",
      " |-- SubTotal: double (nullable = true)\n",
      " |-- TaxAmt: double (nullable = true)\n",
      " |-- Freight: double (nullable = true)\n",
      " |-- TotalDue: double (nullable = true)\n",
      " |-- Comment: string (nullable = true)\n",
      " |-- rowguid: string (nullable = true)\n",
      " |-- ModifiedDate: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfHeader.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85fb4472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SalesOrderID: integer (nullable = true)\n",
      " |-- SalesOrderDetailID: integer (nullable = true)\n",
      " |-- OrderQty: integer (nullable = true)\n",
      " |-- ProductID: integer (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- UnitPriceDiscount: double (nullable = true)\n",
      " |-- LineTotal: double (nullable = true)\n",
      " |-- rowguid: string (nullable = true)\n",
      " |-- ModifiedDate: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfDetail.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3abe3a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|         SalesPerson|     TotalNetPrice|\n",
      "+--------------------+------------------+\n",
      "|adventure-works\\l...|170064.83797000017|\n",
      "|adventure-works\\p...|               0.0|\n",
      "|adventure-works\\d...|               0.0|\n",
      "|adventure-works\\shu0|112646.20642400002|\n",
      "|adventure-works\\m...|               0.0|\n",
      "|adventure-works\\j...|               0.0|\n",
      "|adventure-works\\j...|               0.0|\n",
      "|adventure-works\\jae0| 426024.1086639994|\n",
      "|adventure-works\\g...|               0.0|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfCustomer.select(\"SalesPerson\", \"CustomerID\") \\\n",
    "    .join(dfHeader.select(\"SalesOrderID\", \"CustomerID\"), \"CustomerID\", \"left_outer\") \\\n",
    "    .join(dfDetail.select(\"SalesOrderID\", \"OrderQty\", \"UnitPrice\", \"UnitPriceDiscount\"), \"SalesOrderID\", \"left_outer\") \\\n",
    "    .na.fill(value=0,subset=[\"OrderQty\", \"UnitPrice\", \"UnitPriceDiscount\"]) \\\n",
    "    .withColumn('NetPrice', col(\"OrderQty\") * col(\"UnitPrice\") * (1 - col(\"UnitPriceDiscount\"))) \\\n",
    "    .groupBy(\"SalesPerson\").agg(sum(\"NetPrice\").alias(\"TotalNetPrice\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771c2b40",
   "metadata": {},
   "source": [
    "## GraphFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "816bbd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addPyFile(\"/home/bigbenchung/spark-3.3.2-bin-hadoop3/jars/graphframes-0.8.2-spark3.0-s_2.12.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d850447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020af41a",
   "metadata": {},
   "source": [
    "Given a directed graph without self-loops, write a Spark program using GraphFrame and DataFrame to find all pairs (x, y) such that:\n",
    "\n",
    "1. x and y are different nodes in the graph. \n",
    "2. There exists a unique path of length 3 from x to y, i.e., x->z->w->y, where z and w cannot be equal to x or y.\n",
    "\n",
    "Sample Input: Given in the code below\n",
    "Expected Output: \n",
    "\n",
    "x y\n",
    "\n",
    "a d<br>\n",
    "b e<br>\n",
    "b f<br>\n",
    "\n",
    "Explanation:\n",
    "1. There is only one path of length 3 from a to d, which is a->b->c->d.\n",
    "2. There is only one path of length 3 from b to e, which is b->c->d->e.\n",
    "3. There is only one path of length 3 from b to f, which is b->c->d->f.\n",
    "4. Note that (c, g) is not included in the result because there are 2 paths of length 3 from c to g, which are c->d->e->g and c->d->f->g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2539ed38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import * \n",
    "from pyspark.sql.functions import * \n",
    "\n",
    "v = spark.createDataFrame([ \n",
    "    (\"a\", \"node_a\"), \n",
    "    (\"b\", \"node_b\"),\n",
    "    (\"c\", \"node_c\"),\n",
    "    (\"d\", \"node_d\"),\n",
    "    (\"e\", \"node_e\"),\n",
    "    (\"f\", \"node_f\"),\n",
    "    (\"g\", \"node_g\")\n",
    "], [\"id\", \"name\"])  \n",
    "\n",
    "e = spark.createDataFrame([\n",
    "    (\"a\", \"b\"),\n",
    "    (\"b\", \"c\"),\n",
    "    (\"c\", \"d\"),\n",
    "    (\"d\", \"c\"),\n",
    "    (\"d\", \"e\"),\n",
    "    (\"d\", \"f\"),\n",
    "    (\"e\", \"g\"),\n",
    "    (\"f\", \"g\")\n",
    "], [\"src\", \"dst\"]) \n",
    "\n",
    "g = GraphFrame(v, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d2f6780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|  b|  e|\n",
      "|  a|  d|\n",
      "|  b|  f|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = g.find(\"(a)-[e1]->(b);(b)-[e2]->(c);(c)-[e3]->(d)\") \\\n",
    "        .filter(\"b != a AND b != d AND c != a AND c != d\") \\\n",
    "        .select(col(\"a.id\").alias(\"x\"), col(\"d.id\").alias(\"y\")) \\\n",
    "        .groupBy(\"x\", \"y\").count() \\\n",
    "        .filter(\"count = 1\") \\\n",
    "        .select(\"x\", \"y\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83b1b4d",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f144a5",
   "metadata": {},
   "source": [
    "Grade(<u>cid</u>, <u>sid</u>, score)\n",
    "\n",
    "Columns with underscores are primary keys. The records of Grade are in a streaming fashion, with each record representing a student with student ID sid achieving a score for course ID cid, where the score can be one of 'A', 'B', 'C', or 'F'. \n",
    "\n",
    "Given a DStream representing the Grade table, write a Spark Streaming program to dynamically maintain the number of students achieving 'A', 'B', 'C', and 'F' for each course. After each RDD, print the current state(order doesn't matter).\n",
    "\n",
    "Example:\n",
    "RDD at time 1\n",
    "('103', '1', 'B')\n",
    "\n",
    "RDD at time 2\n",
    "('101', '2', 'A')\n",
    "\n",
    "RDD at time 3\n",
    "('101', '1', 'A')\n",
    "('101', '3', 'B')\n",
    "('102', '2', 'F')\n",
    "\n",
    "Expected output:\n",
    "After RDD at time 1\n",
    "('103', {'B': 1})\n",
    "\n",
    "After RDD at time 2\n",
    "('101', {'A': 1})\n",
    "('103', {'B': 1})\n",
    "\n",
    "After RDD at time 3\n",
    "('102', {'F': 1})\n",
    "('101', {'A': 2, 'B': 1})\n",
    "('103', {'B': 1})\n",
    "\n",
    "Explanation:\n",
    "After time 1, we know that there is 1 student in Course 103 who received a grade 'B'.\n",
    "After time 2, there is 1 student in Course 101 who received a grade 'A', and 1 student in Course 103 who received a grade 'B'.\n",
    "After time 3, there are 2 students in Course 101 who received a grade 'A', and 1 student who received a grade 'B'. In Course 103, there is 1 student who received a grade 'B'. In Course 102, there is 1 student who received a grade 'F'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6a1fd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "ssc = StreamingContext(sc, 5)\n",
    "# Provide a checkpointing directory. Required for stateful transformations\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "numPartitions = 8\n",
    "rdd = sc.textFile('data/grade.txt', numPartitions)\n",
    "rddQueue = rdd.randomSplit([1]*3, 123)\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "grade = lines.map(lambda l: tuple(l.strip().split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f7e289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 21:20:03 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 21:20:05 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[]\n",
      "24/05/20 21:20:05 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 21:20:10 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[('103', ['B']), ('101', ['A'])]\n",
      "24/05/20 21:20:10 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 21:20:15 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[('102', ['F']), ('103', ['B']), ('101', ['A', 'A', 'B'])]\n",
      "24/05/20 21:20:15 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 21:20:20 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[('102', ['F']), ('103', ['B']), ('101', ['A', 'A', 'B'])]\n",
      "24/05/20 21:20:20 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 21:20:25 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[('102', ['F']), ('103', ['B']), ('101', ['A', 'A', 'B'])]\n",
      "24/05/20 21:20:25 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 21:20:30 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[('102', ['F']), ('103', ['B']), ('101', ['A', 'A', 'B'])]\n",
      "24/05/20 21:20:30 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 21:20:33 WARN BatchedWriteAheadLog: BatchedWriteAheadLog Writer queue interrupted.\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "def updateFunc(score, cumulativeGrades):\n",
    "    \n",
    "    if cumulativeGrades is None:\n",
    "        cumulativeGrades = list()\n",
    "    \n",
    "    return cumulativeGrades + score\n",
    "\n",
    "running_tracker = grade.map(lambda x: (x[0], x[2])).updateStateByKey(updateFunc)\n",
    "\n",
    "def printResult(rdd):\n",
    "    print(rdd.collect())\n",
    "    \n",
    "running_tracker.foreachRDD(printResult)\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination(30)\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4c9e5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 21:21:24 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 21:21:25 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[]\n",
      "24/05/20 21:21:25 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 21:21:30 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[('103', {'B': 1}), ('101', {'A': 1})]\n",
      "24/05/20 21:21:30 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 21:21:35 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[('102', {'F': 1}), ('103', {'B': 1}), ('101', {'A': 2, 'B': 1})]\n",
      "24/05/20 21:21:35 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 21:21:40 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[('102', {'F': 1}), ('103', {'B': 1}), ('101', {'A': 2, 'B': 1})]\n",
      "24/05/20 21:21:40 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 21:21:45 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[('102', {'F': 1}), ('103', {'B': 1}), ('101', {'A': 2, 'B': 1})]\n",
      "24/05/20 21:21:45 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 21:21:50 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[('102', {'F': 1}), ('103', {'B': 1}), ('101', {'A': 2, 'B': 1})]\n",
      "24/05/20 21:21:50 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 21:21:54 WARN BatchedWriteAheadLog: BatchedWriteAheadLog Writer queue interrupted.\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "def updateFunc(scores, cumulativeGrades):\n",
    "    \n",
    "    if cumulativeGrades is None:\n",
    "        cumulativeGrades = dict()\n",
    "    \n",
    "    for score in scores:\n",
    "        if score not in cumulativeGrades.keys():\n",
    "            cumulativeGrades[score] = 1\n",
    "        else:\n",
    "            cumulativeGrades[score] += 1\n",
    "    \n",
    "    return cumulativeGrades\n",
    "\n",
    "running_tracker = grade.map(lambda x: (x[0], x[2])).updateStateByKey(updateFunc)\n",
    "\n",
    "def printResult(rdd):\n",
    "    print(rdd.collect())\n",
    "    \n",
    "running_tracker.foreachRDD(printResult)\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination(30)\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad6963c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 21:09:55 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_366_0 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_366_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_354_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_354_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_366_3 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_366_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_378_0 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_378_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_366_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_366_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_378_3 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_378_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_390_0 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_390_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_390_3 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_378_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_390_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_378_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_402_0 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_402_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_402_3 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_390_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_390_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_402_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_414_0 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_414_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_402_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_414_3 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_402_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_414_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_426_0 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_426_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_414_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_426_3 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_426_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_414_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_438_0 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_438_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_426_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_426_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_438_3 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_438_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_450_0 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_450_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_438_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_438_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_450_3 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_450_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_462_0 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_462_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_462_3 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_462_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_450_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_450_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_474_0 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_474_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_462_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_474_3 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_462_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_474_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_486_0 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_486_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_474_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_486_3 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_474_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_486_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_498_0 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_486_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_498_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_498_3 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_486_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_498_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_516_0 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_516_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 ERROR Executor: Exception in task 0.0 in stage 222.0 (TID 769)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:09:55 WARN TaskSetManager: Lost task 0.0 in stage 222.0 (TID 769) (172.22.144.240 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "\n",
      "24/05/20 21:09:55 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 ERROR TaskSetManager: Task 0 in stage 222.0 failed 1 times; aborting job\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_516_3 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_516_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:09:55 WARN TaskSetManager: Lost task 3.0 in stage 222.0 (TID 772) (172.22.144.240 executor driver): TaskKilled (Stage cancelled)\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_498_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_498_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 WARN BlockManager: Putting block rdd_516_1 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/05/20 21:09:55 WARN BlockManager: Block rdd_516_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:09:55 WARN TaskSetManager: Lost task 1.0 in stage 222.0 (TID 770) (172.22.144.240 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 222:==========================================>             (9 + 3) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 21:09:56 ERROR JobScheduler: Error running job streaming job 1716210595000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 18, in printResult\n",
      "    print(rdd.collect())\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 1197, in collect\n",
      "    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 222.0 failed 1 times, most recent failure: Lost task 0.0 in stage 222.0 (TID 769) (172.22.144.240 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.GeneratedMethodAccessor147.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 21:10:00 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_354_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_354_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_366_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_366_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_378_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_378_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_390_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_390_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_402_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_402_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_414_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_414_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_426_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_426_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_366_3 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_366_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_438_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_438_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_378_3 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_378_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_450_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_450_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_462_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_390_3 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_390_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_462_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_366_0 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_366_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_402_3 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_402_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_474_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_474_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_414_3 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_414_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_486_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_486_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_378_0 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_378_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_426_3 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_426_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_498_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_498_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_390_0 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_390_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_438_3 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_438_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_516_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_516_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_402_0 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_402_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_450_3 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_450_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_528_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_528_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_414_0 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_414_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 ERROR Executor: Exception in task 1.0 in stage 238.0 (TID 783)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\t... 8 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_462_3 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_462_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 WARN TaskSetManager: Lost task 1.0 in stage 238.0 (TID 783) (172.22.144.240 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "\n",
      "24/05/20 21:10:00 ERROR TaskSetManager: Task 1 in stage 238.0 failed 1 times; aborting job\n",
      "24/05/20 21:10:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 666, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\t... 27 more\n",
      "24/05/20 21:10:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_426_0 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      ".\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_426_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_474_3 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_438_0 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_474_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_438_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_486_3 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_450_0 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_486_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_450_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_498_3 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_498_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_462_0 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_462_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_474_0 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_474_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_516_3 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_516_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_486_0 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_486_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_528_3 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_528_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 WARN TaskSetManager: Lost task 3.0 in stage 238.0 (TID 785) (172.22.144.240 executor driver): TaskKilled (Stage cancelled)\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_498_0 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_498_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_516_0 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_516_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 WARN BlockManager: Putting block rdd_528_0 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/05/20 21:10:00 WARN BlockManager: Block rdd_528_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 21:10:00 WARN TaskSetManager: Lost task 0.0 in stage 238.0 (TID 782) (172.22.144.240 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 21:10:00 ERROR JobScheduler: Error running job streaming job 1716210600000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 18, in printResult\n",
      "    print(rdd.collect())\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 1197, in collect\n",
      "    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 238.0 failed 1 times, most recent failure: Lost task 1.0 in stage 238.0 (TID 783) (172.22.144.240 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.GeneratedMethodAccessor147.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 2714, in map_values_fn\n",
      "    return kv[0], f(kv[1])\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 854, in <lambda>\n",
      "    state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n",
      "  File \"/tmp/ipykernel_31597/3653572892.py\", line 3, in updateFunc\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.PartitionerAwareUnionRDD.$anonfun$compute$1(PartitionerAwareUnionRDD.scala:100)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "# Create a StreamingContext with batch interval of 5 seconds\n",
    "ssc = StreamingContext(sc, 5)\n",
    "ssc.checkpoint(\"checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44076286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DStream that will connect to localhost at port 9999\n",
    "# Start Netcat server: nc -lk 9999 \n",
    "# lines = ssc.socketTextStream('localhost', 9999)\n",
    "\n",
    "rdd = sc.textFile(\"data/adj_noun_pairs.txt\", 8).map(lambda l: tuple(l.split())).filter(lambda p: len(p)==2)\n",
    "rddQueue = rdd.randomSplit([1]*10, 123)\n",
    "lines = ssc.queueStream(rddQueue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d598d0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:14:01 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "Start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:14:09 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[(('external', 'link'), 836), (('19th', 'century'), 327), (('same', 'time'), 280), (('20th', 'century'), 266), (('first', 'time'), 264), (('other', 'hand'), 236), (('large', 'number'), 227), (('civil', 'war'), 211), (('political', 'party'), 201), (('recent', 'year'), 189)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:14:14 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 18:14:14 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[(('external', 'link'), 1622), (('19th', 'century'), 608), (('same', 'time'), 555), (('20th', 'century'), 544), (('first', 'time'), 532), (('other', 'hand'), 426), (('large', 'number'), 419), (('civil', 'war'), 412), (('recent', 'year'), 404), (('political', 'party'), 391)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:14:19 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 18:14:19 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[(('external', 'link'), 2427), (('19th', 'century'), 897), (('20th', 'century'), 834), (('same', 'time'), 830), (('first', 'time'), 799), (('civil', 'war'), 654), (('large', 'number'), 630), (('other', 'hand'), 629), (('political', 'party'), 571), (('recent', 'year'), 564)]\n",
      "24/05/20 18:14:19 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:14:24 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[(('external', 'link'), 3248), (('19th', 'century'), 1187), (('20th', 'century'), 1128), (('same', 'time'), 1115), (('first', 'time'), 1059), (('civil', 'war'), 909), (('large', 'number'), 852), (('other', 'hand'), 836), (('political', 'party'), 762), (('recent', 'year'), 756)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('external', 'link'), 4075), (('19th', 'century'), 1457), (('20th', 'century'), 1388), (('same', 'time'), 1359), (('first', 'time'), 1310), (('civil', 'war'), 1137), (('large', 'number'), 1065), (('other', 'hand'), 1036), (('political', 'party'), 953), (('recent', 'year'), 944)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:14:36 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 18:14:36 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('external', 'link'), 4890), (('19th', 'century'), 1741), (('20th', 'century'), 1676), (('same', 'time'), 1665), (('first', 'time'), 1558), (('civil', 'war'), 1352), (('large', 'number'), 1282), (('other', 'hand'), 1228), (('political', 'party'), 1149), (('recent', 'year'), 1129)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:14:47 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 18:14:47 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('external', 'link'), 5727), (('19th', 'century'), 2027), (('20th', 'century'), 1950), (('same', 'time'), 1937), (('first', 'time'), 1826), (('civil', 'war'), 1561), (('large', 'number'), 1498), (('other', 'hand'), 1443), (('political', 'party'), 1337), (('other', 'country'), 1301)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:>                                                       (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:14:51 WARN BatchedWriteAheadLog: BatchedWriteAheadLog Writer queue interrupted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:=======================>                                 (5 + 7) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:14:55 ERROR JobScheduler: Error generating jobs for time 1716200085000 ms\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\", line 410, in func\n",
      "    return oldfunc(rdd)  # type: ignore[arg-type, call-arg]\n",
      "  File \"/tmp/ipykernel_21948/1068134567.py\", line 9, in <lambda>\n",
      "    counts_sorted = running_counts.transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 1037, in sortBy\n",
      "    self.keyBy(keyfunc)  # type: ignore[type-var]\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 995, in sortByKey\n",
      "    rddSize = self.count()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 1521, in count\n",
      "    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 1508, in sum\n",
      "    return self.mapPartitions(lambda x: [sum(x)]).fold(  # type: ignore[return-value]\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 1336, in fold\n",
      "    vals = self.mapPartitions(func).collect()\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\", line 1197, in collect\n",
      "    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/bigbenchung/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": java.lang.InterruptedException\n",
      "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1302)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:334)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:943)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.GeneratedMethodAccessor131.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonTransformedDStream.compute(PythonDStream.scala:246)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)\n",
      "\tat org.apache.spark.streaming.DStreamGraph.$anonfun$generateJobs$2(DStreamGraph.scala:123)\n",
      "\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n",
      "\tat scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:75)\n",
      "\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n",
      "\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n",
      "\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n",
      "\tat org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:122)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator.$anonfun$generateJobs$1(JobGenerator.scala:254)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:252)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:186)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:91)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:90)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonTransformedDStream.compute(PythonDStream.scala:246)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)\n",
      "\tat org.apache.spark.streaming.DStreamGraph.$anonfun$generateJobs$2(DStreamGraph.scala:123)\n",
      "\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n",
      "\tat scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:75)\n",
      "\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n",
      "\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n",
      "\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n",
      "\tat org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:122)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator.$anonfun$generateJobs$1(JobGenerator.scala:254)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:252)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:186)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:91)\n",
      "\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:90)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('external', 'link'), 6503), (('19th', 'century'), 2328), (('20th', 'century'), 2238), (('same', 'time'), 2221), (('first', 'time'), 2096), (('civil', 'war'), 1775), (('large', 'number'), 1688), (('other', 'hand'), 1654), (('political', 'party'), 1537), (('other', 'country'), 1463)]\n",
      "Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:==============================================>         (10 + 2) / 12]\r"
     ]
    }
   ],
   "source": [
    "def updateFunc(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    return sum(newValues, runningCount)\n",
    "    # add the new values with the previous running count to get the new count\n",
    "    \n",
    "running_counts = lines.map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y).updateStateByKey(updateFunc)\n",
    "\n",
    "counts_sorted = running_counts.transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n",
    "\n",
    "def printResult(rdd):\n",
    "    print(rdd.take(10))\n",
    "    \n",
    "counts_sorted.foreachRDD(printResult)\n",
    "\n",
    "ssc.start()  # Start the computation\n",
    "print(\"Start\")\n",
    "ssc.awaitTermination(50)  # Wait for the computation to terminate\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcd8e78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:32:47 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "Start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('external', 'link'), 836), (('19th', 'century'), 327), (('same', 'time'), 280), (('other', 'hand'), 236), (('large', 'number'), 227), (('civil', 'war'), 211), (('political', 'party'), 201), (('recent', 'year'), 189), (('other', 'country'), 179), (('many', 'people'), 174)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:32:59 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 18:32:59 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 18:32:59 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[(('external', 'link'), 1622), (('19th', 'century'), 608), (('same', 'time'), 555), (('other', 'hand'), 426), (('large', 'number'), 419), (('civil', 'war'), 412), (('recent', 'year'), 404), (('political', 'party'), 391), (('other', 'country'), 360), (('many', 'people'), 333)]\n",
      "24/05/20 18:32:59 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:33:03 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[(('external', 'link'), 2427), (('19th', 'century'), 897), (('same', 'time'), 830), (('civil', 'war'), 654), (('large', 'number'), 630), (('other', 'hand'), 629), (('political', 'party'), 571), (('recent', 'year'), 564), (('other', 'country'), 549), (('many', 'people'), 500)]\n",
      "24/05/20 18:33:03 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:33:09 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[(('external', 'link'), 3248), (('19th', 'century'), 1187), (('same', 'time'), 1115), (('civil', 'war'), 909), (('large', 'number'), 852), (('other', 'hand'), 836), (('political', 'party'), 762), (('recent', 'year'), 756), (('other', 'country'), 741), (('many', 'people'), 653)]\n",
      "24/05/20 18:33:09 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:33:14 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[(('external', 'link'), 4075), (('19th', 'century'), 1457), (('same', 'time'), 1359), (('civil', 'war'), 1137), (('large', 'number'), 1065), (('other', 'hand'), 1036), (('political', 'party'), 953), (('recent', 'year'), 944), (('other', 'country'), 921), (('many', 'people'), 811)]\n",
      "24/05/20 18:33:14 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('external', 'link'), 4890), (('19th', 'century'), 1741), (('same', 'time'), 1665), (('civil', 'war'), 1352), (('large', 'number'), 1282), (('other', 'hand'), 1228), (('political', 'party'), 1149), (('recent', 'year'), 1129), (('other', 'country'), 1095), (('many', 'people'), 1015)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:33:25 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[(('external', 'link'), 5727), (('19th', 'century'), 2027), (('same', 'time'), 1937), (('civil', 'war'), 1561), (('large', 'number'), 1498), (('other', 'hand'), 1443), (('political', 'party'), 1337), (('other', 'country'), 1301), (('recent', 'year'), 1283), (('many', 'people'), 1207)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:33:32 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "24/05/20 18:33:32 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[(('external', 'link'), 6503), (('19th', 'century'), 2328), (('same', 'time'), 2221), (('civil', 'war'), 1775), (('large', 'number'), 1688), (('other', 'hand'), 1654), (('political', 'party'), 1537), (('other', 'country'), 1463), (('recent', 'year'), 1452), (('many', 'people'), 1399)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 88:====>                                                   (1 + 11) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:33:37 WARN BatchedWriteAheadLog: BatchedWriteAheadLog Writer queue interrupted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 88:===================================================>    (11 + 1) / 12]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/20 18:33:38 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "[(('external', 'link'), 7340), (('19th', 'century'), 2595), (('same', 'time'), 2482), (('civil', 'war'), 2005), (('large', 'number'), 1895), (('other', 'hand'), 1838), (('political', 'party'), 1712), (('other', 'country'), 1662), (('recent', 'year'), 1640), (('many', 'people'), 1564)]\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "def updateFunc(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    return sum(newValues, runningCount)\n",
    "    # add the new values with the previous running count to get the new count\n",
    "    \n",
    "running_counts = lines.map(lambda word: (word, 1)).updateStateByKey(updateFunc)\n",
    "\n",
    "counts_sorted = running_counts.map(lambda x: (x[0][1], x)) \\\n",
    "                .reduceByKey(lambda x,y: x if x[1] > y[1] else y) \\\n",
    "                .map(lambda x: x[1]) \\\n",
    "                .transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n",
    "\n",
    "def printResult(rdd):\n",
    "    print(rdd.take(10))\n",
    "    \n",
    "counts_sorted.foreachRDD(printResult)\n",
    "\n",
    "ssc.start()  # Start the computation\n",
    "print(\"Start\")\n",
    "ssc.awaitTermination(50)  # Wait for the computation to terminate\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4966cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
