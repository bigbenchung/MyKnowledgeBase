{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "919e6505",
   "metadata": {},
   "source": [
    "### Use multi-threading to submit jobs in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64d72454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 3 reports: Pi is roughly 3.142096\n",
      "Worker 1 reports: Pi is roughly 3.1422232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>   (0 + 0) / 10][Stage 3:==> (6 + 4) / 10][Stage 4:>   (0 + 2) / 10]\r",
      "\r",
      "[Stage 2:>                 (0 + 0) / 10][Stage 4:>                 (0 + 6) / 10]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 2 reports: Pi is roughly 3.1424288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                 (0 + 3) / 10][Stage 4:============>     (7 + 3) / 10]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 4 reports: Pi is roughly 3.1419832\n",
      "Worker 0 reports: Pi is roughly 3.1426512\n",
      "Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:====================================================>     (9 + 1) / 10]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import random\n",
    "\n",
    "partitions = 10\n",
    "n = 500000 * partitions\n",
    "\n",
    "# use different seeds in different threads and different partitions\n",
    "# a bit ugly, since mapPartitionsWithIndex takes a function with only index\n",
    "# and it as parameters\n",
    "def f1(index, it):\n",
    "    random.seed(index + 987231)\n",
    "    for i in it:\n",
    "        x = random.random() * 2 - 1\n",
    "        y = random.random() * 2 - 1\n",
    "        yield 1 if x ** 2 + y ** 2 < 1 else 0\n",
    "\n",
    "def f2(index, it):\n",
    "    random.seed(index + 987232)\n",
    "    for i in it:\n",
    "        x = random.random() * 2 - 1\n",
    "        y = random.random() * 2 - 1\n",
    "        yield 1 if x ** 2 + y ** 2 < 1 else 0\n",
    "\n",
    "def f3(index, it):\n",
    "    random.seed(index + 987233)\n",
    "    for i in it:\n",
    "        x = random.random() * 2 - 1\n",
    "        y = random.random() * 2 - 1\n",
    "        yield 1 if x ** 2 + y ** 2 < 1 else 0\n",
    "    \n",
    "def f4(index, it):\n",
    "    random.seed(index + 987234)\n",
    "    for i in it:\n",
    "        x = random.random() * 2 - 1\n",
    "        y = random.random() * 2 - 1\n",
    "        yield 1 if x ** 2 + y ** 2 < 1 else 0\n",
    "    \n",
    "def f5(index, it):\n",
    "    random.seed(index + 987245)\n",
    "    for i in it:\n",
    "        x = random.random() * 2 - 1\n",
    "        y = random.random() * 2 - 1\n",
    "        yield 1 if x ** 2 + y ** 2 < 1 else 0\n",
    "\n",
    "f = [f1, f2, f3, f4, f5]\n",
    "    \n",
    "# the function executed in each thread/job\n",
    "def dojob(i):\n",
    "    count = sc.parallelize(range(1, n + 1), partitions) \\\n",
    "              .mapPartitionsWithIndex(f[i]).reduce(lambda a,b: a+b)\n",
    "    print(\"Worker\", i, \"reports: Pi is roughly\", 4.0 * count / n)\n",
    "\n",
    "# create and execute the threads\n",
    "threads = []\n",
    "for i in range(5):\n",
    "    t = threading.Thread(target=dojob, args=(i,))\n",
    "    threads += [t]\n",
    "    t.start()\n",
    "\n",
    "# wait for all threads to complete\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb139536",
   "metadata": {},
   "source": [
    "### Example: Finding all primes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7cef5d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 97, 113, 193, 241, 257, 337, 353, 401, 433]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "n = 500000\n",
    "allnumbers = sc.parallelize(range(2, n), 8).cache()\n",
    "composite = allnumbers.flatMap(lambda x: range(x*2, n, x)).repartition(8)\n",
    "prime = allnumbers.subtract(composite)\n",
    "print(prime.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87a4d92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62499, 62500, 62500, 62500, 62499, 62500, 62500, 62500]\n",
      "[704805, 704790, 704800, 704800, 704800, 704799, 704800, 704816]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 5169, 1, 5219, 0, 5206, 0, 5189, 0, 5165, 0, 5199, 0, 5191, 0, 5199]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 97, 113, 193]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 19, 67, 83]\n",
      "[44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542]\n"
     ]
    }
   ],
   "source": [
    "# Find the number of elements in each parttion\n",
    "def partitionsize(it): \n",
    "    yield len(list(it))\n",
    "\n",
    "print(allnumbers.mapPartitions(partitionsize).collect())\n",
    "print(composite.mapPartitions(partitionsize).collect())\n",
    "print(prime.mapPartitions(partitionsize).collect())\n",
    "print(prime.glom().take(4)[1][0:4])\n",
    "print(prime.glom().take(4)[2][0:4])\n",
    "print(prime.glom().take(4)[3][0:4])\n",
    "print(composite.glom().take(1)[0][0:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b1aa1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24], [25, 26, 27, 28, 29]]\n",
      "[[], [0, 1, 2, 3, 4, 25, 26, 27, 28, 29], [15, 16, 17, 18, 19, 20, 21, 22, 23, 24], [], [10, 11, 12, 13, 14], [5, 6, 7, 8, 9]]\n",
      "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]]\n"
     ]
    }
   ],
   "source": [
    "# repartition vs coalesce\n",
    "\n",
    "rdd1 = sc.parallelize(range(30), 6)\n",
    "print(rdd1.glom().collect())\n",
    "\n",
    "# repartition can increase or decrease the level of parallelism in this RDD. \n",
    "# Internally, this uses a chunk-based shuffle to redistribute data. \n",
    "rdd2 = rdd1.repartition(6)\n",
    "print(rdd2.glom().collect())\n",
    "\n",
    "#If you are decreasing the number of partitions in this RDD, consider using coalesce, \n",
    "# which can avoid performing a shuffle.\n",
    "# coalesce merges adjacent partitions, so it cannot fix skew issues!\n",
    "rdd3 = rdd1.coalesce(3)\n",
    "print(rdd3.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f44de9e",
   "metadata": {},
   "source": [
    "### Data Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3125c7b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "[[(8, 9), (8, 9)], [(1, 2), (96, 97)], [(240, 241), (400, 401)], [(1, 2), (800, 801), (4, 5), (12, 13)]]\n",
      "[[(8, 18), (96, 97), (240, 241), (400, 401), (800, 801), (4, 5), (12, 13)], [(1, 4)], [], []]\n",
      "<pyspark.rdd.Partitioner object at 0x7fecb15b05b0>\n",
      "<function portable_hash at 0x7fece43443a0>\n",
      "[[(8, 19), (96, 98), (240, 242), (400, 402), (800, 802), (4, 6), (12, 14)], [(1, 5)], [], []]\n",
      "None\n",
      "<function portable_hash at 0x7fece43443a0>\n",
      "[[(1, 4), (4, 5), (8, 18)], [(12, 13), (96, 97)], [(240, 241), (400, 401)], [(800, 801)]]\n",
      "<function RDD.sortByKey.<locals>.rangePartitioner at 0x7fecb0543af0>\n",
      "<function RDD.sortByKey.<locals>.rangePartitioner at 0x7fecb0543af0>\n"
     ]
    }
   ],
   "source": [
    "data = [8, 8, 1, 96, 240, 400, 1, 800, 4, 12]\n",
    "rdd = sc.parallelize(zip(data, data),4)\n",
    "print(rdd.partitioner)\n",
    "rdd = rdd.map(lambda t: (t[0], t[1]+1))\n",
    "print(rdd.partitioner)\n",
    "print(rdd.glom().collect())\n",
    "\n",
    "rdd = rdd.reduceByKey(lambda x,y: x+y)\n",
    "print(rdd.glom().collect())\n",
    "print(rdd.partitioner)\n",
    "print(rdd.partitioner.partitionFunc)\n",
    "\n",
    "rdd1 = rdd.map(lambda x: (x[0], x[1]+1))\n",
    "print(rdd1.glom().collect())\n",
    "print(rdd1.partitioner)\n",
    "\n",
    "rdd2 = rdd.mapValues(lambda x: x+1)\n",
    "print(rdd2.partitioner.partitionFunc)\n",
    "\n",
    "rdd = rdd.sortByKey()\n",
    "print(rdd.glom().collect())\n",
    "print(rdd.partitioner.partitionFunc)\n",
    "rdd3 = rdd.mapValues(lambda x: x+1)\n",
    "print(rdd3.partitioner.partitionFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21602716",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[[(240, 240), (400, 400), (1, 1), (800, 800), (4, 4), (12, 12)], [(1, 1), (96, 96)], [], [(8, 8), (8, 8)]]\n",
      "None\n",
      "[[(8, 8), (8, 8), (96, 96), (240, 240), (400, 400), (800, 800), (4, 4), (12, 12)], [(1, 1), (1, 1)], [], []]\n",
      "<pyspark.rdd.Partitioner object at 0x7fecb15b0550>\n",
      "<function portable_hash at 0x7fece43443a0>\n"
     ]
    }
   ],
   "source": [
    "data = [8, 8, 1, 96, 240, 400, 1, 800, 4, 12]\n",
    "rdd = sc.parallelize(zip(data, data),4)\n",
    "print(rdd.partitioner)\n",
    "\n",
    "# repartition does a random reparitioning, resulting in no partitioner.\n",
    "rdd1 = rdd.repartition(4)\n",
    "print(rdd1.glom().collect())\n",
    "print(rdd1.partitioner)\n",
    "\n",
    "# partitionBy partitions data by hashing the key.\n",
    "# This can only be applied on (key, value) pairs\n",
    "rdd2 = rdd.partitionBy(4)\n",
    "print(rdd2.glom().collect())\n",
    "print(rdd2.partitioner)\n",
    "print(rdd2.partitioner.partitionFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd997d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[312, 312, 312, 312, 312, 312, 312, 312, 312, 312, 312, 312, 312, 312, 312, 320]\n",
      "[2500, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[295, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147]\n",
      "[295, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147, 147]\n",
      "<function f at 0x7fecb0569160>\n"
     ]
    }
   ],
   "source": [
    "# Partition using a custom partition function\n",
    "\n",
    "def partitionsize(it): yield len(list(it))\n",
    "    \n",
    "n = 40000\n",
    "\n",
    "def f(x):\n",
    "    return x % 17\n",
    "\n",
    "data1 = list(range(0, n, 16)) + list(range(0, n, 16))\n",
    "data2 = range(0, n, 8)\n",
    "rdd1 = sc.parallelize(zip(data1, data2), 16)\n",
    "print(rdd1.mapPartitions(partitionsize).collect())\n",
    "rdd2 = rdd1.reduceByKey(lambda x,y: x+y)\n",
    "print(rdd2.mapPartitions(partitionsize).collect())\n",
    "rdd3 = rdd2.partitionBy(16, f)\n",
    "print(rdd3.mapPartitions(partitionsize).collect())\n",
    "rdd4 = rdd1.reduceByKey(lambda x,y: x+y, partitionFunc=f)\n",
    "print(rdd4.mapPartitions(partitionsize).collect())\n",
    "print(rdd4.partitioner.partitionFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adbf6807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "<function portable_hash at 0x7fece43443a0>\n",
      "[(1, (1, 1)), (17, (17, 17)), (33, (33, 33)), (49, (49, 49))]\n",
      "None\n",
      "<function portable_hash at 0x7fece43443a0>\n",
      "<function portable_hash at 0x7fece43443a0>\n",
      "8\n",
      "<function portable_hash at 0x7fece43443a0>\n",
      "[(0, (0, 0)), (8, (8, 8)), (16, (16, 16)), (24, (24, 24))]\n",
      "None\n",
      "16\n",
      "<function portable_hash at 0x7fece43443a0>\n",
      "[(0, (0, 0)), (16, (16, 16)), (32, (32, 32)), (48, (48, 48))]\n"
     ]
    }
   ],
   "source": [
    "# Join two RDDs not co-partitioned\n",
    "# The resulting RDD has twice the partition number\n",
    "\n",
    "a = sc.parallelize(zip(range(10000), range(10000)), 8)\n",
    "b = sc.parallelize(zip(range(10000), range(10000)), 8)\n",
    "c = a.join(b)\n",
    "print(c.getNumPartitions())\n",
    "print(c.partitioner.partitionFunc)\n",
    "print(c.glom().take(2)[1][0:4])\n",
    "\n",
    "# After a shuffling operation, the resulting RDD is hash partitioned\n",
    "print(a.partitioner)\n",
    "a = a.reduceByKey(lambda x,y: x+y)\n",
    "print(a.partitioner.partitionFunc)\n",
    "b = b.reduceByKey(lambda x,y: x+y)\n",
    "print(b.partitioner.partitionFunc)\n",
    "\n",
    "# Join two RDDs co-partitioned: no shuffle is needed and partition number is the same\n",
    "c = a.join(b)\n",
    "print(c.getNumPartitions())\n",
    "print(c.partitioner.partitionFunc)\n",
    "print(c.glom().first()[0:4])\n",
    "\n",
    "# coalesce/repartition removes the partitioner.\n",
    "b = b.coalesce(8)\n",
    "print(b.partitioner)\n",
    "c = a.join(b)  # This join still requires a shuffle\n",
    "print(c.getNumPartitions())\n",
    "print(c.partitioner.partitionFunc)\n",
    "print(c.glom().first()[0:4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee60e8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "<function portable_hash at 0x7fece43443a0>\n",
      "[(0, (0, 0)), (12, (12, 12)), (24, (24, 24)), (36, (36, 36))]\n",
      "8\n",
      "<function portable_hash at 0x7fece43443a0>\n",
      "[(0, (0, 0)), (8, (8, 8)), (16, (16, 16)), (24, (24, 24))]\n"
     ]
    }
   ],
   "source": [
    "# Create two RDDs with different number of partitions\n",
    "a = sc.parallelize(zip(range(10000), range(10000)), 4)\n",
    "b = sc.parallelize(zip(range(10000), range(10000)), 8)\n",
    "\n",
    "# They are not co-partitioned because they have different numbers of partitions.\n",
    "a = a.reduceByKey(lambda x,y: x+y)\n",
    "b = b.reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "c = a.join(b)\n",
    "print(c.getNumPartitions())\n",
    "print(c.partitioner.partitionFunc)\n",
    "print(c.glom().first()[0:4])\n",
    "\n",
    "# To avoid a third shuffle, use the same partition number in the first two shuffles:\n",
    "a = sc.parallelize(zip(range(10000), range(10000)), 4)\n",
    "b = sc.parallelize(zip(range(10000), range(10000)), 8)\n",
    "\n",
    "a = a.reduceByKey(lambda x,y: x+y, 8)\n",
    "b = b.reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "c = a.join(b)\n",
    "print(c.getNumPartitions())\n",
    "print(c.partitioner.partitionFunc)\n",
    "print(c.glom().first()[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37b43d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42]\n",
      "[42]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# lookup: Return the list of values in the RDD for key key. \n",
    "# This operation is done efficiently if the RDD has a known partitioner by only searching the partition that the key maps to.\n",
    "\n",
    "l = range(1000)\n",
    "rdd = sc.parallelize(zip(l, l), 10)\n",
    "print(rdd.lookup(42))  # slow\n",
    "sorted = rdd.sortByKey() # induces a range partitioner\n",
    "print(sorted.lookup(42))  # fast\n",
    "print(sorted.lookup(1024)) # fast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c7ec13",
   "metadata": {},
   "source": [
    "### Partitioning in DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84f22acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [Row(a=5, b=1, c=1)], [Row(a=1, b=2, c=2), Row(a=1, b=2, c=1), Row(a=1, b=2, c=3), Row(a=1, b=2, c=3), Row(a=1, b=1, c=2), Row(a=1, b=1, c=1), Row(a=1, b=1, c=3), Row(a=1, b=1, c=3), Row(a=1, b=3, c=2), Row(a=1, b=3, c=1), Row(a=1, b=3, c=3), Row(a=1, b=3, c=3), Row(a=1, b=3, c=2), Row(a=1, b=3, c=1), Row(a=1, b=3, c=3), Row(a=1, b=3, c=3)], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [Row(a=3, b=4, c=4)], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [Row(a=2, b=1, c=1), Row(a=2, b=1, c=4), Row(a=2, b=1, c=5), Row(a=2, b=4, c=1), Row(a=2, b=4, c=4), Row(a=2, b=4, c=5), Row(a=2, b=5, c=1), Row(a=2, b=5, c=4), Row(a=2, b=5, c=5)], [], [], [], [], [], [], [], [], [], [], [], [Row(a=4, b=5, c=5), Row(a=4, b=5, c=2), Row(a=4, b=2, c=5), Row(a=4, b=2, c=2)], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "None\n",
      "+---+---+---+-----------+---------------+\n",
      "|  a|  b|  c|    hash(a)|(hash(a) % 200)|\n",
      "+---+---+---+-----------+---------------+\n",
      "|  5|  1|  1| 1607884268|             68|\n",
      "|  1|  2|  2|-1712319331|           -131|\n",
      "|  1|  2|  1|-1712319331|           -131|\n",
      "|  1|  2|  3|-1712319331|           -131|\n",
      "|  1|  2|  3|-1712319331|           -131|\n",
      "|  1|  1|  2|-1712319331|           -131|\n",
      "|  1|  1|  1|-1712319331|           -131|\n",
      "|  1|  1|  3|-1712319331|           -131|\n",
      "|  1|  1|  3|-1712319331|           -131|\n",
      "|  1|  3|  2|-1712319331|           -131|\n",
      "|  1|  3|  1|-1712319331|           -131|\n",
      "|  1|  3|  3|-1712319331|           -131|\n",
      "|  1|  3|  3|-1712319331|           -131|\n",
      "|  1|  3|  2|-1712319331|           -131|\n",
      "|  1|  3|  1|-1712319331|           -131|\n",
      "|  1|  3|  3|-1712319331|           -131|\n",
      "|  1|  3|  3|-1712319331|           -131|\n",
      "|  3|  4|  4|  519220707|            107|\n",
      "|  2|  1|  1| -797927272|            -72|\n",
      "|  2|  1|  4| -797927272|            -72|\n",
      "+---+---+---+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Hash partitioner in SparkSQL\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False) \n",
    "\n",
    "import pyspark.sql.functions\n",
    "\n",
    "data1 = [1, 2, 1, 1, 2, 3, 4, 4, 5, 2, 1]\n",
    "data2 = [2, 1, 1, 3, 4, 4, 5, 2, 1, 5, 3]\n",
    "\n",
    "df1 = spark.createDataFrame(zip(data1, data2), ['a', 'b'])\n",
    "df2 = spark.createDataFrame(zip(data1, data2), ['a', 'c'])\n",
    "\n",
    "df1 = df1.join(df2, 'a')\n",
    "print(df1.rdd.getNumPartitions())\n",
    "print(df1.rdd.glom().collect())\n",
    "print(df1.rdd.partitioner)  # This doesn't work for dataframes, as the RDD underlying a dataframe is virtual\n",
    "\n",
    "# SparkSQL uses MurmurHash to make generating adversarial data more difficult\n",
    "# Calling SparkSQL's hash function\n",
    "df1.select('*', pyspark.sql.functions.hash(df1['a']), pyspark.sql.functions.hash(df1['a']) % 200).show()\n",
    "\n",
    "# Calling Python's hash function\n",
    "print(hash(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfc076a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "6\n",
      "[[Row(a=1, b=2)], [Row(a=2, b=1), Row(a=1, b=1)], [Row(a=1, b=3), Row(a=2, b=4)], [Row(a=3, b=4), Row(a=4, b=5)], [Row(a=4, b=2), Row(a=5, b=1)], [Row(a=2, b=5), Row(a=1, b=3)]]\n",
      "+---+-----+\n",
      "|  a|count|\n",
      "+---+-----+\n",
      "|  5|    1|\n",
      "|  1|    4|\n",
      "|  3|    1|\n",
      "|  2|    3|\n",
      "|  4|    2|\n",
      "+---+-----+\n",
      "\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)  # Default in Spark 2.x \n",
    "print(spark.conf.get('spark.sql.shuffle.partitions'))  # number of partitions in a shuffle, default is 200\n",
    "\n",
    "#spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", True)  # Default in Spark 3.x \n",
    "#  When this is set to True, Spark will coalesce contiguous shuffle partitions according to the target size\n",
    "# (specified by spark.sql.adaptive.advisoryPartitionSizeInBytes, default is 64 MB), to avoid too many small tasks.\n",
    "#  May change this if your dataframe is not too large to get better parallelism. \n",
    "\n",
    "data1 = [1, 2, 1, 1, 2, 3, 4, 4, 5, 2, 1]\n",
    "data2 = [2, 1, 1, 3, 4, 4, 5, 2, 1, 5, 3]\n",
    "\n",
    "df1 = spark.createDataFrame(zip(data1, data2), ['a', 'b'])\n",
    "print(df1.rdd.getNumPartitions())\n",
    "print(df1.rdd.glom().collect())\n",
    "\n",
    "df2 = df1.groupBy('a').count()\n",
    "df2.show()\n",
    "print(df2.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c48fa49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 118:============================>                            (3 + 3) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "n = 1000000\n",
    "partitions = 40\n",
    "df = spark.range(n)  # More efficient than using parallelize(range()) \n",
    "df = df.select(df[0].alias('a'), df[0].alias('b')).cache()\n",
    "df.take(3)\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False) # default is True\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.parallelismFirst\", False)  # default is True in Spark 3.2\n",
    "# When true, Spark ignores the target size specified by spark.sql.adaptive.advisoryPartitionSizeInBytes \n",
    "# (default 64MB) when coalescing contiguous shuffle partitions, and only respect the minimum partition\n",
    "# size specified by spark.sql.adaptive.coalescePartitions.minPartitionSize (default 1MB), to maximize the \n",
    "# parallelism. This is to avoid performance regression when enabling adaptive query execution. \n",
    "\n",
    "\n",
    "#spark.conf.set('spark.sql.shuffle.partitions', partitions)  # number of partitions in a shuffle, default is 200\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "print(df.rdd.getNumPartitions())\n",
    "\n",
    "df1 = df.groupBy(df[0]).count()\n",
    "print(df1.rdd.getNumPartitions())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7edd721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a| a1| b1|\n",
      "+---+---+---+\n",
      "|  0|  0|  0|\n",
      "|  1|  1|  1|\n",
      "|  2|  2|  2|\n",
      "|  3|  3|  3|\n",
      "|  4|  4|  4|\n",
      "|  5|  5|  5|\n",
      "|  6|  6|  6|\n",
      "|  7|  7|  7|\n",
      "|  8|  8|  8|\n",
      "|  9|  9|  9|\n",
      "| 10| 10| 10|\n",
      "| 11| 11| 11|\n",
      "| 12| 12| 12|\n",
      "| 13| 13| 13|\n",
      "| 14| 14| 14|\n",
      "| 15| 15| 15|\n",
      "| 16| 16| 16|\n",
      "| 17| 17| 17|\n",
      "| 18| 18| 18|\n",
      "| 19| 19| 19|\n",
      "+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [a#214L, a1#215L, b1#219L]\n",
      "   +- BroadcastHashJoin [a#214L], [a#218L], Inner, BuildRight, false\n",
      "      :- Filter isnotnull(a#214L)\n",
      "      :  +- Scan ExistingRDD[a#214L,a1#215L]\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=421]\n",
      "         +- Filter isnotnull(a#218L)\n",
      "            +- Scan ExistingRDD[a#218L,b1#219L]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Join hints\n",
    "\n",
    "a = spark.createDataFrame(zip(range(1000), range(1000)), ['a', 'a1'])\n",
    "b = spark.createDataFrame(zip(range(1000), range(1000)), ['a', 'b1'])\n",
    "\n",
    "#c = a.join(b, 'a')\n",
    "c = a.join(b.hint('broadcast'), 'a')\n",
    "#c = a.join(b.hint('shuffle_hash'), 'a')\n",
    "# c = a.join(b.hint('merge'), 'a')\n",
    "c.show()\n",
    "\n",
    "c.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee4d611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
