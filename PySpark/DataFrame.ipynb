{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f86d7efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|dept|avg(age)|\n",
      "+----+--------+\n",
      "| CSE|    25.0|\n",
      "| ECE|    28.0|\n",
      "|MATH|    35.0|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create a DataFrame\n",
    "data_df = spark.createDataFrame([(\"Brooke\", \"CSE\", 20), (\"Denny\", \"ECE\", 31), (\"Jules\", \"CSE\", 30),\n",
    "                                 (\"Alice\", \"MATH\", 35), (\"Bob\", \"ECE\", 25)], [\"name\", \"dept\", \"age\"])\n",
    "# Group the same names together, aggregate their ages, and compute an average\n",
    "avg_df = data_df.groupBy(\"dept\").agg(avg(\"age\"))\n",
    "# Show the results of the final execution\n",
    "avg_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafa529c",
   "metadata": {},
   "source": [
    "## Constructing a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a61c15af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n",
      "StructType([StructField('Id', StringType(), True), StructField('First', StringType(), True), StructField('Last', StringType(), True), StructField('Url', StringType(), True), StructField('Published', StringType(), True), StructField('Hits', IntegerType(), True), StructField('Campaigns', ArrayType(StringType(), True), True)])\n",
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- Url: string (nullable = true)\n",
      " |-- Published: string (nullable = true)\n",
      " |-- Hits: integer (nullable = true)\n",
      " |-- Campaigns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define schema for our data using DDL (Data Definition Language)\n",
    "\n",
    "schema = \"Id: STRING, First: STRING, Last: STRING, Url: STRING, Published: STRING, Hits: INT, Campaigns: ARRAY<STRING>\"\n",
    "# Create our static data\n",
    "data = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\", \"LinkedIn\"]],\n",
    "        [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\",\"LinkedIn\"]],\n",
    "        [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\",\"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "        [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568,[\"twitter\", \"FB\"]],\n",
    "        [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\",\"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "        [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568,[\"twitter\", \"LinkedIn\"]]\n",
    "]\n",
    "# Create a DataFrame using the schema defined above\n",
    "blogs_df = spark.createDataFrame(data, schema)\n",
    "# Show the DataFrame; it should reflect our table above\n",
    "blogs_df.show()\n",
    "# Print the schema used by Spark to process the DataFrame\n",
    "print(blogs_df.schema)\n",
    "blogs_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2483b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| _1|       _2|     _3|               _4|       _5|   _6|                  _7|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n",
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: string (nullable = true)\n",
      " |-- _4: string (nullable = true)\n",
      " |-- _5: string (nullable = true)\n",
      " |-- _6: long (nullable = true)\n",
      " |-- _7: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame without specifying schema: the schema will be inferred\n",
    "\n",
    "blogs_df2 = spark.createDataFrame(data)\n",
    "blogs_df2.show()\n",
    "blogs_df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b89c183d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|count|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+-----+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|    1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|    2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|    3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|    4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|    5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|    6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+-----+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n",
      "root\n",
      " |-- count: long (nullable = true)\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- Url: string (nullable = true)\n",
      " |-- Published: string (nullable = true)\n",
      " |-- Hits: long (nullable = true)\n",
      " |-- Campaigns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame from an RDD of tuples with column names, type is inferred\n",
    "blogs_df2 = spark.createDataFrame(data, ['count', 'First', 'Last', 'Url', 'Published', 'Hits', 'Campaigns'])\n",
    "blogs_df2.show()\n",
    "blogs_df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c222545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(count=1, First='Jules', Last='Damji', Url='https://tinyurl.1', Published='1/4/2016', Hits=4535, Campaigns=['twitter', 'LinkedIn']),\n",
       " Row(count=2, First='Brooke', Last='Wenig', Url='https://tinyurl.2', Published='5/5/2018', Hits=8908, Campaigns=['twitter', 'LinkedIn'])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd returns the content as an RDD of Rows\n",
    "blogs_df2.rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca421d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jules'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs_df2.rdd.take(2)[0]['First']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77e3a515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jules'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs_df2.rdd.take(1)[0].First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16b7ff10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Row.count(value, /)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try changing the column name to 'count'.   Then this will not work due to naming conflict \n",
    "blogs_df2.rdd.take(1)[0].count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51901706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because count() is a function for class Row\n",
    "blogs_df2.rdd.take(1)[0].count('Jules')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6065c5",
   "metadata": {},
   "source": [
    "### Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d7aee1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Id', 'First', 'Last', 'Url', 'Published', 'Hits', 'Campaigns']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6235b1be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Id'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access a particular column with col and it returns a Column type\n",
    "blogs_df['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f491a64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Id'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another way to access a column - but be aware of naming conflicts\n",
    "blogs_df.Id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d42503",
   "metadata": {},
   "source": [
    "### Dataframe operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6818ce1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| Id|    First|\n",
      "+---+---------+\n",
      "|  1|    Jules|\n",
      "|  2|   Brooke|\n",
      "|  3|    Denny|\n",
      "|  4|Tathagata|\n",
      "|  5|    Matei|\n",
      "|  6|  Reynold|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Projection: Retrieve specific columns from the dataframe\n",
    "blogs_df.select(blogs_df['Id'], 'First').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f2cf58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-----------------+---------+----+-------------------+\n",
      "| Id| First| Last|              Url|Published|Hits|          Campaigns|\n",
      "+---+------+-----+-----------------+---------+----+-------------------+\n",
      "|  2|Brooke|Wenig|https://tinyurl.2| 5/5/2018|8908|[twitter, LinkedIn]|\n",
      "+---+------+-----+-----------------+---------+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Selection: Retrieve specific rows\n",
    "blogs_df.filter(blogs_df.First=='Brooke').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7c4a898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-----------------+---------+----+-------------------+\n",
      "| Id| First| Last|              Url|Published|Hits|          Campaigns|\n",
      "+---+------+-----+-----------------+---------+----+-------------------+\n",
      "|  2|Brooke|Wenig|https://tinyurl.2| 5/5/2018|8908|[twitter, LinkedIn]|\n",
      "+---+------+-----+-----------------+---------+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Can also use SQL syntax in the selection condition\n",
    "\n",
    "blogs_df.where(\"First='Brooke'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b1d6e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+----------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|(Hits + 1)|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+----------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|      4536|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|      8909|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|      7660|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|     10569|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|     40579|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|     25569|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding/changing columns (equivalent to map)\n",
    "\n",
    "blogs_df.select('*', blogs_df['Hits'] + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78da28cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| Id|(Hits + 1)|\n",
      "+---+----------+\n",
      "|  1|      4536|\n",
      "|  2|      8909|\n",
      "|  3|      7660|\n",
      "|  4|     10569|\n",
      "|  5|     40579|\n",
      "|  6|     25569|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another way:\n",
    "\n",
    "blogs_df.select('Id', col('Hits') + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c88037f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| Id|NewHits|\n",
      "+---+-------+\n",
      "|  1|   4536|\n",
      "|  2|   8909|\n",
      "|  3|   7660|\n",
      "|  4|  10569|\n",
      "|  5|  40579|\n",
      "|  6|  25569|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# giving a name to the new column:\n",
    "\n",
    "blogs_df.select('Id', blogs_df.Hits + 1).withColumnRenamed('(Hits + 1)', 'NewHits').show()\n",
    "\n",
    "# Because DataFrame transformations are immutable, when we\n",
    "# rename a column using withColumnRenamed() we get a new Data�𪀔n# Frame while retaining the original with the old column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43495625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| Id|NewHits|\n",
      "+---+-------+\n",
      "|  1|   4536|\n",
      "|  2|   8909|\n",
      "|  3|   7660|\n",
      "|  4|  10569|\n",
      "|  5|  40579|\n",
      "|  6|  25569|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another way: similar AS \n",
    "\n",
    "blogs_df.select('Id', (blogs_df.Hits + 1).alias('NewHits')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a39a107b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort by a column\n",
    "\n",
    "blogs_df.sort('Hits').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f95d0603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort by a column in descending order\n",
    "\n",
    "blogs_df.sort('Hits', ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c5f7281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort by a column in descending order\n",
    "\n",
    "blogs_df.sort(col('Hits').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7816c6dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CallNumber: integer (nullable = true)\n",
      " |-- UnitID: string (nullable = true)\n",
      " |-- IncidentNumber: integer (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- CallDate: string (nullable = true)\n",
      " |-- WatchDate: string (nullable = true)\n",
      " |-- CallFinalDisposition: string (nullable = true)\n",
      " |-- AvailableDtTm: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- OriginalPriority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- FinalPriority: integer (nullable = true)\n",
      " |-- ALSUnit: boolean (nullable = true)\n",
      " |-- CallTypeGroup: string (nullable = true)\n",
      " |-- NumAlarms: integer (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- UnitSequenceInCallDispatch: integer (nullable = true)\n",
      " |-- FirePreventionDistrict: string (nullable = true)\n",
      " |-- SupervisorDistrict: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      " |-- Delay: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "# Load an external file as a DataFrame\n",
    "\n",
    "# Programmatic way to define a schema\n",
    "fire_schema = StructType([StructField('CallNumber', IntegerType(), True),\n",
    "    StructField('UnitID', StringType(), True),\n",
    "    StructField('IncidentNumber', IntegerType(), True),\n",
    "    StructField('CallType', StringType(), True),\n",
    "    StructField('CallDate', StringType(), True),\n",
    "    StructField('WatchDate', StringType(), True),\n",
    "    StructField('CallFinalDisposition', StringType(), True),\n",
    "    StructField('AvailableDtTm', StringType(), True),\n",
    "    StructField('Address', StringType(), True),\n",
    "    StructField('City', StringType(), True),\n",
    "    StructField('Zipcode', IntegerType(), True),\n",
    "    StructField('Battalion', StringType(), True),\n",
    "    StructField('StationArea', StringType(), True),\n",
    "    StructField('Box', StringType(), True),\n",
    "    StructField('OriginalPriority', StringType(), True),\n",
    "    StructField('Priority', StringType(), True),\n",
    "    StructField('FinalPriority', IntegerType(), True),\n",
    "    StructField('ALSUnit', BooleanType(), True),\n",
    "    StructField('CallTypeGroup', StringType(), True),\n",
    "    StructField('NumAlarms', IntegerType(), True),\n",
    "    StructField('UnitType', StringType(), True),\n",
    "    StructField('UnitSequenceInCallDispatch', IntegerType(), True),\n",
    "    StructField('FirePreventionDistrict', StringType(), True),\n",
    "    StructField('SupervisorDistrict', StringType(), True),\n",
    "    StructField('Neighborhood', StringType(), True),\n",
    "    StructField('Location', StringType(), True),\n",
    "    StructField('RowID', StringType(), True),\n",
    "    StructField('Delay', FloatType(), True)])\n",
    "\n",
    "# Use the DataFrameReader interface to read a CSV file\n",
    "sf_fire_file = \"data/sf-fire-calls.csv\"\n",
    "fire_df = spark.read.csv(sf_fire_file, header=True, schema=fire_schema)\n",
    "\n",
    "fire_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "264fc89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+--------------+\n",
      "|IncidentNumber|AvailableDtTm         |CallType      |\n",
      "+--------------+----------------------+--------------+\n",
      "|2003235       |01/11/2002 01:51:44 AM|Structure Fire|\n",
      "|2003250       |01/11/2002 04:16:46 AM|Vehicle Fire  |\n",
      "|2003259       |01/11/2002 06:01:58 AM|Alarms        |\n",
      "|2003279       |01/11/2002 08:03:26 AM|Structure Fire|\n",
      "|2003301       |01/11/2002 09:46:44 AM|Alarms        |\n",
      "+--------------+----------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Projection & selection\n",
    "few_fire_df = fire_df.select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\") \\\n",
    "    .where(col(\"CallType\") != \"Medical Incident\")\n",
    "few_fire_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0ad266d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|        CallType|\n",
      "+----------------+\n",
      "|  Structure Fire|\n",
      "|Medical Incident|\n",
      "|Medical Incident|\n",
      "|    Vehicle Fire|\n",
      "|          Alarms|\n",
      "|  Structure Fire|\n",
      "|          Alarms|\n",
      "|          Alarms|\n",
      "|Medical Incident|\n",
      "|Medical Incident|\n",
      "|Medical Incident|\n",
      "|  Structure Fire|\n",
      "|Medical Incident|\n",
      "|Medical Incident|\n",
      "|  Structure Fire|\n",
      "|  Structure Fire|\n",
      "|  Structure Fire|\n",
      "|Medical Incident|\n",
      "|Medical Incident|\n",
      "|Medical Incident|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Projection doesn't remove duplicates:\n",
    "\n",
    "fire_df.select('CallType').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13359ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            CallType|\n",
      "+--------------------+\n",
      "|Elevator / Escala...|\n",
      "|         Marine Fire|\n",
      "|  Aircraft Emergency|\n",
      "|      Administrative|\n",
      "|              Alarms|\n",
      "|Odor (Strange / U...|\n",
      "|Citizen Assist / ...|\n",
      "|              HazMat|\n",
      "|Watercraft in Dis...|\n",
      "|           Explosion|\n",
      "|           Oil Spill|\n",
      "|        Vehicle Fire|\n",
      "|  Suspicious Package|\n",
      "|Extrication / Ent...|\n",
      "|               Other|\n",
      "|        Outside Fire|\n",
      "|   Traffic Collision|\n",
      "|       Assist Police|\n",
      "|Gas Leak (Natural...|\n",
      "|        Water Rescue|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use distinct to remove duplicates:\n",
    "fire_df.select('CallType').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94690a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distinct count: COUNT(DISTINCT *)\n",
    "fire_df.select('CallType').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f669cae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DistinctCallTypes|\n",
      "+-----------------+\n",
      "|               30|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is more efficient:\n",
    "fire_df.select('CallType').agg(count_distinct('CallType').alias(\"DistinctCallTypes\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "034d1c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|IncidentDate       |OnWatchDate        |AvailableDtTS      |\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 01:51:44|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 03:01:18|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 02:39:50|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 04:16:46|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 06:01:58|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Like SQL, SparkSQL supports many type-specific operations\n",
    "\n",
    "fire_ts_df = (fire_df\n",
    "    .withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\"))\n",
    "    .drop(\"CallDate\")\n",
    "    .withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\"))\n",
    "    .drop(\"WatchDate\")\n",
    "    .withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"),\n",
    "    \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "    .drop(\"AvailableDtTm\"))\n",
    "\n",
    "# Select the converted columns\n",
    "fire_ts_df.select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ace7ad48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|year(IncidentDate)|\n",
      "+------------------+\n",
      "|              2000|\n",
      "|              2001|\n",
      "|              2002|\n",
      "|              2003|\n",
      "|              2004|\n",
      "|              2005|\n",
      "|              2006|\n",
      "|              2007|\n",
      "|              2008|\n",
      "|              2009|\n",
      "|              2010|\n",
      "|              2011|\n",
      "|              2012|\n",
      "|              2013|\n",
      "|              2014|\n",
      "|              2015|\n",
      "|              2016|\n",
      "|              2017|\n",
      "|              2018|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(fire_ts_df\n",
    "    .select(year('IncidentDate'))\n",
    "    .distinct()\n",
    "    .orderBy(year('IncidentDate'))\n",
    "    .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "88cbec16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+------+\n",
      "|CallType                       |count |\n",
      "+-------------------------------+------+\n",
      "|Medical Incident               |113794|\n",
      "|Structure Fire                 |23319 |\n",
      "|Alarms                         |19406 |\n",
      "|Traffic Collision              |7013  |\n",
      "|Citizen Assist / Service Call  |2524  |\n",
      "|Other                          |2166  |\n",
      "|Outside Fire                   |2094  |\n",
      "|Vehicle Fire                   |854   |\n",
      "|Gas Leak (Natural and LP Gases)|764   |\n",
      "|Water Rescue                   |755   |\n",
      "|Odor (Strange / Unknown)       |490   |\n",
      "|Electrical Hazard              |482   |\n",
      "|Elevator / Escalator Rescue    |453   |\n",
      "|Smoke Investigation (Outside)  |391   |\n",
      "|Fuel Spill                     |193   |\n",
      "|HazMat                         |124   |\n",
      "|Industrial Accidents           |94    |\n",
      "|Explosion                      |89    |\n",
      "|Train / Rail Incident          |57    |\n",
      "|Aircraft Emergency             |36    |\n",
      "+-------------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What were the most common types of fire calls?\n",
    "\n",
    "(fire_ts_df\n",
    "    .select(\"CallType\")\n",
    "    .groupBy(\"CallType\")\n",
    "    .count()\n",
    "    .orderBy(\"count\", ascending=False)\n",
    "    .show(truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd64767a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+-----------+----------+\n",
      "|sum(NumAlarms)|       avg(Delay)| min(Delay)|max(Delay)|\n",
      "+--------------+-----------------+-----------+----------+\n",
      "|        176170|3.892364154521585|0.016666668|   1844.55|\n",
      "+--------------+-----------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# More aggregation functions\n",
    "\n",
    "(fire_ts_df\n",
    "    .select(sum(\"NumAlarms\"), avg(\"Delay\"),\n",
    "    min(\"Delay\"), max(\"Delay\"))\n",
    ".show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2410b02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+------------------+\n",
      "|CallType                     |avg(Delay)        |\n",
      "+-----------------------------+------------------+\n",
      "|Elevator / Escalator Rescue  |4.337821918278603 |\n",
      "|Marine Fire                  |6.928571377481733 |\n",
      "|Aircraft Emergency           |3.773148145940569 |\n",
      "|Administrative               |12.261111179987589|\n",
      "|Alarms                       |3.5427290570836676|\n",
      "|Odor (Strange / Unknown)     |4.9479591785645   |\n",
      "|Citizen Assist / Service Call|5.47334257750617  |\n",
      "|HazMat                       |7.527016133069992 |\n",
      "|Watercraft in Distress       |6.886904835700989 |\n",
      "|Explosion                    |4.110674162259262 |\n",
      "+-----------------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(fire_ts_df\n",
    "    .where(col(\"CallType\").isNotNull())\n",
    "    .groupBy(\"CallType\")\n",
    "    .avg(\"Delay\")\n",
    "    .show(n=10, truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2459a90f",
   "metadata": {},
   "source": [
    "### Rewriting SQL with DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cac71da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from csv files\n",
    "\n",
    "dfCustomer = spark.read.csv('data/Customer.csv', header=True, inferSchema=True)\n",
    "dfProduct = spark.read.csv('data/Product.csv', header=True, inferSchema=True)\n",
    "dfDetail = spark.read.csv('data/SalesOrderDetail.csv', header=True, inferSchema=True)\n",
    "dfHeader = spark.read.csv('data/SalesOrderHeader.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d78f048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+--------------------+---------+--------+\n",
      "|SalesOrderID|SalesOrderDetailID|                Name|UnitPrice|OrderQty|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "|       71938|            113295|Sport-100 Helmet,...|   29.994|       5|\n",
      "|       71902|            112988|Sport-100 Helmet,...|   20.994|       4|\n",
      "|       71797|            111082|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71784|            110795|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71783|            110752|Sport-100 Helmet,...|  20.2942|      11|\n",
      "|       71782|            110690|Sport-100 Helmet,...|   20.994|       7|\n",
      "|       71797|            111045|LL Road Frame - B...|  202.332|       3|\n",
      "|       71783|            110730|LL Road Frame - B...|  202.332|       6|\n",
      "|       71938|            113297|LL Road Frame - B...|  202.332|       3|\n",
      "|       71915|            113090|LL Road Frame - B...|  202.332|       2|\n",
      "|       71815|            111451|LL Road Frame - B...|  202.332|       1|\n",
      "|       71797|            111044|LL Road Frame - B...|  202.332|       1|\n",
      "|       71783|            110710|LL Road Frame - B...|  202.332|       4|\n",
      "|       71936|            113260|HL Mountain Frame...|   809.76|       4|\n",
      "|       71899|            112937|HL Mountain Frame...|   809.76|       1|\n",
      "|       71845|            112137|HL Mountain Frame...|   809.76|       2|\n",
      "|       71832|            111806|HL Mountain Frame...|   809.76|       4|\n",
      "|       71780|            110622|HL Mountain Frame...|   809.76|       1|\n",
      "|       71936|            113235|HL Mountain Frame...|   809.76|       4|\n",
      "|       71845|            112134|HL Mountain Frame...|   809.76|       3|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find all orders and details on black product,\n",
    "# return the product SalesOrderID, SalesOrderDetailID, Name, UnitPrice, and OrderQty\n",
    "\n",
    "# SELECT SalesOrderID, SalesOrderDetailID, Name, UnitPrice, OrderQty \n",
    "# FROM SalesLT.SalesOrderDetail, SalesLT.Product\n",
    "# WHERE SalesOrderDetail.ProductID = Product.ProductID AND Color = 'Black'\n",
    "\n",
    "# SELECT SalesOrderID, SalesOrderDetailID, Name, UnitPrice, OrderQty \n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# JOIN SalesLT.Product ON SalesOrderDetail.ProductID = Product.ProductID\n",
    "# WHERE Color = 'Black'\n",
    "\n",
    "# Spark SQL supports natural joins\n",
    "\n",
    "dfDetail.join(dfProduct, 'ProductID') \\\n",
    "        .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty') \\\n",
    "        .filter(\"Color='Black'\") \\\n",
    "        .show()\n",
    "\n",
    "# If we move the filter to after the projection, it still works.  Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "70f99675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+--------------------+---------+--------+\n",
      "|SalesOrderID|SalesOrderDetailID|                Name|UnitPrice|OrderQty|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "|       71938|            113278|Sport-100 Helmet,...|   20.994|       3|\n",
      "|       71936|            113228|Sport-100 Helmet,...|   20.994|       1|\n",
      "|       71902|            112980|Sport-100 Helmet,...|   20.994|       2|\n",
      "|       71797|            111075|Sport-100 Helmet,...|   20.994|       6|\n",
      "|       71784|            110794|Sport-100 Helmet,...|   20.994|      10|\n",
      "|       71783|            110751|Sport-100 Helmet,...|   20.994|      10|\n",
      "|       71782|            110709|Sport-100 Helmet,...|   20.994|       3|\n",
      "|       71938|            113295|Sport-100 Helmet,...|   29.994|       5|\n",
      "|       71902|            112988|Sport-100 Helmet,...|   20.994|       4|\n",
      "|       71797|            111082|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71784|            110795|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71783|            110752|Sport-100 Helmet,...|  20.2942|      11|\n",
      "|       71782|            110690|Sport-100 Helmet,...|   20.994|       7|\n",
      "|       71938|            113282|Sport-100 Helmet,...|   20.994|       3|\n",
      "|       71902|            112995|Sport-100 Helmet,...|   20.994|       7|\n",
      "|       71863|            112395|Sport-100 Helmet,...|   20.994|       1|\n",
      "|       71797|            111038|Sport-100 Helmet,...|   20.994|       4|\n",
      "|       71784|            110753|Sport-100 Helmet,...|   20.994|       2|\n",
      "|       71783|            110749|Sport-100 Helmet,...|  19.2445|      15|\n",
      "|       71782|            110708|Sport-100 Helmet,...|   20.994|       6|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "|SalesOrderID|SalesOrderDetailID|                Name|UnitPrice|OrderQty|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "|       71938|            113295|Sport-100 Helmet,...|   29.994|       5|\n",
      "|       71902|            112988|Sport-100 Helmet,...|   20.994|       4|\n",
      "|       71797|            111082|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71784|            110795|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71783|            110752|Sport-100 Helmet,...|  20.2942|      11|\n",
      "|       71782|            110690|Sport-100 Helmet,...|   20.994|       7|\n",
      "|       71797|            111045|LL Road Frame - B...|  202.332|       3|\n",
      "|       71783|            110730|LL Road Frame - B...|  202.332|       6|\n",
      "|       71938|            113297|LL Road Frame - B...|  202.332|       3|\n",
      "|       71915|            113090|LL Road Frame - B...|  202.332|       2|\n",
      "|       71815|            111451|LL Road Frame - B...|  202.332|       1|\n",
      "|       71797|            111044|LL Road Frame - B...|  202.332|       1|\n",
      "|       71783|            110710|LL Road Frame - B...|  202.332|       4|\n",
      "|       71936|            113260|HL Mountain Frame...|   809.76|       4|\n",
      "|       71899|            112937|HL Mountain Frame...|   809.76|       1|\n",
      "|       71845|            112137|HL Mountain Frame...|   809.76|       2|\n",
      "|       71832|            111806|HL Mountain Frame...|   809.76|       4|\n",
      "|       71780|            110622|HL Mountain Frame...|   809.76|       1|\n",
      "|       71936|            113235|HL Mountain Frame...|   809.76|       4|\n",
      "|       71845|            112134|HL Mountain Frame...|   809.76|       3|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [SalesOrderID#1113, SalesOrderDetailID#1114, Name#1063, UnitPrice#1117, OrderQty#1115]\n",
      "   +- BroadcastHashJoin [ProductID#1116], [ProductID#1062], Inner, BuildLeft, false\n",
      "      :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[3, int, false] as bigint)),false), [plan_id=1038]\n",
      "      :  +- Filter isnotnull(ProductID#1116)\n",
      "      :     +- FileScan csv [SalesOrderID#1113,SalesOrderDetailID#1114,OrderQty#1115,ProductID#1116,UnitPrice#1117] Batched: false, DataFilters: [isnotnull(ProductID#1116)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/bigbenchung/MyKnowledgeBase/PySpark/data/SalesOrderDetail.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ProductID)], ReadSchema: struct<SalesOrderID:int,SalesOrderDetailID:int,OrderQty:int,ProductID:int,UnitPrice:double>\n",
      "      +- Project [ProductID#1062, Name#1063]\n",
      "         +- Filter ((isnotnull(Color#1065) AND (Color#1065 = Black)) AND isnotnull(ProductID#1062))\n",
      "            +- FileScan csv [ProductID#1062,Name#1063,Color#1065] Batched: false, DataFilters: [isnotnull(Color#1065), (Color#1065 = Black), isnotnull(ProductID#1062)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/bigbenchung/MyKnowledgeBase/PySpark/data/Product.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Color), EqualTo(Color,Black), IsNotNull(ProductID)], ReadSchema: struct<ProductID:int,Name:string,Color:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This also works:\n",
    "\n",
    "d1 = dfDetail.join(dfProduct, 'ProductID') \\\n",
    "             .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty')\n",
    "d1.show()\n",
    "d2 = d1.filter(\"Color = 'Black'\")\n",
    "d2.show()\n",
    "d2.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aef9c542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+--------------------+---------+--------+\n",
      "|SalesOrderID|SalesOrderDetailID|                Name|UnitPrice|OrderQty|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "|       71938|            113278|Sport-100 Helmet,...|   20.994|       3|\n",
      "|       71936|            113228|Sport-100 Helmet,...|   20.994|       1|\n",
      "|       71902|            112980|Sport-100 Helmet,...|   20.994|       2|\n",
      "|       71797|            111075|Sport-100 Helmet,...|   20.994|       6|\n",
      "|       71784|            110794|Sport-100 Helmet,...|   20.994|      10|\n",
      "|       71783|            110751|Sport-100 Helmet,...|   20.994|      10|\n",
      "|       71782|            110709|Sport-100 Helmet,...|   20.994|       3|\n",
      "|       71938|            113295|Sport-100 Helmet,...|   29.994|       5|\n",
      "|       71902|            112988|Sport-100 Helmet,...|   20.994|       4|\n",
      "|       71797|            111082|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71784|            110795|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71783|            110752|Sport-100 Helmet,...|  20.2942|      11|\n",
      "|       71782|            110690|Sport-100 Helmet,...|   20.994|       7|\n",
      "|       71938|            113282|Sport-100 Helmet,...|   20.994|       3|\n",
      "|       71902|            112995|Sport-100 Helmet,...|   20.994|       7|\n",
      "|       71863|            112395|Sport-100 Helmet,...|   20.994|       1|\n",
      "|       71797|            111038|Sport-100 Helmet,...|   20.994|       4|\n",
      "|       71784|            110753|Sport-100 Helmet,...|   20.994|       2|\n",
      "|       71783|            110749|Sport-100 Helmet,...|  19.2445|      15|\n",
      "|       71782|            110708|Sport-100 Helmet,...|   20.994|       6|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [SalesOrderID#1113, SalesOrderDetailID#1114, Name#1063, UnitPrice#1117, OrderQty#1115]\n",
      "   +- BroadcastHashJoin [ProductID#1116], [ProductID#1062], Inner, BuildLeft, false\n",
      "      :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[3, int, false] as bigint)),false), [plan_id=1142]\n",
      "      :  +- Filter isnotnull(ProductID#1116)\n",
      "      :     +- FileScan csv [SalesOrderID#1113,SalesOrderDetailID#1114,OrderQty#1115,ProductID#1116,UnitPrice#1117] Batched: false, DataFilters: [isnotnull(ProductID#1116)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/bigbenchung/MyKnowledgeBase/PySpark/data/SalesOrderDetail.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ProductID)], ReadSchema: struct<SalesOrderID:int,SalesOrderDetailID:int,OrderQty:int,ProductID:int,UnitPrice:double>\n",
      "      +- Filter isnotnull(ProductID#1062)\n",
      "         +- FileScan csv [ProductID#1062,Name#1063] Batched: false, DataFilters: [isnotnull(ProductID#1062)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/bigbenchung/MyKnowledgeBase/PySpark/data/Product.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ProductID)], ReadSchema: struct<ProductID:int,Name:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SparkSQL performs optimization depending on whether intermediate dataframe are cached or not:\n",
    "\n",
    "d1 = dfDetail.join(dfProduct, 'ProductID') \\\n",
    "             .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty')\n",
    "d1.unpersist()\n",
    "d1.show()\n",
    "d1.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "177b6110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+--------------------+---------+--------+\n",
      "|SalesOrderID|SalesOrderDetailID|                Name|UnitPrice|OrderQty|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "|       71784|            110794|Sport-100 Helmet,...|   20.994|      10|\n",
      "|       71783|            110751|Sport-100 Helmet,...|   20.994|      10|\n",
      "|       71797|            111082|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71784|            110795|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71783|            110752|Sport-100 Helmet,...|  20.2942|      11|\n",
      "|       71783|            110749|Sport-100 Helmet,...|  19.2445|      15|\n",
      "|       71784|            110761|        AWC Logo Cap|    5.394|      10|\n",
      "|       71783|            110748|        AWC Logo Cap|   5.2142|      11|\n",
      "|       71782|            110670|        AWC Logo Cap|    5.394|      10|\n",
      "|       71783|            110744|Long-Sleeve Logo ...|  27.4945|      17|\n",
      "|       71936|            113240|Mountain-200 Blac...| 1376.994|      10|\n",
      "|       71797|            111067|Road-550-W Yellow...|  672.294|      10|\n",
      "|       71797|            111070|     Classic Vest, S|   34.925|      23|\n",
      "|       71784|            110791|     Classic Vest, S|   34.925|      23|\n",
      "|       71783|            110727|     Classic Vest, S|   34.925|      15|\n",
      "|       71783|            110747|     Classic Vest, M|     38.1|      10|\n",
      "|       71936|            113236|Women's Mountain ...|  40.5942|      14|\n",
      "|       71832|            111802|Women's Mountain ...|  40.5942|      13|\n",
      "|       71797|            111078|Water Bottle - 30...|   2.8942|      11|\n",
      "|       71784|            110756|Water Bottle - 30...|   2.8942|      11|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [SalesOrderID#1113, SalesOrderDetailID#1114, Name#1063, UnitPrice#1117, OrderQty#1115]\n",
      "   +- BroadcastHashJoin [ProductID#1116], [ProductID#1062], Inner, BuildLeft, false\n",
      "      :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[3, int, false] as bigint)),false), [plan_id=1246]\n",
      "      :  +- Filter ((isnotnull(OrderQty#1115) AND (OrderQty#1115 >= 10)) AND isnotnull(ProductID#1116))\n",
      "      :     +- FileScan csv [SalesOrderID#1113,SalesOrderDetailID#1114,OrderQty#1115,ProductID#1116,UnitPrice#1117] Batched: false, DataFilters: [isnotnull(OrderQty#1115), (OrderQty#1115 >= 10), isnotnull(ProductID#1116)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/bigbenchung/MyKnowledgeBase/PySpark/data/SalesOrderDetail.csv], PartitionFilters: [], PushedFilters: [IsNotNull(OrderQty), GreaterThanOrEqual(OrderQty,10), IsNotNull(ProductID)], ReadSchema: struct<SalesOrderID:int,SalesOrderDetailID:int,OrderQty:int,ProductID:int,UnitPrice:double>\n",
      "      +- Filter isnotnull(ProductID#1062)\n",
      "         +- FileScan csv [ProductID#1062,Name#1063] Batched: false, DataFilters: [isnotnull(ProductID#1062)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/bigbenchung/MyKnowledgeBase/PySpark/data/Product.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ProductID)], ReadSchema: struct<ProductID:int,Name:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#d2 = d1.where(\"Color = 'Black'\")\n",
    "d2 = d1.where(\"OrderQty >= 10\")\n",
    "d2.show()\n",
    "d2.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6ffe0cec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column 'Color' does not exist. Did you mean one of the following? [Name, OrderQty, UnitPrice, SalesOrderID, SalesOrderDetailID]; line 1 pos 0;\n'Filter ('Color = Black)\n+- Relation [SalesOrderID#1485,SalesOrderDetailID#1486,Name#1487,UnitPrice#1488,OrderQty#1489] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11030/1632414363.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0md1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'temp.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0md2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'temp.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0md2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Color = 'Black'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-3.3.2-bin-hadoop3/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   2075\u001b[0m         \"\"\"\n\u001b[1;32m   2076\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2077\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2078\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Conda/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.3.2-bin-hadoop3/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Column 'Color' does not exist. Did you mean one of the following? [Name, OrderQty, UnitPrice, SalesOrderID, SalesOrderDetailID]; line 1 pos 0;\n'Filter ('Color = Black)\n+- Relation [SalesOrderID#1485,SalesOrderDetailID#1486,Name#1487,UnitPrice#1488,OrderQty#1489] csv\n"
     ]
    }
   ],
   "source": [
    "# This will report an error:\n",
    "\n",
    "d1 = dfDetail.join(dfProduct, 'ProductID') \\\n",
    "             .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty')\n",
    "d1.write.csv('temp.csv', mode = 'overwrite', header = True)\n",
    "d2 = spark.read.csv('temp.csv', header = True, inferSchema = True)\n",
    "d2.filter(\"Color = 'Black'\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e2f5d7e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot resolve column name \"Color\" among (SalesOrderID, SalesOrderDetailID, Name, UnitPrice, OrderQty)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11030/471516641.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0md1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfDetail\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfProduct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ProductID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m              \u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SalesOrderID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SalesOrderDetailID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'UnitPrice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'OrderQty'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0md2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Color'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Black'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Because the parser will try to find a column named 'Color' in d1, which doesn't exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.3.2-bin-hadoop3/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1963\u001b[0m         \"\"\"\n\u001b[1;32m   1964\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1965\u001b[0;31m             \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1966\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1967\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Conda/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.3.2-bin-hadoop3/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Cannot resolve column name \"Color\" among (SalesOrderID, SalesOrderDetailID, Name, UnitPrice, OrderQty)"
     ]
    }
   ],
   "source": [
    "# This will report an error, too\n",
    "\n",
    "d1 = dfDetail.join(dfProduct, 'ProductID') \\\n",
    "             .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty')\n",
    "d2 = d1.filter(d1['Color'] == 'Black').show()\n",
    "\n",
    "# Because the parser will try to find a column named 'Color' in d1, which doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bf979065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|SalesOrderID|\n",
      "+------------+\n",
      "|       71902|\n",
      "|       71832|\n",
      "|       71915|\n",
      "|       71831|\n",
      "|       71898|\n",
      "|       71935|\n",
      "|       71938|\n",
      "|       71845|\n",
      "|       71783|\n",
      "|       71815|\n",
      "|       71936|\n",
      "|       71863|\n",
      "|       71780|\n",
      "|       71782|\n",
      "|       71899|\n",
      "|       71784|\n",
      "|       71797|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find all orders that include at least one black product, \n",
    "# return the product SalesOrderID, Name, UnitPrice, and OrderQty\n",
    "\n",
    "# SELECT DISTINCT SalesOrderID\n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# JOIN SalesLT.Product ON SalesOrderDetail.ProductID = Product.ProductID\n",
    "# WHERE Color = 'Black'\n",
    "\n",
    "dfDetail.join(dfProduct.filter(\"Color='Black'\"), 'ProductID') \\\n",
    "        .select('SalesOrderID') \\\n",
    "        .distinct() \\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "85a47850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many colors in the products?\n",
    "\n",
    "# SELECT COUNT(DISTINCT Color)\n",
    "# FROM SalesLT.Product\n",
    "\n",
    "dfProduct.select('Color').distinct().count()\n",
    "\n",
    "# It's 1 more than standard SQL.  In standard SQL, COUNT() does not count NULLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c079b0ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|SalesOrderID|        TotalPrice|\n",
      "+------------+------------------+\n",
      "|       71867|             858.9|\n",
      "|       71902|59894.209199999976|\n",
      "|       71832|      28950.678108|\n",
      "|       71915|1732.8899999999999|\n",
      "|       71946|            31.584|\n",
      "|       71895|221.25600000000003|\n",
      "|       71816|2847.4079999999994|\n",
      "|       71831|          1712.946|\n",
      "|       71923|         96.108824|\n",
      "|       71858|11528.844000000001|\n",
      "|       71917|            37.758|\n",
      "|       71897|          10585.05|\n",
      "|       71885|           524.664|\n",
      "|       71856|500.30400000000003|\n",
      "|       71898| 53248.69200000002|\n",
      "|       71774|           713.796|\n",
      "|       71796| 47848.02600000001|\n",
      "|       71935|5533.8689079999995|\n",
      "|       71938|         74205.228|\n",
      "|       71845|        34118.5356|\n",
      "+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the total price of each order, \n",
    "# return SalesOrderID and total price (column name should be �勇otalprice��)\n",
    "\n",
    "# SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice\n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# GROUP BY SalesOrderID\n",
    "\n",
    "dfDetail.select('*', (dfDetail.UnitPrice * dfDetail.OrderQty\n",
    "                      * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\\n",
    "        .groupBy('SalesOrderID').sum('netprice') \\\n",
    "        .withColumnRenamed('sum(netprice)', 'TotalPrice')\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "187f754a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|SalesOrderID|        TotalPrice|\n",
      "+------------+------------------+\n",
      "|       71902|59894.209199999976|\n",
      "|       71832|      28950.678108|\n",
      "|       71858|11528.844000000001|\n",
      "|       71897|          10585.05|\n",
      "|       71898| 53248.69200000002|\n",
      "|       71796| 47848.02600000001|\n",
      "|       71938|         74205.228|\n",
      "|       71845|        34118.5356|\n",
      "|       71783|      65683.367986|\n",
      "|       71936| 79589.61602399996|\n",
      "|       71780|29923.007999999998|\n",
      "|       71782| 33319.98600000001|\n",
      "|       71784| 89869.27631400003|\n",
      "|       71797| 65123.46341800001|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the total price of each order where the total price > 10000\n",
    "\n",
    "# SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice\n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# GROUP BY SalesOrderID\n",
    "# HAVING SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) > 10000\n",
    "\n",
    "dfDetail.select('*', (dfDetail.UnitPrice * dfDetail. OrderQty\n",
    "                      * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\\n",
    "        .groupBy('SalesOrderID').sum('netprice') \\\n",
    "        .withColumnRenamed('sum(netprice)', 'TotalPrice')\\\n",
    "        .where('TotalPrice > 10000')\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "122b9711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|SalesOrderID|        TotalPrice|\n",
      "+------------+------------------+\n",
      "|       71902|26677.883999999995|\n",
      "|       71832|      16883.748108|\n",
      "|       71938|         33824.448|\n",
      "|       71845|         18109.836|\n",
      "|       71783|15524.117476000003|\n",
      "|       71936|      44490.290424|\n",
      "|       71780|         16964.322|\n",
      "|       71797|      27581.613792|\n",
      "+------------+------------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [SalesOrderID#1113, sum(netprice)#1723 AS TotalPrice#1726]\n",
      "   +- Filter (isnotnull(sum(netprice)#1723) AND (sum(netprice)#1723 > 10000.0))\n",
      "      +- HashAggregate(keys=[SalesOrderID#1113], functions=[sum(netprice#1659)])\n",
      "         +- Exchange hashpartitioning(SalesOrderID#1113, 200), ENSURE_REQUIREMENTS, [plan_id=1911]\n",
      "            +- HashAggregate(keys=[SalesOrderID#1113], functions=[partial_sum(netprice#1659)])\n",
      "               +- Project [SalesOrderID#1113, netprice#1659]\n",
      "                  +- BroadcastHashJoin [ProductID#1116], [ProductID#1062], Inner, BuildLeft, false\n",
      "                     :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[1, int, true] as bigint)),false), [plan_id=1906]\n",
      "                     :  +- Project [SalesOrderID#1113, ProductID#1116, ((UnitPrice#1117 * cast(OrderQty#1115 as double)) * (1.0 - UnitPriceDiscount#1118)) AS netprice#1659]\n",
      "                     :     +- Filter isnotnull(ProductID#1116)\n",
      "                     :        +- FileScan csv [SalesOrderID#1113,OrderQty#1115,ProductID#1116,UnitPrice#1117,UnitPriceDiscount#1118] Batched: false, DataFilters: [isnotnull(ProductID#1116)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/bigbenchung/MyKnowledgeBase/PySpark/data/SalesOrderDetail.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ProductID)], ReadSchema: struct<SalesOrderID:int,OrderQty:int,ProductID:int,UnitPrice:double,UnitPriceDiscount:double>\n",
      "                     +- Project [ProductID#1062]\n",
      "                        +- Filter ((isnotnull(Color#1065) AND (Color#1065 = Black)) AND isnotnull(ProductID#1062))\n",
      "                           +- FileScan csv [ProductID#1062,Color#1065] Batched: false, DataFilters: [isnotnull(Color#1065), (Color#1065 = Black), isnotnull(ProductID#1062)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/bigbenchung/MyKnowledgeBase/PySpark/data/Product.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Color), EqualTo(Color,Black), IsNotNull(ProductID)], ReadSchema: struct<ProductID:int,Color:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the total price on the black products of each order where the total price > 10000\n",
    "\n",
    "# SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice\n",
    "# FROM SalesLT.SalesOrderDetail, SalesLT.Product\n",
    "# WHERE SalesLT.SalesOrderDetail.ProductID = SalesLT.Product.ProductID AND Color = 'Black'\n",
    "# GROUP BY SalesOrderID\n",
    "# HAVING SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) > 10000\n",
    "\n",
    "res = dfDetail.select('*', (dfDetail.UnitPrice * dfDetail. OrderQty\n",
    "                      * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\\n",
    "        .join(dfProduct, 'ProductID') \\\n",
    "        .where(\"Color = 'Black'\")\\\n",
    "        .groupBy('SalesOrderID').sum('netprice') \\\n",
    "        .withColumnRenamed('sum(netprice)', 'TotalPrice')\\\n",
    "        .where('TotalPrice > 10000')\n",
    "\n",
    "res.show()\n",
    "\n",
    "res.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6292437e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+-------------+\n",
      "|CustomerID|   FirstName|    LastName|sum(OrderQty)|\n",
      "+----------+------------+------------+-------------+\n",
      "|     30050|     Krishna|Sunkammurali|           89|\n",
      "|     29796|         Jon|      Grande|           65|\n",
      "|     29957|       Kevin|         Liu|           62|\n",
      "|     29929|     Jeffrey|       Kurtz|           46|\n",
      "|     29546| Christopher|        Beck|           45|\n",
      "|     29922|      Pamala|        Kotc|           34|\n",
      "|     30113|        Raja|   Venugopal|           34|\n",
      "|     29938|       Frank|    Campbell|           29|\n",
      "|     29736|       Terry|   Eminhizer|           23|\n",
      "|     29485|   Catherine|        Abel|           10|\n",
      "|     30019|     Matthew|      Miller|            9|\n",
      "|     29932|     Rebecca|      Laszlo|            7|\n",
      "|     29975|      Walter|        Mays|            5|\n",
      "|     29638|    Rosmarie|     Carroll|            2|\n",
      "|     29531|        Cory|       Booth|            1|\n",
      "|     30089|Michael John|      Troyer|            1|\n",
      "|     29568|      Donald|     Blanton|            1|\n",
      "|       148|        Alan|      Brewer|         null|\n",
      "|       471|        John|        Ford|         null|\n",
      "|       496|      Marcia|      Sultan|         null|\n",
      "+----------+------------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each customer, find the total quantity of black products bought.\n",
    "# Report CustomerID, FirstName, LastName, and total quantity\n",
    "\n",
    "# select saleslt.customer.customerid, FirstName, LastName, sum(orderqty)\n",
    "# from saleslt.customer\n",
    "# left outer join \n",
    "# (\n",
    "# saleslt.salesorderheader\n",
    "# join saleslt.salesorderdetail\n",
    "# on saleslt.salesorderdetail.salesorderid = saleslt.salesorderheader.salesorderid\n",
    "# join saleslt.product\n",
    "# on saleslt.product.productid = saleslt.salesorderdetail.productid and color = 'black'\n",
    "# )\n",
    "# on saleslt.customer.customerid = saleslt.salesorderheader.customerid\n",
    "# group by saleslt.customer.customerid, FirstName, LastName\n",
    "# order by sum(orderqty) desc\n",
    "\n",
    "d1 = dfDetail.join(dfProduct, 'ProductID')\\\n",
    "             .where('Color = \"Black\"') \\\n",
    "             .join(dfHeader, 'SalesOrderID')\\\n",
    "             .groupBy('CustomerID').sum('OrderQty')\n",
    "dfCustomer.join(d1, 'CustomerID', 'left_outer')\\\n",
    "          .select('CustomerID', 'FirstName', 'LastName', 'sum(OrderQty)')\\\n",
    "          .orderBy('sum(OrderQty)', ascending=False)\\\n",
    "          .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6034d787",
   "metadata": {},
   "source": [
    "### Rewriting PMI example using DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "42b436e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|       early radical|\n",
      "|   french revolution|\n",
      "|      pejorative way|\n",
      "|       violent means|\n",
      "|      positive label|\n",
      "|self-defined anar...|\n",
      "|political philosophy|\n",
      "|differ interpreta...|\n",
      "|     relate movement|\n",
      "|     social movement|\n",
      "|authoritarian ins...|\n",
      "|      most anarchist|\n",
      "|  harmonious society|\n",
      "|anti-authoritaria...|\n",
      "|authoritarian str...|\n",
      "| political structure|\n",
      "|coercive institution|\n",
      "|economic institution|\n",
      "|     social relation|\n",
      "|voluntary associa...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "lines = spark.read.text('data/adj_noun_pairs.txt')\n",
    "lines.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b9a7248f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                   w|\n",
      "+--------------------+\n",
      "|    [early, radical]|\n",
      "|[french, revolution]|\n",
      "|   [pejorative, way]|\n",
      "|    [violent, means]|\n",
      "|   [positive, label]|\n",
      "|[self-defined, an...|\n",
      "|[political, philo...|\n",
      "|[differ, interpre...|\n",
      "|  [relate, movement]|\n",
      "|  [social, movement]|\n",
      "|[authoritarian, i...|\n",
      "|   [most, anarchist]|\n",
      "|[harmonious, soci...|\n",
      "|[anti-authoritari...|\n",
      "|[authoritarian, s...|\n",
      "|[political, struc...|\n",
      "|[coercive, instit...|\n",
      "|[economic, instit...|\n",
      "|  [social, relation]|\n",
      "|[voluntary, assoc...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 143:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+\n",
      "|               adj|          noun|\n",
      "+------------------+--------------+\n",
      "|             early|       radical|\n",
      "|            french|    revolution|\n",
      "|        pejorative|           way|\n",
      "|           violent|         means|\n",
      "|          positive|         label|\n",
      "|      self-defined|     anarchist|\n",
      "|         political|    philosophy|\n",
      "|            differ|interpretation|\n",
      "|            relate|      movement|\n",
      "|            social|      movement|\n",
      "|     authoritarian|   institution|\n",
      "|              most|     anarchist|\n",
      "|        harmonious|       society|\n",
      "|anti-authoritarian|       society|\n",
      "|     authoritarian|     structure|\n",
      "|         political|     structure|\n",
      "|          coercive|   institution|\n",
      "|          economic|   institution|\n",
      "|            social|      relation|\n",
      "|         voluntary|   association|\n",
      "+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Converting lines into word pairs. \n",
    "# Data is dirty: some lines have more than 2 words, so filter them out.\n",
    "# pairs = lines.map(lambda l: tuple(l.split())).filter(lambda p: len(p)==2)\n",
    "words = lines.select(split(lines[0],' ').alias('w')).filter(size('w')==2)  \n",
    "words.show()\n",
    "pairs = words.select(words['w'][0].alias('adj'), words['w'][1].alias('noun'))\n",
    "pairs.cache()\n",
    "pairs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "562052bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 144:===================>                                     (2 + 4) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3162692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "N = pairs.count()\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1ef1c8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 147:============================>                            (3 + 3) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|      adj|      noun|count|\n",
      "+---------+----------+-----+\n",
      "| external|      link| 8136|\n",
      "|     19th|   century| 2869|\n",
      "|     20th|   century| 2816|\n",
      "|     same|      time| 2744|\n",
      "|    first|      time| 2632|\n",
      "|    civil|       war| 2236|\n",
      "|    large|    number| 2094|\n",
      "|    other|      hand| 2043|\n",
      "|political|     party| 1899|\n",
      "|    other|   country| 1857|\n",
      "|   recent|      year| 1808|\n",
      "|     many|    people| 1743|\n",
      "|    early|   century| 1568|\n",
      "|     next|      year| 1557|\n",
      "|    total|population| 1463|\n",
      "|     same|      year| 1416|\n",
      "|     many|      year| 1407|\n",
      "|    prime|  minister| 1299|\n",
      "|   ethnic|     group| 1280|\n",
      "|     late|   century| 1270|\n",
      "+---------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Compute the frequency of each pair.\n",
    "# Ignore pairs that not frequent enough\n",
    "# pair_freqs = pairs.map(lambda p: (p,1)).reduceByKey(lambda f1, f2: f1 + f2) \\\n",
    "#                   .filter(lambda pf: pf[1] >= 100)\n",
    "\n",
    "pair_freqs = pairs.groupBy('adj', 'noun').count().filter('count >= 100').orderBy('count', ascending=False)\n",
    "pair_freqs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "edecf1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 150:>                                                        (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+\n",
      "|          adj|adjcount|\n",
      "+-------------+--------+\n",
      "|   indigenous|    1753|\n",
      "|          few|   11663|\n",
      "|     everyday|     712|\n",
      "|       online|    1753|\n",
      "|     cautious|      70|\n",
      "|     inverted|     155|\n",
      "|  unequivocal|      34|\n",
      "|     incoming|     341|\n",
      "|  11-year-old|      12|\n",
      "|       lamian|       1|\n",
      "|        inner|    1491|\n",
      "|precautionary|      63|\n",
      "|   electrical|    2261|\n",
      "|    recognize|     359|\n",
      "| cattle-based|       1|\n",
      "|      balding|      10|\n",
      "|     inertial|     284|\n",
      "|      lyrical|     177|\n",
      "|   convergent|     106|\n",
      "|     elongate|     127|\n",
      "+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Computing the frequencies of the adjectives and the nouns\n",
    "# a_freqs = pairs.map(lambda p: (p[0],1)).reduceByKey(lambda x,y: x+y)\n",
    "# n_freqs = pairs.map(lambda p: (p[1],1)).reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "a_freqs =  pairs.groupBy('adj').count().withColumnRenamed('count', 'adjcount')\n",
    "n_freqs =  pairs.groupBy('noun').count().withColumnRenamed('count', 'nouncount')\n",
    "\n",
    "a_freqs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7bca9ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+------------------+\n",
      "|              adj|        noun|               PMI|\n",
      "+-----------------+------------+------------------+\n",
      "|            magna|       carta|14.410196596376286|\n",
      "|polish-lithuanian|Commonwealth| 13.07137409960666|\n",
      "|          nitrous|       oxide| 12.99060582764508|\n",
      "|       latter-day|      Saints|12.649734254024207|\n",
      "|        stainless|       steel|12.506597586010825|\n",
      "|             pave|      runway|12.482339231599479|\n",
      "|         corporal|  punishment|12.191415428592215|\n",
      "|          capital|  punishment|12.183256905205052|\n",
      "|             rush|        yard|  12.1470236944742|\n",
      "|         globular|     cluster|12.109954005340597|\n",
      "|         teutonic|      knight|12.074200587806475|\n",
      "|       refractive|       index|11.828363002104304|\n",
      "|           spinal|        cord|11.815718560868772|\n",
      "|        alcoholic|    beverage|11.808523043970219|\n",
      "|          unpaved|      runway| 11.79695092191404|\n",
      "|         anglican|   Communion|11.752242121990406|\n",
      "|          coaxial|       cable|11.684885137122127|\n",
      "|          angular|    momentum|11.622460251835882|\n",
      "|          unpaved|          km|11.408626883879302|\n",
      "|           mobile|    cellular|11.315829787299972|\n",
      "+-----------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d1 = pair_freqs.join(a_freqs, 'adj').join(n_freqs, 'noun') \\\n",
    "          .select('adj', 'noun', \n",
    "                  log2(col('count')*N/(col('adjcount')*col('nouncount')))\n",
    "                  .alias('PMI')) \\\n",
    "          .orderBy(desc('PMI')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41489f9e",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "### Embed SQL queries\n",
    "\n",
    "You can also run SQL queries over dataframes once you register them as temporary tables within the SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dfef5678",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"data/departuredelays.csv\"\n",
    "# Read and create a temporary view\n",
    "df = (spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(csv_file))\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "79286ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT distance, origin, destination\n",
    "    FROM us_delay_flights_tbl WHERE distance > 1000\n",
    "    ORDER BY distance DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ed563850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|2190925| 1638|   SFO|        ORD|\n",
      "|1031755|  396|   SFO|        ORD|\n",
      "|1022330|  326|   SFO|        ORD|\n",
      "|1051205|  320|   SFO|        ORD|\n",
      "|1190925|  297|   SFO|        ORD|\n",
      "|2171115|  296|   SFO|        ORD|\n",
      "|1071040|  279|   SFO|        ORD|\n",
      "|1051550|  274|   SFO|        ORD|\n",
      "|3120730|  266|   SFO|        ORD|\n",
      "|1261104|  258|   SFO|        ORD|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT date, delay, origin, destination\n",
    "    FROM us_delay_flights_tbl\n",
    "    WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD'\n",
    "    ORDER by delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a1bd7ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 169:===================>                                     (2 + 4) / 6]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    "    CASE\n",
    "        WHEN delay > 360 THEN 'Very Long Delays'\n",
    "        WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    "        WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    "        WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    "        WHEN delay = 0 THEN 'No Delays'\n",
    "        ELSE 'Early'\n",
    "    END AS Flight_Delays\n",
    "    FROM us_delay_flights_tbl\n",
    "    ORDER BY origin, delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9cff0e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|3061605| 1004|   HNL|        LAX|\n",
      "|1311800| 1004|   HNL|        DFW|\n",
      "|2121625|  932|   HNL|        JFK|\n",
      "|2102155|  724|   HNL|        SFO|\n",
      "|2171300|  626|   HNL|        LAX|\n",
      "|2040725|  622|   HNL|        SFO|\n",
      "|3020745|  600|   HNL|        LAX|\n",
      "|2141405|  586|   HNL|        SFO|\n",
      "|2271255|  521|   HNL|        PHX|\n",
      "|2281300|  474|   HNL|        LAX|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [delay#2420 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(delay#2420 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=2986]\n",
      "      +- Filter (((isnotnull(origin#2422) AND isnotnull(delay#2420)) AND (origin#2422 = HNL)) AND (delay#2420 > 120))\n",
      "         +- FileScan csv [date#2419,delay#2420,origin#2422,destination#2423] Batched: false, DataFilters: [isnotnull(origin#2422), isnotnull(delay#2420), (origin#2422 = HNL), (delay#2420 > 120)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/bigbenchung/MyKnowledgeBase/PySpark/data/departuredelays.csv], PartitionFilters: [], PushedFilters: [IsNotNull(origin), IsNotNull(delay), EqualTo(origin,HNL), GreaterThan(delay,120)], ReadSchema: struct<date:int,delay:int,origin:string,destination:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Can even mix DataFrame API with SQL:\n",
    "df.where('origin = \"HNL\"').createOrReplaceTempView('HNLflights')\n",
    "df2 = spark.sql(\"\"\"SELECT date, delay, origin, destination\n",
    "    FROM HNLflights\n",
    "    WHERE delay > 120 \n",
    "    ORDER by delay DESC\"\"\")\n",
    "df2.show(10)\n",
    "df2.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "23c3181a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+--------+\n",
      "|   date|delay|distance|origin|destination|newdelay|\n",
      "+-------+-----+--------+------+-----------+--------+\n",
      "|1011245|    6|     602|   ABE|        ATL|      13|\n",
      "|1020600|   -8|     369|   ABE|        DTW|     -15|\n",
      "|1021245|   -2|     602|   ABE|        ATL|      -3|\n",
      "|1020605|   -4|     602|   ABE|        ATL|      -7|\n",
      "|1031245|   -4|     602|   ABE|        ATL|      -7|\n",
      "|1030605|    0|     602|   ABE|        ATL|       1|\n",
      "|1041243|   10|     602|   ABE|        ATL|      21|\n",
      "|1040605|   28|     602|   ABE|        ATL|      57|\n",
      "|1051245|   88|     602|   ABE|        ATL|     177|\n",
      "|1050605|    9|     602|   ABE|        ATL|      19|\n",
      "|1061215|   -6|     602|   ABE|        ATL|     -11|\n",
      "|1061725|   69|     602|   ABE|        ATL|     139|\n",
      "|1061230|    0|     369|   ABE|        DTW|       1|\n",
      "|1060625|   -3|     602|   ABE|        ATL|      -5|\n",
      "|1070600|    0|     369|   ABE|        DTW|       1|\n",
      "|1071725|    0|     602|   ABE|        ATL|       1|\n",
      "|1071230|    0|     369|   ABE|        DTW|       1|\n",
      "|1070625|    0|     602|   ABE|        ATL|       1|\n",
      "|1071219|    0|     569|   ABE|        ORD|       1|\n",
      "|1080600|    0|     369|   ABE|        DTW|       1|\n",
      "+-------+-----+--------+------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 171:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# UDF\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "myf = udf(lambda s: s*2 + 1, IntegerType())\n",
    "df.select('*', myf(df['delay']).alias('newdelay')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2aff8f74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+--------+\n",
      "|   date|delay|distance|origin|destination|newdelay|\n",
      "+-------+-----+--------+------+-----------+--------+\n",
      "|1011800|   39|    3288|   HNL|        DFW|      79|\n",
      "|1011700|   -1|    3687|   HNL|        ORD|      -1|\n",
      "|1011950|   -3|    3288|   HNL|        DFW|      -5|\n",
      "|1011340|   -8|    2221|   HNL|        LAX|     -15|\n",
      "|1010830|  -13|    2221|   HNL|        LAX|     -25|\n",
      "|1011615|  -16|    2221|   HNL|        LAX|     -31|\n",
      "|1012155|    7|    2221|   HNL|        LAX|      15|\n",
      "|1021800|  319|    3288|   HNL|        DFW|     639|\n",
      "|1021700|    0|    3687|   HNL|        ORD|       1|\n",
      "|1021950|    2|    3288|   HNL|        DFW|       5|\n",
      "|1021340|    3|    2221|   HNL|        LAX|       7|\n",
      "|1020830|  -14|    2221|   HNL|        LAX|     -27|\n",
      "|1021615|   19|    2221|   HNL|        LAX|      39|\n",
      "|1022155|  130|    2221|   HNL|        LAX|     261|\n",
      "|1031800|   54|    3288|   HNL|        DFW|     109|\n",
      "|1031700|   11|    3687|   HNL|        ORD|      23|\n",
      "|1031950|  239|    3288|   HNL|        DFW|     479|\n",
      "|1031340|   13|    2221|   HNL|        LAX|      27|\n",
      "|1030830|  -10|    2221|   HNL|        LAX|     -19|\n",
      "|1031615|    7|    2221|   HNL|        LAX|      15|\n",
      "+-------+-----+--------+------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register('myf', lambda s: s*2+1, IntegerType())\n",
    "spark.sql('SELECT *, myf(Delay) AS newdelay FROM HNLflights').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e7758c",
   "metadata": {},
   "source": [
    "### Read data from Parquet files\n",
    "Parquet is an open source columnar file format that offers many I/O\n",
    "optimizations (such as compression, which saves storage space and allows for quick\n",
    "access to data columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "49474f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- host_is_superhost: string (nullable = true)\n",
      " |-- cancellation_policy: string (nullable = true)\n",
      " |-- instant_bookable: string (nullable = true)\n",
      " |-- host_total_listings_count: double (nullable = true)\n",
      " |-- neighbourhood_cleansed: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- accommodates: double (nullable = true)\n",
      " |-- bathrooms: double (nullable = true)\n",
      " |-- bedrooms: double (nullable = true)\n",
      " |-- beds: double (nullable = true)\n",
      " |-- bed_type: string (nullable = true)\n",
      " |-- minimum_nights: double (nullable = true)\n",
      " |-- number_of_reviews: double (nullable = true)\n",
      " |-- review_scores_rating: double (nullable = true)\n",
      " |-- review_scores_accuracy: double (nullable = true)\n",
      " |-- review_scores_cleanliness: double (nullable = true)\n",
      " |-- review_scores_checkin: double (nullable = true)\n",
      " |-- review_scores_communication: double (nullable = true)\n",
      " |-- review_scores_location: double (nullable = true)\n",
      " |-- review_scores_value: double (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- bedrooms_na: double (nullable = true)\n",
      " |-- bathrooms_na: double (nullable = true)\n",
      " |-- beds_na: double (nullable = true)\n",
      " |-- review_scores_rating_na: double (nullable = true)\n",
      " |-- review_scores_accuracy_na: double (nullable = true)\n",
      " |-- review_scores_cleanliness_na: double (nullable = true)\n",
      " |-- review_scores_checkin_na: double (nullable = true)\n",
      " |-- review_scores_communication_na: double (nullable = true)\n",
      " |-- review_scores_location_na: double (nullable = true)\n",
      " |-- review_scores_value_na: double (nullable = true)\n",
      "\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|neighbourhood_cleansed|      room_type|bedrooms|bathrooms|number_of_reviews|price|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|      Western Addition|Entire home/apt|     1.0|      1.0|            180.0|170.0|\n",
      "|        Bernal Heights|Entire home/apt|     2.0|      1.0|            111.0|235.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|             17.0| 65.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|              8.0| 65.0|\n",
      "|      Western Addition|Entire home/apt|     2.0|      1.5|             27.0|785.0|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data at https://www.cse.ust.hk/msbd5003/data/sf-airbnb.rar\n",
    "\n",
    "filePath = 'data/sf-airbnb-clean.parquet/'\n",
    "airbnbDF = spark.read.parquet(filePath)\n",
    "\n",
    "airbnbDF.printSchema()\n",
    "\n",
    "# Our goal is to predict the price per night for a rental property, given our features.\n",
    "airbnbDF.select(\"neighbourhood_cleansed\", \"room_type\", \"bedrooms\", \"bathrooms\",\n",
    "\"number_of_reviews\", \"price\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75910dbf",
   "metadata": {},
   "source": [
    "---\n",
    "## Flexible Data Model\n",
    "\n",
    "Sample data file at\n",
    "\n",
    "https://www.cse.ust.hk/msbd5003/data/products.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7070c32c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dimensions: struct (nullable = true)\n",
      " |    |-- height: double (nullable = true)\n",
      " |    |-- length: double (nullable = true)\n",
      " |    |-- width: double (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- warehouseLocation: struct (nullable = true)\n",
      " |    |-- latitude: double (nullable = true)\n",
      " |    |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json('data/products.json')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c86aa5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---+----------------+-----+-----------+-----------------+\n",
      "|      dimensions| id|            name|price|       tags|warehouseLocation|\n",
      "+----------------+---+----------------+-----+-----------+-----------------+\n",
      "|{9.5, 7.0, 12.0}|  2|An ice sculpture| 12.5|[cold, ice]|   {-78.75, 20.4}|\n",
      "| {1.0, 3.1, 1.0}|  3|    A blue mouse| 25.5|       null|    {54.4, -32.7}|\n",
      "+----------------+---+----------------+-----+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1f01bb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|height|\n",
      "+------+\n",
      "|   9.5|\n",
      "|   1.0|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Accessing nested fields\n",
    "\n",
    "df.select(df['dimensions.height']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "270ce4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|height|\n",
      "+------+\n",
      "|   9.5|\n",
      "|   1.0|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('dimensions.height').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e260ad7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|height|\n",
      "+------+\n",
      "|   9.5|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('dimensions.height')\\\n",
    "  .filter(\"tags[0] = 'cold' AND warehouseLocation.latitude < 0\")\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e6680cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dimensions=Row(height=9.5, length=7.0, width=12.0), id=2, name='An ice sculpture', price=12.5, tags=['cold', 'ice'], warehouseLocation=Row(latitude=-78.75, longitude=20.4)),\n",
       " Row(dimensions=Row(height=1.0, length=3.1, width=1.0), id=3, name='A blue mouse', price=25.5, tags=None, warehouseLocation=Row(latitude=54.4, longitude=-32.7))]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8b86f4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|array_remove(tags, ice)|\n",
      "+-----------------------+\n",
      "|                 [cold]|\n",
      "|                   null|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Functions for arrays\n",
    "df.select(array_remove(df.tags, 'ice')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4c855619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|size(tags)|\n",
      "+----------+\n",
      "|         2|\n",
      "|        -1|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(size(df.tags)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9f6ff053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|big tags|\n",
      "+--------+\n",
      "|   [ice]|\n",
      "|    null|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(filter(df.tags, lambda x: x > 'd').alias('big tags')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f3075e",
   "metadata": {},
   "source": [
    "### Closure in DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a8a3dd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: long (nullable = true)\n",
      "\n",
      "+---+\n",
      "| _1|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(i, ) for i in range(10)]\n",
    "df = spark.createDataFrame(data)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "04bc06f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| _1|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n",
      "+---+\n",
      "| _1|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The 'closure' behaviour in RDD doesn't seem to exist for DataFrames\n",
    "\n",
    "x = 5\n",
    "df1 = df.filter(df._1 < x)\n",
    "df1.show()\n",
    "x = 3\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9bfae2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(_1#2812L) AND (_1#2812L < 5))\n",
      "+- *(1) Scan ExistingRDD[_1#2812L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Because of the Catalyst optimizer !\n",
    "\n",
    "df1.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6ee678fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [(cast((_1#2812L * 2) as double) + 12.5) AS ((_1 * 2) + 12.5)#2829]\n",
      "+- *(1) Scan ExistingRDD[_1#2812L]\n",
      "\n",
      "\n",
      "+-----------------+\n",
      "|((_1 * 2) + 12.5)|\n",
      "+-----------------+\n",
      "|             12.5|\n",
      "|             14.5|\n",
      "|             16.5|\n",
      "|             18.5|\n",
      "|             20.5|\n",
      "|             22.5|\n",
      "|             24.5|\n",
      "|             26.5|\n",
      "|             28.5|\n",
      "|             30.5|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x/2 + x*2\n",
    "z = 5\n",
    "df1 = df.select(df._1 * 2 + f(z))\n",
    "df1.explain()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e77025dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(10))\n",
    "x = 5\n",
    "a = rdd.filter(lambda z: z < x)\n",
    "print(a.take(10))\n",
    "x = 3\n",
    "print(a.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cd413d",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Monotonicity checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6f76cc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|value|  i|\n",
      "+-----+---+\n",
      "|    0|  0|\n",
      "|    1|  1|\n",
      "|    2|  2|\n",
      "|    3|  3|\n",
      "|    5|  4|\n",
      "|    2|  5|\n",
      "|    7|  6|\n",
      "|    8|  7|\n",
      "|   10|  8|\n",
      "+-----+---+\n",
      "\n",
      "+------+-----+\n",
      "|ivalue|nexti|\n",
      "+------+-----+\n",
      "|     0|    1|\n",
      "|     1|    2|\n",
      "|     2|    3|\n",
      "|     3|    4|\n",
      "|     5|    5|\n",
      "|     2|    6|\n",
      "|     7|    7|\n",
      "|     8|    8|\n",
      "|    10|    9|\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+-----+---+\n",
      "|ivalue|nexti|value|  i|\n",
      "+------+-----+-----+---+\n",
      "|     0|    1|    1|  1|\n",
      "|     1|    2|    2|  2|\n",
      "|     2|    3|    3|  3|\n",
      "|     3|    4|    5|  4|\n",
      "|     5|    5|    2|  5|\n",
      "|     2|    6|    7|  6|\n",
      "|     7|    7|    8|  7|\n",
      "|     8|    8|   10|  8|\n",
      "+------+-----+-----+---+\n",
      "\n",
      "[Row(ivalue=5, nexti=5, value=2, i=5)]\n"
     ]
    }
   ],
   "source": [
    "data = [0, 1, 2, 3, 5, 2, 7, 8, 10]\n",
    "\n",
    "rdd = sc.parallelize(data)\n",
    "rdd = rdd.zipWithIndex()\n",
    "\n",
    "df = spark.createDataFrame(rdd, ['value', 'i'])\n",
    "df.show()\n",
    "\n",
    "df1 = df.select(df['value'].alias('ivalue'), (col('i') + 1).alias('nexti'))\n",
    "df1.show()\n",
    "df2 = df1.join(df, df1[\"nexti\"] == df[\"i\"])\n",
    "df2.show()\n",
    "print(df2.filter(\"ivalue > value\").take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745a5b99",
   "metadata": {},
   "source": [
    "# Page Rank Algorithm in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "94889db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|src|              rank|\n",
      "+---+------------------+\n",
      "|  1|1.2981882732854677|\n",
      "|  3|0.9999999999999998|\n",
      "|  4|0.9999999999999998|\n",
      "|  2|0.7018117267145316|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "numOfIterations = 10\n",
    "\n",
    "lines = spark.read.text(\"data/pagerank_data.txt\")\n",
    "\n",
    "a = lines.select(split(lines[0],' '))\n",
    "links = a.select(a[0][0].alias('src'), a[0][1].alias('dst'))\n",
    "outdegrees = links.groupBy('src').count()\n",
    "ranks = outdegrees.select('src', lit(1).alias('rank'))\n",
    "\n",
    "for iteration in range(numOfIterations):\n",
    "    contribs = links.join(ranks, \"src\").join(outdegrees, \"src\").withColumn('rank', col(\"rank\") / col(\"count\"))\n",
    "    ranks = contribs.groupBy(\"dst\").agg(sum(\"rank\")*0.85+0.15).withColumnRenamed('((sum(rank) * 0.85) + 0.15)', \"rank\") \\\n",
    "        .withColumnRenamed('dst', \"src\")\n",
    "\n",
    "ranks.orderBy(desc('rank')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d37373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
